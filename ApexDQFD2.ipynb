{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kukjin/anaconda3/envs/minerl/lib/python3.7/site-packages/ray/autoscaler/_private/cli_logger.py:61: FutureWarning: Not all Ray CLI dependencies were found. In Ray 1.4+, the Ray CLI, autoscaler, and dashboard will only be usable via `pip install 'ray[default]'`. Please update your install command.\n",
      "  \"update your install command.\", FutureWarning)\n",
      "/home/kukjin/anaconda3/envs/minerl/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    }
   ],
   "source": [
    "from model import DQN\n",
    "import os\n",
    "import minerl\n",
    "import gym\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import wandb\n",
    "import ray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from _collections import deque\n",
    "from utils import *\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mneverparadise\u001b[0m (use `wandb login --relogin` to force relogin)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.10.30 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Tracking run with wandb version 0.10.26<br/>\n",
       "                Syncing run <strong style=\"color:#cdcd00\">misunderstood-rain-40</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://wandb.ai/neverparadise/apex_dqfd\" target=\"_blank\">https://wandb.ai/neverparadise/apex_dqfd</a><br/>\n",
       "                Run page: <a href=\"https://wandb.ai/neverparadise/apex_dqfd/runs/10g82hcs\" target=\"_blank\">https://wandb.ai/neverparadise/apex_dqfd/runs/10g82hcs</a><br/>\n",
       "                Run data is saved locally in <code>/home/kukjin/Study/RL/소프트웨어 융합 캡스톤 디자인/ApexDQFD/wandb/run-20210527_175146-10g82hcs</code><br/><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h1>Run(10g82hcs)</h1><iframe src=\"https://wandb.ai/neverparadise/apex_dqfd/runs/10g82hcs\" style=\"border:none;width:100%;height:400px\"></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x7f601c4c9350>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from subprocess import call\n",
    "wandb.init(project='apex_dqfd', entity='neverparadise')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#하이퍼 파라미터\n",
    "learning_rate = 0.0003\n",
    "gamma = 0.999\n",
    "buffer_limit = 50000\n",
    "L1 = 0.9\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def margin_loss(q_value, action, demo, weigths):\n",
    "    ae = F.one_hot(action, num_classes=19)\n",
    "    zero_indices = (ae == 0)\n",
    "    one_indices = (ae == 1)\n",
    "    ae[zero_indices] = 1\n",
    "    ae[one_indices] = 0\n",
    "    ae = ae.to(float)\n",
    "    max_value = torch.max(q_value + ae, axis=1)\n",
    "\n",
    "    ae = F.one_hot(action, num_classes=19)\n",
    "    ae = ae.to(float)\n",
    "\n",
    "    J_e = torch.abs(torch.sum(q_value * ae,axis=1) - max_value.values)\n",
    "    J_e = torch.mean(J_e * weigths * demo)\n",
    "    return J_e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from st import SumTree\n",
    "@ray.remote\n",
    "class Memory:  # stored as ( s, a, r, s_, n_rewards ) in SumTree\n",
    "    e = 0.01\n",
    "    a = 0.6\n",
    "    beta = 0.4\n",
    "    beta_increment_per_sampling = 0.001\n",
    "\n",
    "    def __init__(self, capacity):\n",
    "        self.tree = SumTree(capacity)\n",
    "        self.capacity = capacity\n",
    "\n",
    "    def _get_priority(self, error):\n",
    "        return (np.abs(error) + self.e) ** self.a\n",
    "\n",
    "    def add(self, error, sample):\n",
    "        p = self._get_priority(error)\n",
    "        self.tree.add(p, sample)\n",
    "\n",
    "    def size(self):\n",
    "        return self.tree.n_entries\n",
    "\n",
    "    def sample(self, n):\n",
    "        batch = []\n",
    "        idxs = []\n",
    "        segment = self.tree.total() / n\n",
    "        priorities = []\n",
    "\n",
    "        self.beta = np.min([1., self.beta + self.beta_increment_per_sampling])\n",
    "\n",
    "        for i in range(n):\n",
    "            a = segment * i\n",
    "            b = segment * (i + 1)\n",
    "\n",
    "            s = random.uniform(a, b)\n",
    "            (idx, p, data) = self.tree.get(s)\n",
    "            priorities.append(p)\n",
    "            batch.append(data)\n",
    "            idxs.append(idx)\n",
    "\n",
    "        sampling_probabilities = priorities / self.tree.total()\n",
    "        is_weight = np.power(self.tree.n_entries * sampling_probabilities, -self.beta)\n",
    "        is_weight /= is_weight.max() + 1e-5\n",
    "\n",
    "        return batch, idxs, is_weight\n",
    "\n",
    "    def update(self, idx, error):\n",
    "        p = self._get_priority(error)\n",
    "        self.tree.update(idx, p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_dqn(policy_net, target_net, demos, batch_size, demo_prob, optimizer):\n",
    "    demo_batch, idxs, is_weights = demos.sample.remote(batch_size)\n",
    "    # demo_batch = (batch_size, state, action, reward, next_state, done, n_rewards)\n",
    "    #print(len(demo_batch[0])) # 0번째 배치이므로 0이 나옴\n",
    "    state_list = []\n",
    "    action_list = []\n",
    "    reward_list =[]\n",
    "    next_state_list = []\n",
    "    done_mask_list = []\n",
    "    n_rewards_list = []\n",
    "\n",
    "    for transition in demo_batch:\n",
    "        s, a, r, s_prime, done_mask, n_rewards = transition\n",
    "        state_list.append(s)\n",
    "        action_list.append([a])\n",
    "        reward_list.append([r])\n",
    "        next_state_list.append(s_prime)\n",
    "        done_mask_list.append([done_mask])\n",
    "        n_rewards_list.append([n_rewards])\n",
    "\n",
    "    #a = state_list\n",
    "    #b = torch.tensor(action_list, dtype=torch.int64)\n",
    "    #c = torch.tensor(reward_list)\n",
    "    #d = next_state_list\n",
    "    #e = torch.tensor(done_mask_list)\n",
    "    #f = torch.tensor(n_rewards_list)\n",
    "\n",
    "    s = torch.stack(state_list).float().to(device)\n",
    "    a = torch.tensor(action_list, dtype=torch.int64).to(device)\n",
    "    r =  torch.tensor(reward_list).to(device)\n",
    "    s_prime = torch.stack(next_state_list).float().to(device)\n",
    "    done_mask = torch.tensor(done_mask_list).float().to(device)\n",
    "    nr =  torch.tensor(n_rewards_list).to(device)\n",
    "\n",
    "    q_vals = policy_net(s)\n",
    "    state_action_values = q_vals.gather(1, a)\n",
    "\n",
    "    # comparing the q values to the values expected using the next states and reward\n",
    "    next_state_values = target_net(s_prime).max(1)[0].unsqueeze(1)\n",
    "    target = r + (next_state_values * gamma)\n",
    "\n",
    "    # calculating the q loss, n-step return lossm supervised_loss\n",
    "    is_weights = torch.FloatTensor(is_weights).to(device)\n",
    "    q_loss = (is_weights * F.mse_loss(state_action_values, target)).mean()\n",
    "    n_step_loss = (state_action_values.max(1)[0] + nr).mean()\n",
    "    supervised_loss = margin_loss(q_vals, a, 1, 1)\n",
    "\n",
    "    loss = q_loss + supervised_loss + n_step_loss\n",
    "    wandb.log({\"Q-loss\" : q_loss.item()})\n",
    "    wandb.log({\"n-step loss\" : n_step_loss.item()})\n",
    "    wandb.log({\"super_vised loss\" : supervised_loss.item()})\n",
    "    wandb.log({\"total loss\" : loss.item()})\n",
    "    \n",
    "    errors = torch.abs(state_action_values - target).data.cpu()\n",
    "    errors = errors.numpy()\n",
    "    # update priority\n",
    "    for i in range(batch_size):\n",
    "        idx = idxs[i]\n",
    "        demos.update.remote(idx, errors[i])\n",
    "\n",
    "    # optimization step and logging\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    torch.nn.utils.clip_grad_norm(policy_net.parameters(), 100)\n",
    "    optimizer.step()\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def append_sample(memory, model, target_model, state, action, reward, next_state, done, n_rewards):\n",
    "    # Caluclating Priority (TD Error)\n",
    "    target = model(state.float()).data\n",
    "    old_val = target[0][action].cpu()\n",
    "    target_val = target_model(next_state.float()).data.cpu()\n",
    "    if done:\n",
    "        target[0][action] = reward\n",
    "    else:\n",
    "        target[0][action] = reward + 0.99 * torch.max(target_val)\n",
    "\n",
    "    error = abs(old_val - target[0][action])\n",
    "    error = error.cpu() \n",
    "    memory.add(error, [state, action, reward, next_state, done, n_rewards])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_train(env_name, rep_buffer, policy_net, target_net, optimizer,threshold=10, num_epochs=1, batch_size=16, seq_len=10, gamma=0.99):\n",
    "    data = minerl.data.make(env_name)\n",
    "    print(\"data loading sucess\")\n",
    "    demo_num = 0\n",
    "    for s_batch, a_batch, r_batch, ns_batch, d_batch in data.batch_iter(num_epochs=num_epochs, batch_size=batch_size,\n",
    "                                                                        seq_len=seq_len):\n",
    "        demo_num += 1\n",
    "        print(demo_num)\n",
    "        if r_batch.sum() < threshold:\n",
    "            continue\n",
    "        \"\"\"\n",
    "        state_batch : (batch_size, seq_len, 64, 64, 3)\n",
    "        action_batch : (batch_size, seq_len, action['property'].shape) ex camera = 2 otherwise 1\n",
    "\n",
    "        reward_batch : (batch_size, seq_len)\n",
    "        next_state_batch : (batch_size, seq_len, 64, 64, 3)\n",
    "        done_batch : (batch_size, seq_len)\n",
    "\n",
    "    \n",
    "        reward, _ = stats.mode(r_batch, axis=1)\n",
    "        reward = np.squeeze(reward)\n",
    "        done, _ = stats.mode(d_batch, axis=1)\n",
    "        done = np.squeeze(done)\n",
    "        \"\"\"\n",
    "        parse_ts = 0\n",
    "\n",
    "        # 각 state에 대한 action discretize를 위해 반복문을 사용\n",
    "        batch_length = (s_batch['pov'].shape)[0]  # (batch, seq, 64, 64, 3)[0]\n",
    "        for i in range(0, batch_length):\n",
    "            episode_start_ts = 0\n",
    "\n",
    "            n_step = 10\n",
    "            n_step_state_buffer = deque(maxlen=n_step)\n",
    "            n_step_action_buffer = deque(maxlen=n_step)\n",
    "            n_step_reward_buffer = deque(maxlen=n_step)\n",
    "            n_step_n_rewards_buffer = deque(maxlen=n_step)\n",
    "            n_step_next_state_buffer = deque(maxlen=n_step)\n",
    "            n_step_done_buffer = deque(maxlen=n_step)\n",
    "            gamma_list = [gamma ** i for i in range(n_step)]\n",
    "\n",
    "            for j in range(0, seq_len):\n",
    "                av = a_batch['attack'][i][j]  # attack value\n",
    "                aj = a_batch['jump'][i][j]  # jump value\n",
    "                af = a_batch['forward'][i][j]  # forward value\n",
    "                ab = a_batch['back'][i][j]  # back value\n",
    "                al = a_batch['left'][i][j]  # left value\n",
    "                ar = a_batch['right'][i][j]  # right value\n",
    "                va = a_batch['camera'][i][j][0]  # vertical angle and\n",
    "                ha = a_batch['camera'][i][j][1]  # horizontal angle\n",
    "\n",
    "                camera_thresholds = (abs(va) + abs(ha)) / 2.0\n",
    "                # 카메라를 움직이는 경우\n",
    "                if (camera_thresholds > 2.5):\n",
    "                    # camera = [0, -5]\n",
    "                    if abs(va) < abs(ha) and ha < 0:\n",
    "                        if av == 0:\n",
    "                            action_index = 0\n",
    "                        else:\n",
    "                            action_index = 1\n",
    "                    # camera = [0, 5]\n",
    "                    elif abs(va) < abs(ha) and ha > 0:\n",
    "                        if av == 0:\n",
    "                            action_index = 2\n",
    "                        else:\n",
    "                            action_index = 3\n",
    "                    # camera = [-5, 0]\n",
    "                    elif abs(va) > abs(ha) and ha < 0:\n",
    "                        if av == 0:\n",
    "                            action_index = 4\n",
    "                        else:\n",
    "                            action_index = 5\n",
    "                    # camera = [5, 0]\n",
    "                    elif abs(va) > abs(ha) and ha > 0:\n",
    "                        if av == 0:\n",
    "                            action_index = 6\n",
    "                        else:\n",
    "                            action_index = 7\n",
    "\n",
    "                            # 카메라를 안움직이는 경우\n",
    "                # 점프하는 경우\n",
    "                elif (aj == 1):\n",
    "                    if (af == 0):\n",
    "                        action_index = 8\n",
    "                    else:\n",
    "                        action_index = 9\n",
    "\n",
    "                # 앞으로 가는 경우\n",
    "                elif (af == 1):\n",
    "                    if (av == 0):\n",
    "                        action_index = 10\n",
    "                    else:\n",
    "                        action_index = 11\n",
    "\n",
    "                # 뒤로 가는 경우\n",
    "                elif (ab == 1):\n",
    "                    if (av == 0):\n",
    "                        action_index = 12\n",
    "                    else:\n",
    "                        action_index = 13\n",
    "\n",
    "                # 왼쪽으로 가는 경우\n",
    "                elif (al == 1):\n",
    "                    if (av == 0):\n",
    "                        action_index = 14\n",
    "                    else:\n",
    "                        action_index = 15\n",
    "\n",
    "                # 오른쪽으로 가는 경우\n",
    "                elif (ar == 1):\n",
    "                    if (av == 0):\n",
    "                        action_index = 16\n",
    "                    else:\n",
    "                        action_index = 17\n",
    "\n",
    "                # 카메라, 움직임이 다 0이고 공격만 하는 것\n",
    "                else:\n",
    "                    if (av == 0):\n",
    "                        continue\n",
    "                    else:\n",
    "                        action_index = 18\n",
    "\n",
    "                a_index = torch.LongTensor([action_index]).cpu()\n",
    "                curr_obs = converter2(s_batch['pov'][i][j]).float().cpu()\n",
    "                _obs = converter2(ns_batch['pov'][i][j]).float().cpu()\n",
    "                _reward = torch.FloatTensor([r_batch[i][j]]).cpu()\n",
    "                _done = d_batch[i][j]  # .astype(int)\n",
    "\n",
    "                n_step_state_buffer.append(curr_obs)\n",
    "                n_step_action_buffer.append(a_index)\n",
    "                n_step_reward_buffer.append(_reward)\n",
    "                n_step_next_state_buffer.append(_obs)\n",
    "                n_step_done_buffer.append(_done)\n",
    "                n_rewards = sum([gamma * reward for gamma, reward in zip(gamma_list, n_step_reward_buffer)])\n",
    "                n_step_n_rewards_buffer.append(n_rewards)\n",
    "                \n",
    "\n",
    "                append_sample(rep_buffer, policy_net, target_net, n_step_state_buffer[j], \\\n",
    "                              n_step_action_buffer[j], n_step_reward_buffer[j], \\\n",
    "                              n_step_next_state_buffer[j], \\\n",
    "                              n_step_done_buffer[j], \\\n",
    "                              n_step_n_rewards_buffer[j])\n",
    "                episode_start_ts += 1\n",
    "                parse_ts += 1\n",
    "                # if episode done we reset\n",
    "                if _done:\n",
    "                    break\n",
    "\n",
    "        # replay is over emptying the deques\n",
    "        #if rep_buffer.size() > rep_buffer.buffer_limit:\n",
    "        #    rep_buffer.buffer.popleft()\n",
    "        print('Parse finished. {} expert samples added.'.format(parse_ts))\n",
    "        train_dqn(policy_net, target_net, rep_buffer, batch_size, 1, optimizer)\n",
    "        torch.save(policy_net.state_dict(), model_path + 'pre_trained3.pth')        \n",
    "        if demo_num % 5 == 0 and demo_num != 0:\n",
    "        # 특정 반복 수가 되면 타겟 네트워크도 업데이트\n",
    "            print(\"target network updated\")\n",
    "            target_net.load_state_dict(policy_net.state_dict())\n",
    "        print(\"train {} step finished\".format(demo_num))\n",
    "    print('pre_train finished')\n",
    "    return rep_buffer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_episodes = 1000\n",
    "startEpsilon = 1.0\n",
    "endEpsilon = 0.05\n",
    "epsilon = startEpsilon\n",
    "\n",
    "root_path = os.curdir\n",
    "model_path = root_path + '/dqn_model/'\n",
    "\n",
    "stepDrop = (startEpsilon - endEpsilon) / total_episodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "@ray.remote(num_cpus=6)\n",
    "class Actor:\n",
    "    def __init__(self, shared_network_cpu, actor_idx, epsilon):\n",
    "        # environment initialization\n",
    "        import gym\n",
    "        import minerl\n",
    "        self.actor_idx = actor_idx\n",
    "        self.env = gym.make(\"MineRLTreechop-v0\")\n",
    "        self.port_number = int(\"12340\")+actor_idx\n",
    "        print(\"actor environment %d initialize successfully\" % self.actor_idx)\n",
    "        \n",
    "        #self.shared_memory = ray.get(shared_memory_id)\n",
    "        #print(\"shared memory assign successfully\")\n",
    "        \n",
    "        # network initalization\n",
    "        self.actor_network = DQN(19).cpu()\n",
    "        self.actor_target_network = DQN(19).cpu()\n",
    "        self.actor_network.load_state_dict(shared_network_cpu.state_dict())\n",
    "        self.actor_target_network.load_state_dict(self.actor_network.state_dict())\n",
    "        print(\"actor network %d initialize successfully\" % self.actor_idx)\n",
    "        \n",
    "        # exploring info\n",
    "        self.epsilon = epsilon\n",
    "        self.max_step = 10\n",
    "        self.local_buffer_size = 100\n",
    "        self.local_buffer = deque(maxlen=self.local_buffer_size)\n",
    "        \n",
    "    # 1. 네트워크 파라미터 복사\n",
    "    # 2. 환경 탐험 (초기화, 행동)\n",
    "    # 3. 로컬버퍼에 저장\n",
    "    # 4. priority 계산\n",
    "    # 5. 글로벌 버퍼에 저장\n",
    "    # 6. 주기적으로 네트워크 업데이트 \n",
    "    \n",
    "    # 각 환경 인스턴스에서 각 엡실론에 따라 탐험을 진행한다.\n",
    "    # 탐험 과정에서 local buffer에 transition들을 저장한다.\n",
    "    # local buffer의 개수가 특정 개수 이상이면 global buffer에 추가해준다. \n",
    "    def make_inveractive(self):\n",
    "        self.env.make_interactive(port=self.port_number, realtime=False)\n",
    "        \n",
    "    def explore(self, shared_network, shared_memory):\n",
    "        self.env.make_interactive(port=self.port_number, realtime=False)\n",
    "        for num_epi in range(self.max_step):\n",
    "            obs = self.env.reset()\n",
    "            state = converter(obs).cpu()\n",
    "            state = state.float()\n",
    "            done = False\n",
    "            total_reward = 0\n",
    "            steps = 0\n",
    "            total_steps = 0\n",
    "            self.epsilon = 0.5\n",
    "            if(self.epsilon > endEpsilon):\n",
    "                self.epsilon -= stepDrop / (self.actor_idx + 1)\n",
    "            \n",
    "            n_step = 10\n",
    "            n_step_state_buffer = deque(maxlen=n_step)\n",
    "            n_step_action_buffer = deque(maxlen=n_step)\n",
    "            n_step_reward_buffer = deque(maxlen=n_step)\n",
    "            n_step_n_rewards_buffer = deque(maxlen=n_step)\n",
    "            n_step_next_state_buffer = deque(maxlen=n_step)\n",
    "            n_step_done_buffer = deque(maxlen=n_step)\n",
    "            gamma_list = [gamma ** i for i in range(n_step)]\n",
    "            \n",
    "            while not done:\n",
    "                steps += 1\n",
    "                total_steps += 1\n",
    "                a_out = self.actor_network.sample_action(state,self.epsilon)\n",
    "                action_index = a_out\n",
    "                action = make_action(self.env, action_index)\n",
    "                action['attack'] = 1\n",
    "                obs_prime, reward, done, info = self.env.step(action)\n",
    "                total_reward += reward\n",
    "                state_prime = converter(obs_prime)\n",
    "                \n",
    "                # local buffer add\n",
    "                n_step_state_buffer.append(state)\n",
    "                n_step_action_buffer.append(action_index)\n",
    "                n_step_reward_buffer.append(reward)\n",
    "                n_step_next_state_buffer.append(state_prime)\n",
    "                n_step_done_buffer.append(done)\n",
    "                n_rewards = sum([gamma * reward for gamma, reward in zip(gamma_list, n_step_reward_buffer)])\n",
    "                n_step_n_rewards_buffer.append(n_rewards)\n",
    "                \n",
    "                if (len(n_step_state_buffer) >= n_step):\n",
    "                    # LocalBuffer Get\n",
    "                    # Compute Priorities\n",
    "                    for i in range(n_step):\n",
    "                        self.append_sample(shared_memory, self.actor_network , self.actor_target_network, \\\n",
    "                              n_step_state_buffer[i], \\\n",
    "                              n_step_action_buffer[i], n_step_reward_buffer[i], \\\n",
    "                              n_step_next_state_buffer[i], \\\n",
    "                              n_step_done_buffer[i], \\\n",
    "                              n_step_n_rewards_buffer[i])\n",
    "                        if(n_step_done_buffer[i]):\n",
    "                            break\n",
    "                print((shared_memory.size()))                \n",
    "                \n",
    "                    \n",
    "                state = state_prime.float().cpu()\n",
    "                if done:\n",
    "                    break\n",
    "\n",
    "            if done:\n",
    "                print(\"%d episode is done\" % num_epi)\n",
    "                print(\"total rewards : %d \" % total_reward)\n",
    "            \n",
    "            if (num_epi % 5 == 0 and num_epi != 0):\n",
    "                self.update_params(shared_network)\n",
    "                print(\"actor network is updated \")\n",
    "                \n",
    "    def env_close(self):\n",
    "        self.env.close()\n",
    "\n",
    "    def update_params(self, shared_network):\n",
    "        self.actor_network.load_state_dict(shared_network.state_dict())\n",
    "        \n",
    "    def append_sample(self, memory, model, target_model, state, action, reward, next_state, done, n_rewards):\n",
    "        # Caluclating Priority (TD Error)\n",
    "        target = model(state.float()).data\n",
    "        old_val = target[0][action].cpu()\n",
    "        target_val = target_model(next_state.float()).data.cpu()\n",
    "        if done:\n",
    "            target[0][action] = reward\n",
    "        else:\n",
    "            target[0][action] = reward + 0.99 * torch.max(target_val)\n",
    "\n",
    "        error = abs(old_val - target[0][action])\n",
    "        error = error.cpu() \n",
    "        memory.add(error, [state, action, reward, next_state, done, n_rewards])\n",
    "\n",
    "\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Learner:\n",
    "    def __init__(self, network, batch_size):\n",
    "        self.learner_network = DQN(19).to(device)\n",
    "        self.learner_target_network = DQN(19).to(device)\n",
    "        \n",
    "        self.learner_network.load_state_dict(network.state_dict())\n",
    "        self.learner_target_network.load_state_dict(network.state_dict())\n",
    "        \n",
    "        self.batch_size = batch_size\n",
    "    # 1. sampling\n",
    "    # 2. calculate gradient\n",
    "    # 3. weight update\n",
    "    # 4. compute priorities\n",
    "    # 5. priorities of buffer update\n",
    "    # 6. remove old memory \n",
    "    def get_network(self):\n",
    "        return self.learner_network\n",
    "\n",
    "    def update_network(memory, demos, batch_size, demo_prob, optimizer):\n",
    "        \n",
    "        agent_batch, agent_idxs, agent_weights = memory.sample(batch_size)\n",
    "        demo_batch, demo_idxs, demo_weights = demos.sample(batch_size)\n",
    "        \n",
    "        # demo_batch = (batch_size, state, action, reward, next_state, done, n_rewards)\n",
    "        #print(len(demo_batch[0])) # 0번째 배치이므로 0이 나옴\n",
    "        state_list = []\n",
    "        action_list = []\n",
    "        reward_list =[]\n",
    "        next_state_list = []\n",
    "        done_mask_list = []\n",
    "        n_rewards_list = []\n",
    "\n",
    "        for agent_transition, expert_transition in zip(agent_batch, demo_batch):\n",
    "            s, a, r, s_prime, done_mask, n_rewards = agent_transition\n",
    "            state_list.append(s)\n",
    "            action_list.append([a])\n",
    "            reward_list.append([r])\n",
    "            next_state_list.append(s_prime)\n",
    "            done_mask_list.append([done_mask])\n",
    "            n_rewards_list.append([n_rewards])\n",
    "            \n",
    "            s, a, r, s_prime, done_mask, n_rewards = expert_transition\n",
    "            state_list.append(s)\n",
    "            action_list.append([a])\n",
    "            reward_list.append([r])\n",
    "            next_state_list.append(s_prime)\n",
    "            done_mask_list.append([done_mask])\n",
    "            n_rewards_list.append([n_rewards])\n",
    "            \n",
    "\n",
    "        s = torch.stack(state_list).float().to(device)\n",
    "        a = torch.tensor(action_list, dtype=torch.int64).to(device)\n",
    "        r =  torch.tensor(reward_list).to(device)\n",
    "        s_prime = torch.stack(next_state_list).float().to(device)\n",
    "        done_mask = torch.tensor(done_mask_list).float().to(device)\n",
    "        nr =  torch.tensor(n_rewards_list).to(device)\n",
    "        \n",
    "        q_vals = policy_net(s)\n",
    "        state_action_values = q_vals.gather(1, a)\n",
    "\n",
    "        # comparing the q values to the values expected using the next states and reward\n",
    "        next_state_values = target_net(s_prime).max(1)[0].unsqueeze(1)\n",
    "        target = r + (next_state_values * gamma)\n",
    "\n",
    "        # calculating the q loss, n-step return lossm supervised_loss\n",
    "        is_weights = torch.FloatTensor(is_weights).to(device)\n",
    "        q_loss = (is_weights * F.mse_loss(state_action_values, target)).mean()\n",
    "        n_step_loss = (state_action_values.max(1)[0] + nr).mean()\n",
    "        supervised_loss = margin_loss(q_vals, a, 1, 1)\n",
    "\n",
    "        loss = q_loss + supervised_loss + n_step_loss\n",
    "        wandb.log({\"Q-loss\" : q_loss.item()})\n",
    "        wandb.log({\"n-step loss\" : n_step_loss.item()})\n",
    "        wandb.log({\"super_vised loss\" : supervised_loss.item()})\n",
    "        wandb.log({\"total loss\" : loss.item()})\n",
    "\n",
    "        errors = torch.abs(state_action_values - target).data.cpu()\n",
    "        errors = errors.numpy()\n",
    "        # update priority\n",
    "        for i in range(batch_size):\n",
    "            idx = idxs[i]\n",
    "            memory.remote.update(idx, errors[i])\n",
    "\n",
    "        # optimization step and logging\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm(policy_net.parameters(), 100)\n",
    "        optimizer.step()\n",
    "        return loss\n",
    "    \n",
    "    def update_target_networks(self):\n",
    "        self.learner_target_network.load_state_dict(self.learner_network.state_dict())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-05-27 02:30:14,735\tINFO services.py:1269 -- View the Ray dashboard at \u001b[1m\u001b[32mhttp://127.0.0.1:8265\u001b[39m\u001b[22m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'node_ip_address': '192.168.0.22',\n",
       " 'raylet_ip_address': '192.168.0.22',\n",
       " 'redis_address': '192.168.0.22:54860',\n",
       " 'object_store_address': '/tmp/ray/session_2021-05-27_02-30-13_854772_36645/sockets/plasma_store',\n",
       " 'raylet_socket_name': '/tmp/ray/session_2021-05-27_02-30-13_854772_36645/sockets/raylet',\n",
       " 'webui_url': '127.0.0.1:8265',\n",
       " 'session_dir': '/tmp/ray/session_2021-05-27_02-30-13_854772_36645',\n",
       " 'metrics_export_port': 59064,\n",
       " 'node_id': 'a1643c12189b9baaa2cf914ad72c90c3dc61dc2f886d70c2f30a7a94'}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ray.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy_net = DQN(19).to(device=device)\n",
    "target_net = DQN(19).to(device=device)\n",
    "target_net.load_state_dict(policy_net.state_dict())\n",
    "memory = Memory(50000)\n",
    "demos = Memory(50000)\n",
    "optimizer = optim.Adam(policy_net.parameters(), lr=learning_rate, weight_decay=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#print(\"pre_train start\")\n",
    "#demos = pre_train(\"MineRLTreechop-v0\", demos, policy_net, target_net, optimizer, threshold=60, num_epochs=10, batch_size=256, seq_len=10, gamma=0.99)\n",
    "#print(\"pre_train finished\")\n",
    "#print(demos.size.remote())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Copy network params from pretrained Agent\n",
    "model_path = './dqn_model/pre_trained.pth'\n",
    "policy_net.load_state_dict(torch.load(model_path, map_location='cuda:0'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_network(shared_network_id):\n",
    "    shared_network = ray.get(shared_network_id).get_network()\n",
    "    return shared_network.cpu()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Generating each own instances\n",
    "# main()\n",
    "num_actors = 2\n",
    "epsilon = 0.5\n",
    "\n",
    "# learner network initialzation\n",
    "learner = Learner(policy_net, 64)\n",
    "\n",
    "# shared network\n",
    "learner_network_id = ray.put(learner)\n",
    "\n",
    "\n",
    "# actor network, environments initialization\n",
    "actor_list = [Actor.remote(get_network(learner_network_id), i, 0.5) for i in range(num_actors)]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=36868)\u001b[0m /home/kukjin/anaconda3/envs/minerl/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=36868)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n",
      "\u001b[2m\u001b[36m(pid=36871)\u001b[0m /home/kukjin/anaconda3/envs/minerl/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=36871)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    }
   ],
   "source": [
    "batch_size = 64\n",
    "demo_prob = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actor_list[0].make_inveractive.remote()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory_id = ray.put(memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=36871)\u001b[0m MineRL agent is public, connect on port 12340 with Minecraft 1.11\n",
      "\u001b[2m\u001b[36m(pid=36871)\u001b[0m Traceback (most recent call last):\n",
      "\u001b[2m\u001b[36m(pid=36871)\u001b[0m   File \"python/ray/_raylet.pyx\", line 488, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(pid=36871)\u001b[0m   File \"python/ray/_raylet.pyx\", line 495, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(pid=36871)\u001b[0m   File \"python/ray/_raylet.pyx\", line 505, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(pid=36871)\u001b[0m   File \"python/ray/_raylet.pyx\", line 449, in ray._raylet.execute_task.function_executor\n",
      "\u001b[2m\u001b[36m(pid=36871)\u001b[0m   File \"/home/kukjin/anaconda3/envs/minerl/lib/python3.7/site-packages/ray/_private/function_manager.py\", line 556, in actor_method_executor\n",
      "\u001b[2m\u001b[36m(pid=36871)\u001b[0m     return method(__ray_actor, *args, **kwargs)\n",
      "\u001b[2m\u001b[36m(pid=36871)\u001b[0m   File \"<ipython-input-10-d1758773fc4d>\", line 93, in explore\n",
      "\u001b[2m\u001b[36m(pid=36871)\u001b[0m   File \"<ipython-input-10-d1758773fc4d>\", line 129, in append_sample\n",
      "\u001b[2m\u001b[36m(pid=36871)\u001b[0m   File \"<ipython-input-6-36415015b796>\", line 17, in add\n",
      "\u001b[2m\u001b[36m(pid=36871)\u001b[0m   File \"/home/kukjin/Study/RL/소프트웨어 융합 캡스톤 디자인/ApexDQFD/st.py\", line 60, in add\n",
      "\u001b[2m\u001b[36m(pid=36871)\u001b[0m     self.update(idx, p)\n",
      "\u001b[2m\u001b[36m(pid=36871)\u001b[0m   File \"/home/kukjin/Study/RL/소프트웨어 융합 캡스톤 디자인/ApexDQFD/st.py\", line 74, in update\n",
      "\u001b[2m\u001b[36m(pid=36871)\u001b[0m     self.tree[idx] = p\n",
      "\u001b[2m\u001b[36m(pid=36871)\u001b[0m ValueError: assignment destination is read-only\n",
      "\u001b[2m\u001b[36m(pid=36871)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=36871)\u001b[0m During handling of the above exception, another exception occurred:\n",
      "\u001b[2m\u001b[36m(pid=36871)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=36871)\u001b[0m Traceback (most recent call last):\n",
      "\u001b[2m\u001b[36m(pid=36871)\u001b[0m   File \"python/ray/_raylet.pyx\", line 599, in ray._raylet.task_execution_handler\n",
      "\u001b[2m\u001b[36m(pid=36871)\u001b[0m   File \"python/ray/_raylet.pyx\", line 451, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(pid=36871)\u001b[0m   File \"python/ray/_raylet.pyx\", line 556, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(pid=36871)\u001b[0m   File \"/home/kukjin/anaconda3/envs/minerl/lib/python3.7/site-packages/ray/_private/utils.py\", line 102, in push_error_to_driver\n",
      "\u001b[2m\u001b[36m(pid=36871)\u001b[0m     worker.core_worker.push_error(job_id, error_type, message, time.time())\n",
      "\u001b[2m\u001b[36m(pid=36871)\u001b[0m   File \"python/ray/_raylet.pyx\", line 1682, in ray._raylet.CoreWorker.push_error\n",
      "\u001b[2m\u001b[36m(pid=36871)\u001b[0m UnicodeEncodeError: 'ascii' codec can't encode characters in position 627-631: ordinal not in range(128)\n",
      "\u001b[2m\u001b[36m(pid=36871)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=36871)\u001b[0m During handling of the above exception, another exception occurred:\n",
      "\u001b[2m\u001b[36m(pid=36871)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=36871)\u001b[0m Traceback (most recent call last):\n",
      "\u001b[2m\u001b[36m(pid=36871)\u001b[0m   File \"/home/kukjin/anaconda3/envs/minerl/lib/python3.7/site-packages/ray/_private/utils.py\", line 102, in push_error_to_driver\n",
      "\u001b[2m\u001b[36m(pid=36871)\u001b[0m     worker.core_worker.push_error(job_id, error_type, message, time.time())\n",
      "\u001b[2m\u001b[36m(pid=36871)\u001b[0m   File \"python/ray/_raylet.pyx\", line 1682, in ray._raylet.CoreWorker.push_error\n",
      "\u001b[2m\u001b[36m(pid=36871)\u001b[0m UnicodeEncodeError: 'ascii' codec can't encode characters in position 744-748: ordinal not in range(128)\n",
      "\u001b[2m\u001b[36m(pid=36871)\u001b[0m Exception ignored in: 'ray._raylet.task_execution_handler'\n",
      "\u001b[2m\u001b[36m(pid=36871)\u001b[0m Traceback (most recent call last):\n",
      "\u001b[2m\u001b[36m(pid=36871)\u001b[0m   File \"/home/kukjin/anaconda3/envs/minerl/lib/python3.7/site-packages/ray/_private/utils.py\", line 102, in push_error_to_driver\n",
      "\u001b[2m\u001b[36m(pid=36871)\u001b[0m     worker.core_worker.push_error(job_id, error_type, message, time.time())\n",
      "\u001b[2m\u001b[36m(pid=36871)\u001b[0m   File \"python/ray/_raylet.pyx\", line 1682, in ray._raylet.CoreWorker.push_error\n",
      "\u001b[2m\u001b[36m(pid=36871)\u001b[0m UnicodeEncodeError: 'ascii' codec can't encode characters in position 744-748: ordinal not in range(128)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=36871)\u001b[0m 0\n",
      "\u001b[2m\u001b[36m(pid=36871)\u001b[0m 0\n",
      "\u001b[2m\u001b[36m(pid=36871)\u001b[0m 0\n",
      "\u001b[2m\u001b[36m(pid=36871)\u001b[0m 0\n",
      "\u001b[2m\u001b[36m(pid=36871)\u001b[0m 0\n",
      "\u001b[2m\u001b[36m(pid=36871)\u001b[0m 0\n",
      "\u001b[2m\u001b[36m(pid=36871)\u001b[0m 0\n",
      "\u001b[2m\u001b[36m(pid=36871)\u001b[0m 0\n",
      "\u001b[2m\u001b[36m(pid=36871)\u001b[0m 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=36868)\u001b[0m MineRL agent is public, connect on port 12341 with Minecraft 1.11\n",
      "\u001b[2m\u001b[36m(pid=36868)\u001b[0m Traceback (most recent call last):\n",
      "\u001b[2m\u001b[36m(pid=36868)\u001b[0m   File \"python/ray/_raylet.pyx\", line 488, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(pid=36868)\u001b[0m   File \"python/ray/_raylet.pyx\", line 495, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(pid=36868)\u001b[0m   File \"python/ray/_raylet.pyx\", line 505, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(pid=36868)\u001b[0m   File \"python/ray/_raylet.pyx\", line 449, in ray._raylet.execute_task.function_executor\n",
      "\u001b[2m\u001b[36m(pid=36868)\u001b[0m   File \"/home/kukjin/anaconda3/envs/minerl/lib/python3.7/site-packages/ray/_private/function_manager.py\", line 556, in actor_method_executor\n",
      "\u001b[2m\u001b[36m(pid=36868)\u001b[0m     return method(__ray_actor, *args, **kwargs)\n",
      "\u001b[2m\u001b[36m(pid=36868)\u001b[0m   File \"<ipython-input-10-d1758773fc4d>\", line 93, in explore\n",
      "\u001b[2m\u001b[36m(pid=36868)\u001b[0m   File \"<ipython-input-10-d1758773fc4d>\", line 129, in append_sample\n",
      "\u001b[2m\u001b[36m(pid=36868)\u001b[0m   File \"<ipython-input-6-36415015b796>\", line 17, in add\n",
      "\u001b[2m\u001b[36m(pid=36868)\u001b[0m   File \"/home/kukjin/Study/RL/소프트웨어 융합 캡스톤 디자인/ApexDQFD/st.py\", line 60, in add\n",
      "\u001b[2m\u001b[36m(pid=36868)\u001b[0m     self.update(idx, p)\n",
      "\u001b[2m\u001b[36m(pid=36868)\u001b[0m   File \"/home/kukjin/Study/RL/소프트웨어 융합 캡스톤 디자인/ApexDQFD/st.py\", line 74, in update\n",
      "\u001b[2m\u001b[36m(pid=36868)\u001b[0m     self.tree[idx] = p\n",
      "\u001b[2m\u001b[36m(pid=36868)\u001b[0m ValueError: assignment destination is read-only\n",
      "\u001b[2m\u001b[36m(pid=36868)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=36868)\u001b[0m During handling of the above exception, another exception occurred:\n",
      "\u001b[2m\u001b[36m(pid=36868)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=36868)\u001b[0m Traceback (most recent call last):\n",
      "\u001b[2m\u001b[36m(pid=36868)\u001b[0m   File \"python/ray/_raylet.pyx\", line 599, in ray._raylet.task_execution_handler\n",
      "\u001b[2m\u001b[36m(pid=36868)\u001b[0m   File \"python/ray/_raylet.pyx\", line 451, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(pid=36868)\u001b[0m   File \"python/ray/_raylet.pyx\", line 556, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(pid=36868)\u001b[0m   File \"/home/kukjin/anaconda3/envs/minerl/lib/python3.7/site-packages/ray/_private/utils.py\", line 102, in push_error_to_driver\n",
      "\u001b[2m\u001b[36m(pid=36868)\u001b[0m     worker.core_worker.push_error(job_id, error_type, message, time.time())\n",
      "\u001b[2m\u001b[36m(pid=36868)\u001b[0m   File \"python/ray/_raylet.pyx\", line 1682, in ray._raylet.CoreWorker.push_error\n",
      "\u001b[2m\u001b[36m(pid=36868)\u001b[0m UnicodeEncodeError: 'ascii' codec can't encode characters in position 627-631: ordinal not in range(128)\n",
      "\u001b[2m\u001b[36m(pid=36868)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=36868)\u001b[0m During handling of the above exception, another exception occurred:\n",
      "\u001b[2m\u001b[36m(pid=36868)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=36868)\u001b[0m Traceback (most recent call last):\n",
      "\u001b[2m\u001b[36m(pid=36868)\u001b[0m   File \"/home/kukjin/anaconda3/envs/minerl/lib/python3.7/site-packages/ray/_private/utils.py\", line 102, in push_error_to_driver\n",
      "\u001b[2m\u001b[36m(pid=36868)\u001b[0m     worker.core_worker.push_error(job_id, error_type, message, time.time())\n",
      "\u001b[2m\u001b[36m(pid=36868)\u001b[0m   File \"python/ray/_raylet.pyx\", line 1682, in ray._raylet.CoreWorker.push_error\n",
      "\u001b[2m\u001b[36m(pid=36868)\u001b[0m UnicodeEncodeError: 'ascii' codec can't encode characters in position 744-748: ordinal not in range(128)\n",
      "\u001b[2m\u001b[36m(pid=36868)\u001b[0m Exception ignored in: 'ray._raylet.task_execution_handler'\n",
      "\u001b[2m\u001b[36m(pid=36868)\u001b[0m Traceback (most recent call last):\n",
      "\u001b[2m\u001b[36m(pid=36868)\u001b[0m   File \"/home/kukjin/anaconda3/envs/minerl/lib/python3.7/site-packages/ray/_private/utils.py\", line 102, in push_error_to_driver\n",
      "\u001b[2m\u001b[36m(pid=36868)\u001b[0m     worker.core_worker.push_error(job_id, error_type, message, time.time())\n",
      "\u001b[2m\u001b[36m(pid=36868)\u001b[0m   File \"python/ray/_raylet.pyx\", line 1682, in ray._raylet.CoreWorker.push_error\n",
      "\u001b[2m\u001b[36m(pid=36868)\u001b[0m UnicodeEncodeError: 'ascii' codec can't encode characters in position 744-748: ordinal not in range(128)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=36868)\u001b[0m 0\n",
      "\u001b[2m\u001b[36m(pid=36868)\u001b[0m 0\n",
      "\u001b[2m\u001b[36m(pid=36868)\u001b[0m 0\n",
      "\u001b[2m\u001b[36m(pid=36868)\u001b[0m 0\n",
      "\u001b[2m\u001b[36m(pid=36868)\u001b[0m 0\n",
      "\u001b[2m\u001b[36m(pid=36868)\u001b[0m 0\n",
      "\u001b[2m\u001b[36m(pid=36868)\u001b[0m 0\n",
      "\u001b[2m\u001b[36m(pid=36868)\u001b[0m 0\n",
      "\u001b[2m\u001b[36m(pid=36868)\u001b[0m 0\n"
     ]
    }
   ],
   "source": [
    "explore = [actor.explore.remote(get_network(learner_network_id), ray.get(memory_id)) for actor in actor_list]\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    for i in range(1000):\n",
    "        if(ray.get(memory.size.remote()) > 1000):\n",
    "            learner.update_network(memory, demos, batch_size, demo_prob, optimizer)\n",
    "            print(\"learner network updated\")\n",
    "\n",
    "        if (i % 5 == 0 and i != 0):\n",
    "            learner.update_target_networks()\n",
    "            print(\"learner target network updated\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ray.shutdown()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_list = []\n",
    "\n",
    "def make_envs(number):\n",
    "    for i in range(number):\n",
    "        env = gym.make(\"MineRLTreechop-v0\")\n",
    "        port_number = int(\"12340\")+i\n",
    "        env.make_interactive(port=port_number, realtime=False)\n",
    "        env_list.append(env)\n",
    "\n",
    "make_envs(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ray.util.register_serializer(env_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@ray.remote\n",
    "def action(env):\n",
    "    env.reset()\n",
    "    for i in range(1000):\n",
    "        action = env.action_space.sample()\n",
    "        env.step(action)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_id_list = [ray.put(env) for env in env_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "action_list = [action.remote(env) for env in env_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_list[0].reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10000):\n",
    "    action = env_list[0].action_space.sample()    \n",
    "    env_list[0].step(action)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
