{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "annual-printing",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from model import DQN\n",
    "import os\n",
    "import minerl\n",
    "import gym\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import ray\n",
    "import pandas as pd\n",
    "import gc\n",
    "import asyncio\n",
    "from _collections import deque\n",
    "from utils import *\n",
    "import random\n",
    "import copy\n",
    "\n",
    "\n",
    "# learner에 샘플을 추가한다. \n",
    "# \n",
    "def learner_append_sample(memory, model, target_model, state, action, reward, next_state, done):\n",
    "    # Caluclating Priority (TD Error)\n",
    "    target = model(state.float()).data.cpu()\n",
    "    old_val = target[0][action].cpu()\n",
    "    target_val = target_model(next_state.float()).data.cpu()\n",
    "    if done:\n",
    "        target[0][action] = reward\n",
    "    else:\n",
    "        target[0][action] = reward + 0.99 * torch.max(target_val)\n",
    "\n",
    "    error = abs(old_val - target[0][action])\n",
    "    error = error.cpu()\n",
    "    memory.add.remote(error, [state, action, reward, next_state, done])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b768282f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "a = np.array(([1, 1, 1]))\n",
    "c = np.power(a, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "54d6e1a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.33333333, 0.33333333, 0.33333333])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = a * 1/3\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5a47a6f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.11111111 0.11111111 0.11111111]\n"
     ]
    }
   ],
   "source": [
    "c = np.power(b, 2)\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "express-reservation",
   "metadata": {},
   "outputs": [],
   "source": [
    "@ray.remote\n",
    "class Actor:\n",
    "    def __init__(self, learner, actor_idx, startEpsilon, endEpsilon, paramServer):\n",
    "        # environment initialization\n",
    "        self.actor_idx = actor_idx\n",
    "        self.env = gym.make(\"MineRLTreechop-v0\")\n",
    "        self.port_number = int(\"12340\") + actor_idx\n",
    "        print(\"actor environment %d initialize successfully\" % self.actor_idx)\n",
    "        self.env.make_interactive(port=self.port_number, realtime=False)\n",
    "        self.shared_network_cpu = ray.get(learner.get_network.remote())\n",
    "        # self.shared_memory = ray.get(shared_memory_id)\n",
    "        # print(\"shared memory assign successfully\")\n",
    "        \n",
    "        # network initalization\n",
    "        self.actor_network = DQN(19).cpu()\n",
    "        self.actor_target_network = DQN(19).cpu()\n",
    "        self.actor_network.load_state_dict(self.shared_network_cpu.state_dict())\n",
    "        self.actor_target_network.load_state_dict(self.actor_network.state_dict())\n",
    "        print(\"actor network %d initialize successfully\" % self.actor_idx)\n",
    "\n",
    "        self.initialized = False\n",
    "        self.epi_counter = 0\n",
    "        # exploring info\n",
    "        self.startEpsilon = startEpsilon\n",
    "        self.endEpsilon = endEpsilon\n",
    "        self.max_episodes = 1000\n",
    "\n",
    "        self.paramServer = paramServer\n",
    "        \n",
    "    \n",
    "    # 1. 네트워크 파라미터 복사\n",
    "    # 2. 환경 탐험 (초기화, 행동)\n",
    "    # 3. 로컬버퍼에 저장\n",
    "    # 4. priority 계산\n",
    "    # 5. 글로벌 버퍼에 저장\n",
    "    # 6. 주기적으로 네트워크 업데이트\n",
    "\n",
    "    def get_initialized(self):\n",
    "        return self.initialized\n",
    "\n",
    "    def get_counter(self):\n",
    "        return self.epi_counter\n",
    "\n",
    "    # 각 환경 인스턴스에서 각 엡실론에 따라 탐험을 진행한다.\n",
    "    # 탐험 과정에서 local buffer에 transition들을 저장한다.\n",
    "    # local buffer의 개수가 특정 개수 이상이면 global buffer에 추가해준다.\n",
    "\n",
    "    def explore(self, learner, shared_memory):\n",
    "        \n",
    "        self.initialized = True\n",
    "        stepDrop = (self.startEpsilon - self.endEpsilon) / self.max_episodes\n",
    "        epsilon = self.startEpsilon\n",
    "        total_steps = 0\n",
    "        \n",
    "        episodes = [x for x in range(self.max_episodes)]\n",
    "        train_stats = pd.DataFrame(index=episodes, columns=['rewards'])\n",
    "        \n",
    "        for num_epi in range(self.max_episodes):\n",
    "            obs = self.env.reset()\n",
    "            state = converter(obs).cpu().float()\n",
    "            done = False\n",
    "            total_reward = 0\n",
    "            steps = 0\n",
    "            if (epsilon > self.endEpsilon):\n",
    "                epsilon -= stepDrop\n",
    "                \n",
    "            while not done:\n",
    "                steps += 1\n",
    "                total_steps += 1\n",
    "                a_out = self.actor_network.sample_action(state, epsilon)\n",
    "                action_index = a_out\n",
    "                action = make_action(self.env, action_index)\n",
    "                obs_prime, reward, done, info = self.env.step(action)\n",
    "                total_reward += reward\n",
    "                state_prime = converter(obs_prime)\n",
    "\n",
    "                self.actor_append_sample(shared_memory, self.actor_network, self.actor_target_network, \\\n",
    "                                       state, action_index, reward, state_prime, done)\n",
    "\n",
    "                state = state_prime.float().cpu()\n",
    "                if done:\n",
    "                    break\n",
    "                    \n",
    "            # pandas로 리워드 기록하기\n",
    "            print(\"%d episode is done\" % num_epi)\n",
    "            print(\"total rewards : %d \" % total_reward)\n",
    "            train_stats.loc[num_epi]['rewards'] = total_reward\n",
    "            train_stats.to_csv('train_stat_minerl_agent {}.csv'.format(str(self.actor_idx)))\n",
    "            \n",
    "  \n",
    "            self.pull_parameters(learner) \n",
    "            print(\"actor network is updated \")\n",
    "            print(\"actor target_network is updated\")\n",
    "    \n",
    "    def pull_parameters(self, learner):\n",
    "        ray.get(self.paramServer.pull_parameters.remote(learner)) \n",
    "        policy_params, target_params = ray.get(self.paramServer.return_parameters.remote())\n",
    "        self.actor_network.load_state_dict(policy_params)\n",
    "        self.actor_target_network.load_state_dict(target_params)\n",
    "        \n",
    "    def env_close(self):\n",
    "        self.env.close()        \n",
    "\n",
    "    def actor_append_sample(self, memory, model, target_model, state, action, reward, next_state, done):\n",
    "        # Caluclating Priority (TD Error)\n",
    "        target = model(state.float()).data.cpu()\n",
    "        old_val = target[0][action].cpu()\n",
    "        target_val = target_model(next_state.float()).data.cpu()\n",
    "        if done:\n",
    "            target[0][action] = reward\n",
    "        else:\n",
    "            target[0][action] = reward + 0.99 * torch.max(target_val)\n",
    "\n",
    "        error = abs(old_val - target[0][action])\n",
    "        error = error.cpu()\n",
    "        memory.add.remote(error, [state, action, reward, next_state, done])\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "racial-reliance",
   "metadata": {},
   "outputs": [],
   "source": [
    "@ray.remote\n",
    "class ParameterServer:\n",
    "    def __init__(self):\n",
    "        self.policy_params = DQN(19).state_dict()\n",
    "        self.target_params = DQN(19).state_dict()\n",
    "    \n",
    "    def pull_parameters(self, learner):\n",
    "        learner.push_parameters.remote(self.policy_params, self.target_params)\n",
    "        return 1\n",
    "    \n",
    "    def return_parameters(self):\n",
    "        return self.policy_params, self.target_params\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "correct-pride",
   "metadata": {},
   "outputs": [],
   "source": [
    "@ray.remote(num_gpus=1)\n",
    "class Learner:\n",
    "    def __init__(self, network, batch_size, paramServer):\n",
    "        self.learner_network = DQN(19).cuda().float()\n",
    "        self.learner_target_network = DQN(19).cuda().float()\n",
    "        self.learner_network.load_state_dict(network.state_dict())\n",
    "        self.learner_target_network.load_state_dict(network.state_dict())\n",
    "        self.shared_network = DQN(19).cpu()\n",
    "        self.shared_target_network = DQN(19).cpu()\n",
    "        \n",
    "        self.paramServer = paramServer\n",
    "        \n",
    "        self.count = 0\n",
    "        self.batch_size = batch_size\n",
    "        self.max_counts= 1000000\n",
    "\n",
    "    # 1. sampling\n",
    "    # 2. calculate gradient\n",
    "    # 3. weight update\n",
    "    # 4. compute priorities\n",
    "    # 5. priorities of buffer update\n",
    "    # 6. remove old memory\n",
    "    \n",
    "    def push_parameters(self, server_policy_params, server_target_params):\n",
    "        self.shared_network.load_state_dict(self.learner_network.state_dict())\n",
    "        self.shared_target_network.load_state_dict(self.learner_target_network.state_dict())\n",
    "        policy_net_params = self.shared_network.state_dict()\n",
    "        target_net_params = self.shared_target_network.state_dict()\n",
    "        server_policy_params = policy_net_params\n",
    "        server_target_params = target_net_params\n",
    "        \n",
    "    def count(self):\n",
    "        return self.count\n",
    "    \n",
    "    def get_network(self):\n",
    "        self.shared_network.load_state_dict(self.learner_network.state_dict())\n",
    "        print(\"return learner network\")\n",
    "        return self.shared_network\n",
    "    \n",
    "    def get_target_network(self):\n",
    "        self.shared_target_network.load_state_dict(self.learner_target_network.state_dict())\n",
    "        return self.shared_target_network\n",
    "\n",
    "    def update_network(self, memory, demos, batch_size, optimizer):\n",
    "        print(\"started\")\n",
    "\n",
    "        \n",
    "        counts = [x for x in range(self.max_counts)]\n",
    "        train_stats = pd.DataFrame(index=counts, columns=['loss'])\n",
    "        while(self.count < 1000000):\n",
    "           \n",
    "\n",
    "            agent_batch, agent_idxs, agent_weights = ray.get(memory.sample.remote(batch_size))\n",
    "            demo_batch, demo_idxs, demo_weights = ray.get(demos.sample.remote(batch_size))\n",
    "\n",
    "            # demo_batch = (batch_size, state, action, reward, next_state, done, n_rewards)\n",
    "            # print(len(demo_batch[0])) # 0번째 배치이므로 0이 나옴\n",
    "            state_list = []\n",
    "            action_list = []\n",
    "            reward_list = []\n",
    "            next_state_list = []\n",
    "            done_mask_list = []\n",
    "\n",
    "            #print(\"agent batch len : {} \".format(str(len(agent_batch))))\n",
    "            for agent_transition in agent_batch:\n",
    "                s, a, r, s_prime, done_mask = agent_transition\n",
    "                state_list.append(s)\n",
    "                action_list.append([a])\n",
    "                reward_list.append([r])\n",
    "                next_state_list.append(s_prime)\n",
    "                done_mask_list.append([done_mask])\n",
    "\n",
    "            #print(\"demo batch len : {} \".format(str(len(demo_batch))))\n",
    "            \n",
    "            for expert_transition in demo_batch:\n",
    "                s, a, r, s_prime, done_mask = expert_transition\n",
    "                state_list.append(s)\n",
    "                action_list.append([a])\n",
    "                reward_list.append([r])\n",
    "                next_state_list.append(s_prime)\n",
    "                done_mask_list.append([done_mask])\n",
    "\n",
    "            s = torch.stack(state_list).float().cuda()\n",
    "            a = torch.tensor(action_list, dtype=torch.int64).cuda()\n",
    "            r = torch.tensor(reward_list).cuda()\n",
    "            s_prime = torch.stack(next_state_list).float().cuda()\n",
    "            done_mask = torch.tensor(done_mask_list).float().cuda()\n",
    "\n",
    "            q_vals = self.learner_network(s)\n",
    "            state_action_values = q_vals.gather(1, a)\n",
    "\n",
    "            # comparing the q values to the values expected using the next states and reward\n",
    "            next_state_values = self.learner_target_network(s_prime).max(1)[0].unsqueeze(1)\n",
    "            target = r + (next_state_values * gamma * done_mask)\n",
    "\n",
    "            # calculating the q loss, n-step return lossm supervised_loss\n",
    "            is_weights = torch.FloatTensor(agent_weights).to(device)\n",
    "            q_loss = (is_weights * F.mse_loss(state_action_values, target)).mean()\n",
    "            #supervised_loss = margin_loss(q_vals, a, 1, 1)\n",
    "\n",
    "            loss = q_loss #+ supervised_loss\n",
    "            errors = torch.abs(state_action_values - target).data.cpu().detach()\n",
    "            errors = errors.numpy()\n",
    "            # update priority\n",
    "            for i in range(batch_size):\n",
    "                idx = agent_idxs[i]\n",
    "                memory.update.remote(idx, errors[i])\n",
    "\n",
    "            train_stats.loc[self.count ]['loss'] = float(loss.item())\n",
    "            train_stats.to_csv('train_stat_minerl_learner.csv')\n",
    "\n",
    "            # optimization step and logging\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            self.count +=1\n",
    "            if(self.count % 50 == 0 and self.count != 0):\n",
    "                self.learner_target_network.load_state_dict(self.learner_network.state_dict())\n",
    "                print(\"Count : {} leaner_target_network updated\".format(self.count))\n",
    "                \n",
    "            if(self.count % 10 == 0 and self.count!= 0):\n",
    "                print(\"Count : {} leaner_network updated\".format(self.count))\n",
    "                torch.save(self.learner_network.state_dict(), model_path + \"apex_dqfd_learner.pth\")\n",
    "                print(\"learner model saved\")\n",
    "                \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "universal-plain",
   "metadata": {},
   "outputs": [],
   "source": [
    "@ray.remote\n",
    "def parse_demo(env_name, rep_buffer, policy_net, target_net, threshold=10, num_epochs=1, batch_size=16,\n",
    "      seq_len=10, gamma=0.99):\n",
    "    data = minerl.data.make(env_name)\n",
    "    print(\"data loading sucess\")\n",
    "    demo_num = 0\n",
    "    parse_ts = 0\n",
    "\n",
    "    for s_batch, a_batch, r_batch, ns_batch, d_batch in data.batch_iter(num_epochs=num_epochs, batch_size=batch_size,seq_len=seq_len):\n",
    "        print(ray.get(rep_buffer.size.remote()))\n",
    "        if(ray.get(rep_buffer.size.remote()) > 9999):\n",
    "            del data\n",
    "            return\n",
    "\n",
    "        demo_num += 1\n",
    "        print(demo_num)\n",
    "        print(r_batch.sum())\n",
    "        if r_batch.sum() < threshold:\n",
    "            del s_batch, a_batch, r_batch, d_batch, ns_batch\n",
    "            continue\n",
    "\n",
    "\n",
    "        batch_length = (s_batch['pov'].shape)[0]  # (batch, seq, 64, 64, 3)[0]\n",
    "        for i in range(0, batch_length):\n",
    "            episode_start_ts = 0\n",
    "\n",
    "            for j in range(0, seq_len):\n",
    "                av = a_batch['attack'][i][j]  # attack value\n",
    "                aj = a_batch['jump'][i][j]  # jump value\n",
    "                af = a_batch['forward'][i][j]  # forward value\n",
    "                ab = a_batch['back'][i][j]  # back value\n",
    "                al = a_batch['left'][i][j]  # left value\n",
    "                ar = a_batch['right'][i][j]  # right value\n",
    "                va = a_batch['camera'][i][j][0]  # vertical angle and\n",
    "                ha = a_batch['camera'][i][j][1]  # horizontal angle\n",
    "\n",
    "                camera_thresholds = (abs(va) + abs(ha)) / 2.0\n",
    "                # 카메라를 움직이는 경우\n",
    "                if (camera_thresholds > 2.5):\n",
    "                    # camera = [0, -5]\n",
    "                    if abs(va) < abs(ha) and ha < 0:\n",
    "                        if av == 0:\n",
    "                            action_index = 0\n",
    "                        else:\n",
    "                            action_index = 1\n",
    "                    # camera = [0, 5]\n",
    "                    elif abs(va) < abs(ha) and ha > 0:\n",
    "                        if av == 0:\n",
    "                            action_index = 2\n",
    "                        else:\n",
    "                            action_index = 3\n",
    "                    # camera = [-5, 0]\n",
    "                    elif abs(va) > abs(ha) and ha < 0:\n",
    "                        if av == 0:\n",
    "                            action_index = 4\n",
    "                        else:\n",
    "                            action_index = 5\n",
    "                    # camera = [5, 0]\n",
    "                    elif abs(va) > abs(ha) and ha > 0:\n",
    "                        if av == 0:\n",
    "                            action_index = 6\n",
    "                        else:\n",
    "                            action_index = 7\n",
    "\n",
    "                            # 카메라를 안움직이는 경우\n",
    "                # 점프하는 경우\n",
    "                elif (aj == 1):\n",
    "                    if (af == 0):\n",
    "                        action_index = 8\n",
    "                    else:\n",
    "                        action_index = 9\n",
    "\n",
    "                # 앞으로 가는 경우\n",
    "                elif (af == 1):\n",
    "                    if (av == 0):\n",
    "                        action_index = 10\n",
    "                    else:\n",
    "                        action_index = 11\n",
    "\n",
    "                # 뒤로 가는 경우\n",
    "                elif (ab == 1):\n",
    "                    if (av == 0):\n",
    "                        action_index = 12\n",
    "                    else:\n",
    "                        action_index = 13\n",
    "\n",
    "                # 왼쪽으로 가는 경우\n",
    "                elif (al == 1):\n",
    "                    if (av == 0):\n",
    "                        action_index = 14\n",
    "                    else:\n",
    "                        action_index = 15\n",
    "\n",
    "                # 오른쪽으로 가는 경우\n",
    "                elif (ar == 1):\n",
    "                    if (av == 0):\n",
    "                        action_index = 16\n",
    "                    else:\n",
    "                        action_index = 17\n",
    "\n",
    "                # 카메라, 움직임이 다 0이고 공격만 하는 것\n",
    "                else:\n",
    "                    if (av == 0):\n",
    "                        continue\n",
    "                    else:\n",
    "                        action_index = 18\n",
    "\n",
    "                a_index = torch.LongTensor([action_index]).cpu()\n",
    "                state = converter2(s_batch['pov'][i][j]).float().cpu()\n",
    "                next_state = converter2(ns_batch['pov'][i][j]).float().cpu()\n",
    "                reward = torch.FloatTensor([r_batch[i][j]]).cpu()\n",
    "                done = d_batch[i][j]  # .astype(int)\n",
    "\n",
    "\n",
    "\n",
    "                learner_append_sample(rep_buffer, policy_net, target_net, state, a_index, reward, next_state, done)\n",
    "                episode_start_ts += 1\n",
    "                parse_ts += 1\n",
    "\n",
    "                # if episode done we reset\n",
    "                if done:\n",
    "                    break\n",
    "            print('{} expert samples added.'.format(episode_start_ts))\n",
    "\n",
    "        gc.collect()\n",
    "        print('Batch Parsed finished. {} expert samples added.'.format(parse_ts))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dirty-summary",
   "metadata": {},
   "outputs": [],
   "source": [
    "ray.init()\n",
    "\n",
    "#하이퍼 파라미터\n",
    "learning_rate = 0.0003\n",
    "gamma = 0.99\n",
    "buffer_limit = 50000\n",
    "L1 = 0.9\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "root_path = os.curdir\n",
    "model_path = root_path + '/dqn_model/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "certain-savage",
   "metadata": {},
   "outputs": [],
   "source": [
    "policy_net = DQN(19).cuda()\n",
    "target_net = DQN(19).cuda()\n",
    "target_net.load_state_dict(policy_net.state_dict())\n",
    "memory = Memory.remote(30000)\n",
    "demos = Memory.remote(10000)\n",
    "optimizer = optim.Adam(policy_net.parameters(), lr=learning_rate, weight_decay=1e-5)\n",
    "\n",
    "# Copy network params from pretrained Agent\n",
    "model_path = './dqn_model/per_dqn.pth'\n",
    "policy_net.load_state_dict(torch.load(model_path, map_location='cuda:0'))\n",
    "target_net.load_state_dict(policy_net.state_dict())\n",
    "\n",
    "#parse_demo2.remote(\"MineRLTreechop-v0\", demos, policy_net.cpu(), target_net.cpu(), optimizer, threshold=60, num_epochs=1, batch_size=4, seq_len=60, gamma=0.99, model_name='pre_trained4.pth')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "annoying-nepal",
   "metadata": {},
   "outputs": [],
   "source": [
    "params_server = ParameterServer.remote()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "macro-criticism",
   "metadata": {},
   "outputs": [],
   "source": [
    "# learner network initialzation\n",
    "batch_size = 256\n",
    "learner = Learner.remote(policy_net, batch_size, params_server)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fundamental-heart",
   "metadata": {},
   "outputs": [],
   "source": [
    "# actor network, environments initialization\n",
    "# Generating each own instances\n",
    "\n",
    "actor1 = Actor.remote(learner, 0, 0.95, 0.05, params_server)\n",
    "actor2 = Actor.remote(learner, 1, 0.5, 0.025, params_server)\n",
    "actor3 = Actor.remote(learner, 2, 0.05, 0.01, params_server)\n",
    "#actor_list = [actor1, actor2]\n",
    "actor_list = [actor1, actor2, actor3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "restricted-grass",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#parse = parse_demo.remote(\"MineRLTreechop-v0\", demos, ray.get(learner.get_network.remote()), ray.get(learner.get_target_network.remote()), threshold=40, num_epochs=1, batch_size=4,\n",
    "#              seq_len=400, gamma=0.99)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "executed-convergence",
   "metadata": {},
   "outputs": [],
   "source": [
    "explore = [actor.explore.remote(learner, memory) for actor in actor_list]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "prescription-matter",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "update = learner.update_network.remote(memory, demos, batch_size, optimizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "accurate-scope",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "defined-quarterly",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "foreign-synthesis",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
