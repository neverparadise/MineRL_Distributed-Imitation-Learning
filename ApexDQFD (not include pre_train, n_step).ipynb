{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "annual-printing",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kukjin/anaconda3/envs/minerl/lib/python3.7/site-packages/ray/autoscaler/_private/cli_logger.py:61: FutureWarning: Not all Ray CLI dependencies were found. In Ray 1.4+, the Ray CLI, autoscaler, and dashboard will only be usable via `pip install 'ray[default]'`. Please update your install command.\n",
      "  \"update your install command.\", FutureWarning)\n",
      "/home/kukjin/anaconda3/envs/minerl/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    }
   ],
   "source": [
    "from model import DQN\n",
    "import os\n",
    "import minerl\n",
    "import gym\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import ray\n",
    "import pandas as pd\n",
    "import gc\n",
    "import asyncio\n",
    "from _collections import deque\n",
    "from utils import *\n",
    "import random\n",
    "\n",
    "def learner_append_sample(memory, model, target_model, state, action, reward, next_state, done):\n",
    "    # Caluclating Priority (TD Error)\n",
    "    target = model(state.float()).data.cpu()\n",
    "    old_val = target[0][action].cpu()\n",
    "    target_val = target_model(next_state.float()).data.cpu()\n",
    "    if done:\n",
    "        target[0][action] = reward\n",
    "    else:\n",
    "        target[0][action] = reward + 0.99 * torch.max(target_val)\n",
    "\n",
    "    error = abs(old_val - target[0][action])\n",
    "    error = error.cpu()\n",
    "    memory.add.remote(error, [state, action, reward, next_state, done])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "virgin-airline",
   "metadata": {},
   "outputs": [],
   "source": [
    "@ray.remote\n",
    "class ParameterServer:\n",
    "    def update_parameters(self, learner_params): \n",
    "        self.learner_params = learner_params\n",
    "\n",
    "    def return_parameters(self):\n",
    "        return self.learner_params\n",
    "\n",
    "    def return_saving_status(self):\n",
    "        return self.is_saved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "express-reservation",
   "metadata": {},
   "outputs": [],
   "source": [
    "@ray.remote\n",
    "class Actor:\n",
    "    def __init__(self, learner, actor_idx, startEpsilon, endEpsilon):\n",
    "        # environment initialization\n",
    "        self.actor_idx = actor_idx\n",
    "        self.env = gym.make(\"MineRLTreechop-v0\")\n",
    "        self.port_number = int(\"12340\") + actor_idx\n",
    "        print(\"actor environment %d initialize successfully\" % self.actor_idx)\n",
    "        self.env.make_interactive(port=self.port_number, realtime=False)\n",
    "        self.shared_network_cpu = ray.get(learner.get_network.remote())\n",
    "        # self.shared_memory = ray.get(shared_memory_id)\n",
    "        # print(\"shared memory assign successfully\")\n",
    "        \n",
    "        # network initalization\n",
    "        self.actor_network = DQN(19).cpu()\n",
    "        self.actor_target_network = DQN(19).cpu()\n",
    "        self.actor_network.load_state_dict(self.shared_network_cpu.state_dict())\n",
    "        self.actor_target_network.load_state_dict(self.actor_network.state_dict())\n",
    "        print(\"actor network %d initialize successfully\" % self.actor_idx)\n",
    "\n",
    "        self.initialized = False\n",
    "        self.epi_counter = 0\n",
    "        # exploring info\n",
    "        self.startEpsilon = startEpsilon\n",
    "        self.endEpsilon = endEpsilon\n",
    "        self.max_episodes = 1000\n",
    "\n",
    "\n",
    "\n",
    "    # 1. 네트워크 파라미터 복사\n",
    "    # 2. 환경 탐험 (초기화, 행동)\n",
    "    # 3. 로컬버퍼에 저장\n",
    "    # 4. priority 계산\n",
    "    # 5. 글로벌 버퍼에 저장\n",
    "    # 6. 주기적으로 네트워크 업데이트\n",
    "\n",
    "    def get_initialized(self):\n",
    "        return self.initialized\n",
    "\n",
    "    def get_counter(self):\n",
    "        return self.epi_counter\n",
    "\n",
    "    # 각 환경 인스턴스에서 각 엡실론에 따라 탐험을 진행한다.\n",
    "    # 탐험 과정에서 local buffer에 transition들을 저장한다.\n",
    "    # local buffer의 개수가 특정 개수 이상이면 global buffer에 추가해준다.\n",
    "\n",
    "    def explore(self, learner, shared_memory):\n",
    "        \n",
    "        self.initialized = True\n",
    "        stepDrop = (self.startEpsilon - self.endEpsilon) / self.max_episodes\n",
    "        epsilon = self.startEpsilon\n",
    "        total_steps = 0\n",
    "        \n",
    "        episodes = [x for x in range(self.max_episodes)]\n",
    "        train_stats = pd.DataFrame(index=episodes, columns=['rewards'])\n",
    "        \n",
    "        for num_epi in range(self.max_episodes):\n",
    "            obs = self.env.reset()\n",
    "            state = converter(obs).cpu().float()\n",
    "            done = False\n",
    "            total_reward = 0\n",
    "            steps = 0\n",
    "            if (epsilon > self.endEpsilon):\n",
    "                epsilon -= stepDrop\n",
    "                \n",
    "            while not done:\n",
    "                steps += 1\n",
    "                total_steps += 1\n",
    "                a_out = self.actor_network.sample_action(state, epsilon)\n",
    "                action_index = a_out\n",
    "                action = make_action(self.env, action_index)\n",
    "                obs_prime, reward, done, info = self.env.step(action)\n",
    "                total_reward += reward\n",
    "                state_prime = converter(obs_prime)\n",
    "\n",
    "                self.actor_append_sample(shared_memory, self.actor_network, self.actor_target_network, \\\n",
    "                                       state, action_index, reward, state_prime, done)\n",
    "\n",
    "                state = state_prime.float().cpu()\n",
    "                if done:\n",
    "                    break\n",
    "                    \n",
    "            print(ray.get(shared_memory.size.remote()))\n",
    "\n",
    "            # pandas로 리워드 기록하기\n",
    "            print(\"%d episode is done\" % num_epi)\n",
    "            print(\"total rewards : %d \" % total_reward)\n",
    "            train_stats.loc[num_epi]['rewards'] = total_reward\n",
    "            train_stats.to_csv('train_stat_minerl_agent {}.csv'.format(str(self.actor_idx)))\n",
    "\n",
    "\n",
    "            #if (num_epi % 2 == 0 and num_epi != 0):\n",
    "            shared_network = ray.get(learner.get_network.remote())\n",
    "            self.actor_network.load_state_dict(shared_network.state_dict())\n",
    "            print(\"actor network is updated \")\n",
    "\n",
    "    def env_close(self):\n",
    "        self.env.close()        \n",
    "\n",
    "    def actor_append_sample(self, memory, model, target_model, state, action, reward, next_state, done):\n",
    "        # Caluclating Priority (TD Error)\n",
    "        target = model(state.float()).data.cpu()\n",
    "        old_val = target[0][action].cpu()\n",
    "        target_val = target_model(next_state.float()).data.cpu()\n",
    "        if done:\n",
    "            target[0][action] = reward\n",
    "        else:\n",
    "            target[0][action] = reward + 0.99 * torch.max(target_val)\n",
    "\n",
    "        error = abs(old_val - target[0][action])\n",
    "        error = error.cpu()\n",
    "        memory.add.remote(error, [state, action, reward, next_state, done])\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "correct-pride",
   "metadata": {},
   "outputs": [],
   "source": [
    "@ray.remote(num_gpus=1)\n",
    "class Learner:\n",
    "    def __init__(self, network, batch_size):\n",
    "        self.learner_network = DQN(19).cuda().float()\n",
    "        self.learner_target_network = DQN(19).cuda().float()\n",
    "        self.learner_network.load_state_dict(network.state_dict())\n",
    "        self.learner_target_network.load_state_dict(network.state_dict())\n",
    "        self.shared_network = DQN(19).cpu()\n",
    "        self.shared_target_network = DQN(19).cpu()\n",
    "        \n",
    "        self.count = 0\n",
    "        self.batch_size = batch_size\n",
    "        self.max_counts= 1000000\n",
    "\n",
    "    # 1. sampling\n",
    "    # 2. calculate gradient\n",
    "    # 3. weight update\n",
    "    # 4. compute priorities\n",
    "    # 5. priorities of buffer update\n",
    "    # 6. remove old memory\n",
    "    def count(self):\n",
    "        return self.count\n",
    "    \n",
    "    def get_network(self):\n",
    "        self.shared_network.load_state_dict(self.learner_network.state_dict())\n",
    "        print(\"return learner network\")\n",
    "        return self.shared_network\n",
    "    \n",
    "    def get_target_network(self):\n",
    "        self.shared_target_network.load_state_dict(self.learner_target_network.state_dict())\n",
    "        return self.shared_target_network\n",
    "\n",
    "    def update_network(self, memory, demos, batch_size, optimizer):\n",
    "        print(\"started\")\n",
    "        print(ray.get(memory.size.remote()))\n",
    "        print(ray.get(demos.size.remote()))\n",
    "        \n",
    "        counts = [x for x in range(self.max_counts)]\n",
    "        train_stats = pd.DataFrame(index=counts, columns=['loss'])\n",
    "        while(self.count < 1000000):\n",
    "           \n",
    "\n",
    "            agent_batch, agent_idxs, agent_weights = ray.get(memory.sample.remote(batch_size))\n",
    "            demo_batch, demo_idxs, demo_weights = ray.get(demos.sample.remote(batch_size))\n",
    "\n",
    "            # demo_batch = (batch_size, state, action, reward, next_state, done, n_rewards)\n",
    "            # print(len(demo_batch[0])) # 0번째 배치이므로 0이 나옴\n",
    "            state_list = []\n",
    "            action_list = []\n",
    "            reward_list = []\n",
    "            next_state_list = []\n",
    "            done_mask_list = []\n",
    "\n",
    "            print(\"agent batch len : {} \".format(str(len(agent_batch))))\n",
    "            for agent_transition in agent_batch:\n",
    "                s, a, r, s_prime, done_mask = agent_transition\n",
    "                state_list.append(s)\n",
    "                action_list.append([a])\n",
    "                reward_list.append([r])\n",
    "                next_state_list.append(s_prime)\n",
    "                done_mask_list.append([done_mask])\n",
    "\n",
    "            print(\"demo batch len : {} \".format(str(len(demo_batch))))\n",
    "            \n",
    "            for expert_transition in demo_batch:\n",
    "                s, a, r, s_prime, done_mask = expert_transition\n",
    "                state_list.append(s)\n",
    "                action_list.append([a])\n",
    "                reward_list.append([r])\n",
    "                next_state_list.append(s_prime)\n",
    "                done_mask_list.append([done_mask])\n",
    "\n",
    "            s = torch.stack(state_list).float().cuda()\n",
    "            a = torch.tensor(action_list, dtype=torch.int64).cuda()\n",
    "            r = torch.tensor(reward_list).cuda()\n",
    "            s_prime = torch.stack(next_state_list).float().cuda()\n",
    "            done_mask = torch.tensor(done_mask_list).float().cuda()\n",
    "\n",
    "            q_vals = self.learner_network(s)\n",
    "            state_action_values = q_vals.gather(1, a)\n",
    "\n",
    "            # comparing the q values to the values expected using the next states and reward\n",
    "            next_state_values = self.learner_target_network(s_prime).max(1)[0].unsqueeze(1)\n",
    "            target = r + (next_state_values * gamma * done_mask)\n",
    "\n",
    "            # calculating the q loss, n-step return lossm supervised_loss\n",
    "            is_weights = torch.FloatTensor(agent_weights).to(device)\n",
    "            q_loss = (is_weights * F.mse_loss(state_action_values, target)).mean()\n",
    "            #supervised_loss = margin_loss(q_vals, a, 1, 1)\n",
    "\n",
    "            loss = q_loss #+ supervised_loss\n",
    "            errors = torch.abs(state_action_values - target).data.cpu().detach()\n",
    "            errors = errors.numpy()\n",
    "            # update priority\n",
    "            for i in range(batch_size):\n",
    "                idx = agent_idxs[i]\n",
    "                memory.update.remote(idx, errors[i])\n",
    "\n",
    "            train_stats.loc[self.count ]['loss'] = float(loss.item())\n",
    "            train_stats.to_csv('train_stat_minerl_learner.csv')\n",
    "\n",
    "            # optimization step and logging\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            torch.save(self.learner_network.state_dict(), model_path + \"apex_dqfd_learner.pth\")\n",
    "            self.count +=1\n",
    "            if(self.count % 100 == 0 and self.count != 0):\n",
    "                self.learner_target_network.load_state_dict(self.learner_network.state_dict())\n",
    "                print(\"Count : {} leaner_target_network updated\".format(self.count))\n",
    "                \n",
    "            if(self.count % 10 == 0 and self.count!= 0):\n",
    "                print(\"Count : {} leaner_network updated\".format(self.count))\n",
    "  \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "universal-plain",
   "metadata": {},
   "outputs": [],
   "source": [
    "@ray.remote\n",
    "def parse_demo(env_name, rep_buffer, policy_net, target_net, threshold=10, num_epochs=1, batch_size=16,\n",
    "      seq_len=10, gamma=0.99):\n",
    "    data = minerl.data.make(env_name)\n",
    "    print(\"data loading sucess\")\n",
    "    demo_num = 0\n",
    "    parse_ts = 0\n",
    "\n",
    "    for s_batch, a_batch, r_batch, ns_batch, d_batch in data.batch_iter(num_epochs=num_epochs, batch_size=batch_size,seq_len=seq_len):\n",
    "        print(ray.get(rep_buffer.size.remote()))\n",
    "        if(ray.get(rep_buffer.size.remote()) > 9999):\n",
    "            del data\n",
    "            return\n",
    "\n",
    "        demo_num += 1\n",
    "        print(demo_num)\n",
    "        print(r_batch.sum())\n",
    "        if r_batch.sum() < threshold:\n",
    "            del s_batch, a_batch, r_batch, d_batch, ns_batch\n",
    "            continue\n",
    "\n",
    "\n",
    "        batch_length = (s_batch['pov'].shape)[0]  # (batch, seq, 64, 64, 3)[0]\n",
    "        for i in range(0, batch_length):\n",
    "            episode_start_ts = 0\n",
    "\n",
    "            for j in range(0, seq_len):\n",
    "                av = a_batch['attack'][i][j]  # attack value\n",
    "                aj = a_batch['jump'][i][j]  # jump value\n",
    "                af = a_batch['forward'][i][j]  # forward value\n",
    "                ab = a_batch['back'][i][j]  # back value\n",
    "                al = a_batch['left'][i][j]  # left value\n",
    "                ar = a_batch['right'][i][j]  # right value\n",
    "                va = a_batch['camera'][i][j][0]  # vertical angle and\n",
    "                ha = a_batch['camera'][i][j][1]  # horizontal angle\n",
    "\n",
    "                camera_thresholds = (abs(va) + abs(ha)) / 2.0\n",
    "                # 카메라를 움직이는 경우\n",
    "                if (camera_thresholds > 2.5):\n",
    "                    # camera = [0, -5]\n",
    "                    if abs(va) < abs(ha) and ha < 0:\n",
    "                        if av == 0:\n",
    "                            action_index = 0\n",
    "                        else:\n",
    "                            action_index = 1\n",
    "                    # camera = [0, 5]\n",
    "                    elif abs(va) < abs(ha) and ha > 0:\n",
    "                        if av == 0:\n",
    "                            action_index = 2\n",
    "                        else:\n",
    "                            action_index = 3\n",
    "                    # camera = [-5, 0]\n",
    "                    elif abs(va) > abs(ha) and ha < 0:\n",
    "                        if av == 0:\n",
    "                            action_index = 4\n",
    "                        else:\n",
    "                            action_index = 5\n",
    "                    # camera = [5, 0]\n",
    "                    elif abs(va) > abs(ha) and ha > 0:\n",
    "                        if av == 0:\n",
    "                            action_index = 6\n",
    "                        else:\n",
    "                            action_index = 7\n",
    "\n",
    "                            # 카메라를 안움직이는 경우\n",
    "                # 점프하는 경우\n",
    "                elif (aj == 1):\n",
    "                    if (af == 0):\n",
    "                        action_index = 8\n",
    "                    else:\n",
    "                        action_index = 9\n",
    "\n",
    "                # 앞으로 가는 경우\n",
    "                elif (af == 1):\n",
    "                    if (av == 0):\n",
    "                        action_index = 10\n",
    "                    else:\n",
    "                        action_index = 11\n",
    "\n",
    "                # 뒤로 가는 경우\n",
    "                elif (ab == 1):\n",
    "                    if (av == 0):\n",
    "                        action_index = 12\n",
    "                    else:\n",
    "                        action_index = 13\n",
    "\n",
    "                # 왼쪽으로 가는 경우\n",
    "                elif (al == 1):\n",
    "                    if (av == 0):\n",
    "                        action_index = 14\n",
    "                    else:\n",
    "                        action_index = 15\n",
    "\n",
    "                # 오른쪽으로 가는 경우\n",
    "                elif (ar == 1):\n",
    "                    if (av == 0):\n",
    "                        action_index = 16\n",
    "                    else:\n",
    "                        action_index = 17\n",
    "\n",
    "                # 카메라, 움직임이 다 0이고 공격만 하는 것\n",
    "                else:\n",
    "                    if (av == 0):\n",
    "                        continue\n",
    "                    else:\n",
    "                        action_index = 18\n",
    "\n",
    "                a_index = torch.LongTensor([action_index]).cpu()\n",
    "                state = converter2(s_batch['pov'][i][j]).float().cpu()\n",
    "                next_state = converter2(ns_batch['pov'][i][j]).float().cpu()\n",
    "                reward = torch.FloatTensor([r_batch[i][j]]).cpu()\n",
    "                done = d_batch[i][j]  # .astype(int)\n",
    "\n",
    "\n",
    "\n",
    "                learner_append_sample(rep_buffer, policy_net, target_net, state, a_index, reward, next_state, done)\n",
    "                episode_start_ts += 1\n",
    "                parse_ts += 1\n",
    "\n",
    "                # if episode done we reset\n",
    "                if done:\n",
    "                    break\n",
    "            print('{} expert samples added.'.format(episode_start_ts))\n",
    "\n",
    "        gc.collect()\n",
    "        print('Batch Parsed finished. {} expert samples added.'.format(parse_ts))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dirty-summary",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-06-06 23:42:48,384\tINFO services.py:1269 -- View the Ray dashboard at \u001b[1m\u001b[32mhttp://127.0.0.1:8265\u001b[39m\u001b[22m\n"
     ]
    }
   ],
   "source": [
    "ray.init()\n",
    "\n",
    "#하이퍼 파라미터\n",
    "learning_rate = 0.0003\n",
    "gamma = 0.99\n",
    "buffer_limit = 50000\n",
    "L1 = 0.9\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "root_path = os.curdir\n",
    "model_path = root_path + '/dqn_model/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "certain-savage",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[33m(raylet)\u001b[0m /home/kukjin/anaconda3/envs/minerl/lib/python3.7/site-packages/ray/autoscaler/_private/cli_logger.py:61: FutureWarning: Not all Ray CLI dependencies were found. In Ray 1.4+, the Ray CLI, autoscaler, and dashboard will only be usable via `pip install 'ray[default]'`. Please update your install command.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m   \"update your install command.\", FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "policy_net = DQN(19).cuda()\n",
    "target_net = DQN(19).cuda()\n",
    "target_net.load_state_dict(policy_net.state_dict())\n",
    "memory = Memory.remote(30000)\n",
    "demos = Memory.remote(10000)\n",
    "optimizer = optim.Adam(policy_net.parameters(), lr=learning_rate, weight_decay=1e-5)\n",
    "\n",
    "# Copy network params from pretrained Agent\n",
    "model_path = './dqn_model/per_dqn.pth'\n",
    "policy_net.load_state_dict(torch.load(model_path, map_location='cuda:0'))\n",
    "target_net.load_state_dict(policy_net.state_dict())\n",
    "\n",
    "#parse_demo2.remote(\"MineRLTreechop-v0\", demos, policy_net.cpu(), target_net.cpu(), optimizer, threshold=60, num_epochs=1, batch_size=4, seq_len=60, gamma=0.99, model_name='pre_trained4.pth')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "macro-criticism",
   "metadata": {},
   "outputs": [],
   "source": [
    "# learner network initialzation\n",
    "batch_size = 256\n",
    "learner = Learner.remote(policy_net, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fundamental-heart",
   "metadata": {},
   "outputs": [],
   "source": [
    "# actor network, environments initialization\n",
    "# Generating each own instances\n",
    "\n",
    "actor1 = Actor.remote(learner, 0, 0.95, 0.05)\n",
    "actor2 = Actor.remote(learner, 1, 0.5, 0.025)\n",
    "#actor3 = Actor.remote(learner, 2, 0.1, 0.01)\n",
    "actor_list = [actor1, actor2]\n",
    "#actor_list = [actor1, actor2, actor3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "restricted-grass",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=93773)\u001b[0m Memory is initialized\n",
      "\u001b[2m\u001b[36m(pid=93774)\u001b[0m Memory is initialized\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=93776)\u001b[0m /home/kukjin/anaconda3/envs/minerl/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=93776)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n",
      "\u001b[2m\u001b[36m(pid=93775)\u001b[0m /home/kukjin/anaconda3/envs/minerl/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=93775)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=93772)\u001b[0m return learner network\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=93772)\u001b[0m /home/kukjin/anaconda3/envs/minerl/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=93772)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n",
      "\u001b[2m\u001b[36m(pid=93769)\u001b[0m /home/kukjin/anaconda3/envs/minerl/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=93769)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n",
      "\u001b[2m\u001b[36m(pid=93773)\u001b[0m /home/kukjin/anaconda3/envs/minerl/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=93773)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n",
      "\u001b[2m\u001b[36m(pid=93771)\u001b[0m /home/kukjin/anaconda3/envs/minerl/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=93771)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n",
      "\u001b[2m\u001b[36m(pid=93767)\u001b[0m /home/kukjin/anaconda3/envs/minerl/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=93767)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n",
      "\u001b[2m\u001b[36m(pid=93766)\u001b[0m /home/kukjin/anaconda3/envs/minerl/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=93766)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n",
      "\u001b[2m\u001b[36m(pid=93768)\u001b[0m /home/kukjin/anaconda3/envs/minerl/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=93768)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n",
      "\u001b[2m\u001b[36m(pid=93770)\u001b[0m /home/kukjin/anaconda3/envs/minerl/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=93770)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n",
      "\u001b[2m\u001b[36m(pid=93774)\u001b[0m /home/kukjin/anaconda3/envs/minerl/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=93774)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n",
      "\u001b[2m\u001b[36m(pid=93765)\u001b[0m /home/kukjin/anaconda3/envs/minerl/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=93765)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=93771)\u001b[0m data loading sucess\n",
      "\u001b[2m\u001b[36m(pid=93771)\u001b[0m 0\n",
      "\u001b[2m\u001b[36m(pid=93771)\u001b[0m 1\n",
      "\u001b[2m\u001b[36m(pid=93771)\u001b[0m 56.0\n",
      "\u001b[2m\u001b[36m(pid=93771)\u001b[0m 395 expert samples added.\n",
      "\u001b[2m\u001b[36m(pid=93771)\u001b[0m 390 expert samples added.\n",
      "\u001b[2m\u001b[36m(pid=93771)\u001b[0m 389 expert samples added.\n",
      "\u001b[2m\u001b[36m(pid=93771)\u001b[0m 359 expert samples added.\n",
      "\u001b[2m\u001b[36m(pid=93771)\u001b[0m Batch Parsed finished. 1533 expert samples added.\n",
      "\u001b[2m\u001b[36m(pid=93771)\u001b[0m 1533\n",
      "\u001b[2m\u001b[36m(pid=93771)\u001b[0m 2\n",
      "\u001b[2m\u001b[36m(pid=93771)\u001b[0m 58.0\n",
      "\u001b[2m\u001b[36m(pid=93771)\u001b[0m 399 expert samples added.\n",
      "\u001b[2m\u001b[36m(pid=93771)\u001b[0m 400 expert samples added.\n",
      "\u001b[2m\u001b[36m(pid=93771)\u001b[0m 354 expert samples added.\n"
     ]
    }
   ],
   "source": [
    "parse = parse_demo.remote(\"MineRLTreechop-v0\", demos, ray.get(learner.get_network.remote()), ray.get(learner.get_target_network.remote()), threshold=40, num_epochs=1, batch_size=4,\n",
    "              seq_len=400, gamma=0.99)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "executed-convergence",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=93771)\u001b[0m 391 expert samples added.\n",
      "\u001b[2m\u001b[36m(pid=93771)\u001b[0m Batch Parsed finished. 3077 expert samples added.\n",
      "\u001b[2m\u001b[36m(pid=93771)\u001b[0m 3077\n",
      "\u001b[2m\u001b[36m(pid=93771)\u001b[0m 3\n",
      "\u001b[2m\u001b[36m(pid=93771)\u001b[0m 62.0\n",
      "\u001b[2m\u001b[36m(pid=93771)\u001b[0m 398 expert samples added.\n",
      "\u001b[2m\u001b[36m(pid=93771)\u001b[0m 383 expert samples added.\n",
      "\u001b[2m\u001b[36m(pid=93771)\u001b[0m 384 expert samples added.\n",
      "\u001b[2m\u001b[36m(pid=93771)\u001b[0m 358 expert samples added.\n",
      "\u001b[2m\u001b[36m(pid=93771)\u001b[0m Batch Parsed finished. 4600 expert samples added.\n",
      "\u001b[2m\u001b[36m(pid=93771)\u001b[0m 4600\n",
      "\u001b[2m\u001b[36m(pid=93771)\u001b[0m 4\n",
      "\u001b[2m\u001b[36m(pid=93771)\u001b[0m 52.0\n",
      "\u001b[2m\u001b[36m(pid=93771)\u001b[0m 380 expert samples added.\n",
      "\u001b[2m\u001b[36m(pid=93771)\u001b[0m 390 expert samples added.\n",
      "\u001b[2m\u001b[36m(pid=93771)\u001b[0m 394 expert samples added.\n",
      "\u001b[2m\u001b[36m(pid=93771)\u001b[0m 364 expert samples added.\n",
      "\u001b[2m\u001b[36m(pid=93771)\u001b[0m Batch Parsed finished. 6128 expert samples added.\n",
      "\u001b[2m\u001b[36m(pid=93771)\u001b[0m 6128\n",
      "\u001b[2m\u001b[36m(pid=93771)\u001b[0m 5\n",
      "\u001b[2m\u001b[36m(pid=93771)\u001b[0m 45.0\n",
      "\u001b[2m\u001b[36m(pid=93771)\u001b[0m 394 expert samples added.\n",
      "\u001b[2m\u001b[36m(pid=93771)\u001b[0m 374 expert samples added.\n",
      "\u001b[2m\u001b[36m(pid=93771)\u001b[0m 238 expert samples added.\n",
      "\u001b[2m\u001b[36m(pid=93771)\u001b[0m 345 expert samples added.\n",
      "\u001b[2m\u001b[36m(pid=93771)\u001b[0m Batch Parsed finished. 7479 expert samples added.\n",
      "\u001b[2m\u001b[36m(pid=93771)\u001b[0m 7479\n",
      "\u001b[2m\u001b[36m(pid=93771)\u001b[0m 6\n",
      "\u001b[2m\u001b[36m(pid=93771)\u001b[0m 55.0\n",
      "\u001b[2m\u001b[36m(pid=93771)\u001b[0m 392 expert samples added.\n",
      "\u001b[2m\u001b[36m(pid=93771)\u001b[0m 372 expert samples added.\n",
      "\u001b[2m\u001b[36m(pid=93771)\u001b[0m 381 expert samples added.\n",
      "\u001b[2m\u001b[36m(pid=93771)\u001b[0m 173 expert samples added.\n",
      "\u001b[2m\u001b[36m(pid=93771)\u001b[0m Batch Parsed finished. 8797 expert samples added.\n",
      "\u001b[2m\u001b[36m(pid=93771)\u001b[0m 8797\n",
      "\u001b[2m\u001b[36m(pid=93771)\u001b[0m 7\n",
      "\u001b[2m\u001b[36m(pid=93771)\u001b[0m 56.0\n",
      "\u001b[2m\u001b[36m(pid=93776)\u001b[0m actor environment 0 initialize successfully\n",
      "\u001b[2m\u001b[36m(pid=93772)\u001b[0m return learner network\n",
      "\u001b[2m\u001b[36m(pid=93776)\u001b[0m actor network 0 initialize successfully\n",
      "\u001b[2m\u001b[36m(pid=93772)\u001b[0m return learner network\n",
      "\u001b[2m\u001b[36m(pid=93775)\u001b[0m actor environment 1 initialize successfully\n",
      "\u001b[2m\u001b[36m(pid=93775)\u001b[0m actor network 1 initialize successfully\n",
      "\u001b[2m\u001b[36m(pid=93771)\u001b[0m 395 expert samples added.\n",
      "\u001b[2m\u001b[36m(pid=93771)\u001b[0m 376 expert samples added.\n",
      "\u001b[2m\u001b[36m(pid=93771)\u001b[0m 338 expert samples added.\n",
      "\u001b[2m\u001b[36m(pid=93771)\u001b[0m 336 expert samples added.\n",
      "\u001b[2m\u001b[36m(pid=93771)\u001b[0m Batch Parsed finished. 10242 expert samples added.\n",
      "\u001b[2m\u001b[36m(pid=93771)\u001b[0m 10000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=93776)\u001b[0m MineRL agent is public, connect on port 12340 with Minecraft 1.11\n",
      "\u001b[2m\u001b[36m(pid=93775)\u001b[0m MineRL agent is public, connect on port 12341 with Minecraft 1.11\n"
     ]
    }
   ],
   "source": [
    "explore = [actor.explore.remote(learner, memory) for actor in actor_list]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "prescription-matter",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=93772)\u001b[0m started\n",
      "\u001b[2m\u001b[36m(pid=93772)\u001b[0m 1689\n",
      "\u001b[2m\u001b[36m(pid=93772)\u001b[0m 10000\n",
      "\u001b[2m\u001b[36m(pid=93772)\u001b[0m agent batch len : 256 \n",
      "\u001b[2m\u001b[36m(pid=93772)\u001b[0m demo batch len : 256 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=93772)\u001b[0m /home/kukjin/anaconda3/envs/minerl/lib/python3.7/site-packages/ray/workers/default_worker.py:87: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /pytorch/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=93772)\u001b[0m   default=\"WORKER\",\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=93772)\u001b[0m agent batch len : 256 \n",
      "\u001b[2m\u001b[36m(pid=93772)\u001b[0m demo batch len : 256 \n",
      "\u001b[2m\u001b[36m(pid=93772)\u001b[0m agent batch len : 256 \n",
      "\u001b[2m\u001b[36m(pid=93772)\u001b[0m demo batch len : 256 \n",
      "\u001b[2m\u001b[36m(pid=93772)\u001b[0m agent batch len : 256 \n",
      "\u001b[2m\u001b[36m(pid=93772)\u001b[0m demo batch len : 256 \n",
      "\u001b[2m\u001b[36m(pid=93772)\u001b[0m agent batch len : 256 \n",
      "\u001b[2m\u001b[36m(pid=93772)\u001b[0m demo batch len : 256 \n",
      "\u001b[2m\u001b[36m(pid=93772)\u001b[0m agent batch len : 256 \n",
      "\u001b[2m\u001b[36m(pid=93772)\u001b[0m demo batch len : 256 \n",
      "\u001b[2m\u001b[36m(pid=93772)\u001b[0m agent batch len : 256 \n",
      "\u001b[2m\u001b[36m(pid=93772)\u001b[0m demo batch len : 256 \n",
      "\u001b[2m\u001b[36m(pid=93772)\u001b[0m agent batch len : 256 \n",
      "\u001b[2m\u001b[36m(pid=93772)\u001b[0m demo batch len : 256 \n",
      "\u001b[2m\u001b[36m(pid=93772)\u001b[0m agent batch len : 256 \n",
      "\u001b[2m\u001b[36m(pid=93772)\u001b[0m demo batch len : 256 \n",
      "\u001b[2m\u001b[36m(pid=93772)\u001b[0m agent batch len : 256 \n",
      "\u001b[2m\u001b[36m(pid=93772)\u001b[0m demo batch len : 256 \n",
      "\u001b[2m\u001b[36m(pid=93772)\u001b[0m Count : 10 leaner_network updated\n",
      "\u001b[2m\u001b[36m(pid=93772)\u001b[0m agent batch len : 256 \n",
      "\u001b[2m\u001b[36m(pid=93772)\u001b[0m demo batch len : 256 \n",
      "\u001b[2m\u001b[36m(pid=93772)\u001b[0m agent batch len : 256 \n",
      "\u001b[2m\u001b[36m(pid=93772)\u001b[0m demo batch len : 256 \n",
      "\u001b[2m\u001b[36m(pid=93772)\u001b[0m agent batch len : 256 \n",
      "\u001b[2m\u001b[36m(pid=93772)\u001b[0m demo batch len : 256 \n",
      "\u001b[2m\u001b[36m(pid=93772)\u001b[0m agent batch len : 256 \n",
      "\u001b[2m\u001b[36m(pid=93772)\u001b[0m demo batch len : 256 \n",
      "\u001b[2m\u001b[36m(pid=93772)\u001b[0m agent batch len : 256 \n",
      "\u001b[2m\u001b[36m(pid=93772)\u001b[0m demo batch len : 256 \n",
      "\u001b[2m\u001b[36m(pid=93772)\u001b[0m agent batch len : 256 \n",
      "\u001b[2m\u001b[36m(pid=93772)\u001b[0m demo batch len : 256 \n",
      "\u001b[2m\u001b[36m(pid=93772)\u001b[0m agent batch len : 256 \n",
      "\u001b[2m\u001b[36m(pid=93772)\u001b[0m demo batch len : 256 \n",
      "\u001b[2m\u001b[36m(pid=93772)\u001b[0m agent batch len : 256 \n",
      "\u001b[2m\u001b[36m(pid=93772)\u001b[0m demo batch len : 256 \n",
      "\u001b[2m\u001b[36m(pid=93772)\u001b[0m agent batch len : 256 \n",
      "\u001b[2m\u001b[36m(pid=93772)\u001b[0m demo batch len : 256 \n",
      "\u001b[2m\u001b[36m(pid=93772)\u001b[0m agent batch len : 256 \n",
      "\u001b[2m\u001b[36m(pid=93772)\u001b[0m demo batch len : 256 \n",
      "\u001b[2m\u001b[36m(pid=93772)\u001b[0m Count : 20 leaner_network updated\n",
      "\u001b[2m\u001b[36m(pid=93772)\u001b[0m agent batch len : 256 \n",
      "\u001b[2m\u001b[36m(pid=93772)\u001b[0m demo batch len : 256 \n",
      "\u001b[2m\u001b[36m(pid=93772)\u001b[0m agent batch len : 256 \n",
      "\u001b[2m\u001b[36m(pid=93772)\u001b[0m demo batch len : 256 \n",
      "\u001b[2m\u001b[36m(pid=93772)\u001b[0m agent batch len : 256 \n",
      "\u001b[2m\u001b[36m(pid=93772)\u001b[0m demo batch len : 256 \n",
      "\u001b[2m\u001b[36m(pid=93772)\u001b[0m agent batch len : 256 \n",
      "\u001b[2m\u001b[36m(pid=93772)\u001b[0m demo batch len : 256 \n",
      "\u001b[2m\u001b[36m(pid=93772)\u001b[0m agent batch len : 256 \n",
      "\u001b[2m\u001b[36m(pid=93772)\u001b[0m demo batch len : 256 \n",
      "\u001b[2m\u001b[36m(pid=93772)\u001b[0m agent batch len : 256 \n",
      "\u001b[2m\u001b[36m(pid=93772)\u001b[0m demo batch len : 256 \n",
      "\u001b[2m\u001b[36m(pid=93772)\u001b[0m agent batch len : 256 \n",
      "\u001b[2m\u001b[36m(pid=93772)\u001b[0m demo batch len : 256 \n",
      "\u001b[2m\u001b[36m(pid=93772)\u001b[0m agent batch len : 256 \n",
      "\u001b[2m\u001b[36m(pid=93772)\u001b[0m demo batch len : 256 \n",
      "\u001b[2m\u001b[36m(pid=93772)\u001b[0m agent batch len : 256 \n",
      "\u001b[2m\u001b[36m(pid=93772)\u001b[0m demo batch len : 256 \n",
      "\u001b[2m\u001b[36m(pid=93772)\u001b[0m agent batch len : 256 \n",
      "\u001b[2m\u001b[36m(pid=93772)\u001b[0m demo batch len : 256 \n",
      "\u001b[2m\u001b[36m(pid=93772)\u001b[0m Count : 30 leaner_network updated\n",
      "\u001b[2m\u001b[36m(pid=93772)\u001b[0m agent batch len : 256 \n",
      "\u001b[2m\u001b[36m(pid=93772)\u001b[0m demo batch len : 256 \n",
      "\u001b[2m\u001b[36m(pid=93772)\u001b[0m agent batch len : 256 \n",
      "\u001b[2m\u001b[36m(pid=93772)\u001b[0m demo batch len : 256 \n",
      "\u001b[2m\u001b[36m(pid=93772)\u001b[0m agent batch len : 256 \n",
      "\u001b[2m\u001b[36m(pid=93772)\u001b[0m demo batch len : 256 \n",
      "\u001b[2m\u001b[36m(pid=93772)\u001b[0m agent batch len : 256 \n",
      "\u001b[2m\u001b[36m(pid=93772)\u001b[0m demo batch len : 256 \n",
      "\u001b[2m\u001b[36m(pid=93772)\u001b[0m agent batch len : 256 \n",
      "\u001b[2m\u001b[36m(pid=93772)\u001b[0m demo batch len : 256 \n",
      "\u001b[2m\u001b[36m(pid=93772)\u001b[0m agent batch len : 256 \n",
      "\u001b[2m\u001b[36m(pid=93772)\u001b[0m demo batch len : 256 \n",
      "\u001b[2m\u001b[36m(pid=93772)\u001b[0m agent batch len : 256 \n",
      "\u001b[2m\u001b[36m(pid=93772)\u001b[0m demo batch len : 256 \n",
      "\u001b[2m\u001b[36m(pid=93772)\u001b[0m agent batch len : 256 \n",
      "\u001b[2m\u001b[36m(pid=93772)\u001b[0m demo batch len : 256 \n",
      "\u001b[2m\u001b[36m(pid=93772)\u001b[0m agent batch len : 256 \n",
      "\u001b[2m\u001b[36m(pid=93772)\u001b[0m demo batch len : 256 \n",
      "\u001b[2m\u001b[36m(pid=93772)\u001b[0m agent batch len : 256 \n",
      "\u001b[2m\u001b[36m(pid=93772)\u001b[0m demo batch len : 256 \n",
      "\u001b[2m\u001b[36m(pid=93772)\u001b[0m Count : 40 leaner_network updated\n",
      "\u001b[2m\u001b[36m(pid=93772)\u001b[0m agent batch len : 256 \n",
      "\u001b[2m\u001b[36m(pid=93772)\u001b[0m demo batch len : 256 \n",
      "\u001b[2m\u001b[36m(pid=93772)\u001b[0m agent batch len : 256 \n",
      "\u001b[2m\u001b[36m(pid=93772)\u001b[0m demo batch len : 256 \n",
      "\u001b[2m\u001b[36m(pid=93772)\u001b[0m agent batch len : 256 \n",
      "\u001b[2m\u001b[36m(pid=93772)\u001b[0m demo batch len : 256 \n",
      "\u001b[2m\u001b[36m(pid=93772)\u001b[0m agent batch len : 256 \n",
      "\u001b[2m\u001b[36m(pid=93772)\u001b[0m demo batch len : 256 \n",
      "\u001b[2m\u001b[36m(pid=93772)\u001b[0m agent batch len : 256 \n",
      "\u001b[2m\u001b[36m(pid=93772)\u001b[0m demo batch len : 256 \n",
      "\u001b[2m\u001b[36m(pid=93772)\u001b[0m agent batch len : 256 \n",
      "\u001b[2m\u001b[36m(pid=93772)\u001b[0m demo batch len : 256 \n",
      "\u001b[2m\u001b[36m(pid=93772)\u001b[0m agent batch len : 256 \n",
      "\u001b[2m\u001b[36m(pid=93772)\u001b[0m demo batch len : 256 \n",
      "\u001b[2m\u001b[36m(pid=93772)\u001b[0m agent batch len : 256 \n",
      "\u001b[2m\u001b[36m(pid=93772)\u001b[0m demo batch len : 256 \n",
      "\u001b[2m\u001b[36m(pid=93772)\u001b[0m agent batch len : 256 \n",
      "\u001b[2m\u001b[36m(pid=93772)\u001b[0m demo batch len : 256 \n",
      "\u001b[2m\u001b[36m(pid=93776)\u001b[0m 15771\n",
      "\u001b[2m\u001b[36m(pid=93776)\u001b[0m 0 episode is done\n",
      "\u001b[2m\u001b[36m(pid=93776)\u001b[0m total rewards : 1 \n",
      "\u001b[2m\u001b[36m(pid=93772)\u001b[0m agent batch len : 256 \n",
      "\u001b[2m\u001b[36m(pid=93772)\u001b[0m demo batch len : 256 \n",
      "\u001b[2m\u001b[36m(pid=93772)\u001b[0m Count : 50 leaner_network updated\n",
      "\u001b[2m\u001b[36m(pid=93772)\u001b[0m agent batch len : 256 \n",
      "\u001b[2m\u001b[36m(pid=93772)\u001b[0m demo batch len : 256 \n",
      "\u001b[2m\u001b[36m(pid=93775)\u001b[0m 16000\n",
      "\u001b[2m\u001b[36m(pid=93775)\u001b[0m 0 episode is done\n",
      "\u001b[2m\u001b[36m(pid=93775)\u001b[0m total rewards : 0 \n",
      "\u001b[2m\u001b[36m(pid=93772)\u001b[0m agent batch len : 256 \n",
      "\u001b[2m\u001b[36m(pid=93772)\u001b[0m demo batch len : 256 \n",
      "\u001b[2m\u001b[36m(pid=93772)\u001b[0m agent batch len : 256 \n",
      "\u001b[2m\u001b[36m(pid=93772)\u001b[0m demo batch len : 256 \n",
      "\u001b[2m\u001b[36m(pid=93772)\u001b[0m agent batch len : 256 \n",
      "\u001b[2m\u001b[36m(pid=93772)\u001b[0m demo batch len : 256 \n",
      "\u001b[2m\u001b[36m(pid=93772)\u001b[0m agent batch len : 256 \n",
      "\u001b[2m\u001b[36m(pid=93772)\u001b[0m demo batch len : 256 \n",
      "\u001b[2m\u001b[36m(pid=93772)\u001b[0m agent batch len : 256 \n",
      "\u001b[2m\u001b[36m(pid=93772)\u001b[0m demo batch len : 256 \n",
      "\u001b[2m\u001b[36m(pid=93772)\u001b[0m agent batch len : 256 \n",
      "\u001b[2m\u001b[36m(pid=93772)\u001b[0m demo batch len : 256 \n",
      "\u001b[2m\u001b[36m(pid=93772)\u001b[0m agent batch len : 256 \n",
      "\u001b[2m\u001b[36m(pid=93772)\u001b[0m demo batch len : 256 \n",
      "\u001b[2m\u001b[36m(pid=93772)\u001b[0m agent batch len : 256 \n",
      "\u001b[2m\u001b[36m(pid=93772)\u001b[0m demo batch len : 256 \n",
      "\u001b[2m\u001b[36m(pid=93772)\u001b[0m agent batch len : 256 \n",
      "\u001b[2m\u001b[36m(pid=93772)\u001b[0m demo batch len : 256 \n",
      "\u001b[2m\u001b[36m(pid=93772)\u001b[0m Count : 60 leaner_network updated\n",
      "\u001b[2m\u001b[36m(pid=93772)\u001b[0m agent batch len : 256 \n",
      "\u001b[2m\u001b[36m(pid=93772)\u001b[0m demo batch len : 256 \n",
      "\u001b[2m\u001b[36m(pid=93772)\u001b[0m agent batch len : 256 \n",
      "\u001b[2m\u001b[36m(pid=93772)\u001b[0m demo batch len : 256 \n",
      "\u001b[2m\u001b[36m(pid=93772)\u001b[0m agent batch len : 256 \n",
      "\u001b[2m\u001b[36m(pid=93772)\u001b[0m demo batch len : 256 \n",
      "\u001b[2m\u001b[36m(pid=93772)\u001b[0m agent batch len : 256 \n",
      "\u001b[2m\u001b[36m(pid=93772)\u001b[0m demo batch len : 256 \n",
      "\u001b[2m\u001b[36m(pid=93772)\u001b[0m agent batch len : 256 \n",
      "\u001b[2m\u001b[36m(pid=93772)\u001b[0m demo batch len : 256 \n",
      "\u001b[2m\u001b[36m(pid=93772)\u001b[0m agent batch len : 256 \n",
      "\u001b[2m\u001b[36m(pid=93772)\u001b[0m demo batch len : 256 \n",
      "\u001b[2m\u001b[36m(pid=93772)\u001b[0m agent batch len : 256 \n",
      "\u001b[2m\u001b[36m(pid=93772)\u001b[0m demo batch len : 256 \n",
      "\u001b[2m\u001b[36m(pid=93772)\u001b[0m agent batch len : 256 \n",
      "\u001b[2m\u001b[36m(pid=93772)\u001b[0m demo batch len : 256 \n",
      "\u001b[2m\u001b[36m(pid=93772)\u001b[0m agent batch len : 256 \n",
      "\u001b[2m\u001b[36m(pid=93772)\u001b[0m demo batch len : 256 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=93774)\u001b[0m /home/kukjin/Study/RL/softwardcapstonedesign/ApexDQFD/utils.py:101: RuntimeWarning: divide by zero encountered in power\n",
      "\u001b[2m\u001b[36m(pid=93774)\u001b[0m   is_weight = np.power(self.tree.n_entries * sampling_probabilities, -self.beta)\n",
      "\u001b[2m\u001b[36m(pid=93774)\u001b[0m /home/kukjin/Study/RL/softwardcapstonedesign/ApexDQFD/utils.py:102: RuntimeWarning: invalid value encountered in true_divide\n",
      "\u001b[2m\u001b[36m(pid=93774)\u001b[0m   is_weight /= (is_weight.max() + 1e-5)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=93772)\u001b[0m agent batch len : 256 \n",
      "\u001b[2m\u001b[36m(pid=93772)\u001b[0m return learner network\n",
      "\u001b[2m\u001b[36m(pid=93772)\u001b[0m return learner network\n",
      "\u001b[2m\u001b[36m(pid=93776)\u001b[0m actor network is updated \n",
      "\u001b[2m\u001b[36m(pid=93775)\u001b[0m actor network is updated \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-06-06 23:45:34,290\tERROR worker.py:1056 -- Possible unhandled error from worker: \u001b[36mray::Learner.update_network()\u001b[39m (pid=93772, ip=192.168.0.22)\n",
      "  File \"python/ray/_raylet.pyx\", line 505, in ray._raylet.execute_task\n",
      "  File \"python/ray/_raylet.pyx\", line 449, in ray._raylet.execute_task.function_executor\n",
      "  File \"/home/kukjin/anaconda3/envs/minerl/lib/python3.7/site-packages/ray/_private/function_manager.py\", line 556, in actor_method_executor\n",
      "    return method(__ray_actor, *args, **kwargs)\n",
      "  File \"<ipython-input-4-4b5e34112ebc>\", line 56, in update_network\n",
      "TypeError: cannot unpack non-iterable int object\n",
      "\u001b[2m\u001b[36m(pid=93776)\u001b[0m MineRL agent is public, connect on port 12340 with Minecraft 1.11\n"
     ]
    }
   ],
   "source": [
    "update = learner.update_network.remote(memory, demos, batch_size, optimizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "accurate-scope",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "defined-quarterly",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "foreign-synthesis",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
