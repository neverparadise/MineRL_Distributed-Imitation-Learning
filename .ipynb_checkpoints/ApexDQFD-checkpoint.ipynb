{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kukjin/anaconda3/envs/minerl/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n",
      "/home/kukjin/anaconda3/envs/minerl/lib/python3.7/site-packages/ray/autoscaler/_private/cli_logger.py:61: FutureWarning: Not all Ray CLI dependencies were found. In Ray 1.4+, the Ray CLI, autoscaler, and dashboard will only be usable via `pip install 'ray[default]'`. Please update your install command.\n",
      "  \"update your install command.\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "from model import DQN\n",
    "import os\n",
    "import minerl\n",
    "import gym\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import wandb\n",
    "import ray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from _collections import deque\n",
    "from utils import *\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mneverparadise\u001b[0m (use `wandb login --relogin` to force relogin)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.10.30 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Tracking run with wandb version 0.10.26<br/>\n",
       "                Syncing run <strong style=\"color:#cdcd00\">pious-dew-3</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://wandb.ai/neverparadise/apex_dqfd\" target=\"_blank\">https://wandb.ai/neverparadise/apex_dqfd</a><br/>\n",
       "                Run page: <a href=\"https://wandb.ai/neverparadise/apex_dqfd/runs/17zm3d3e\" target=\"_blank\">https://wandb.ai/neverparadise/apex_dqfd/runs/17zm3d3e</a><br/>\n",
       "                Run data is saved locally in <code>/home/kukjin/Study/RL/소프트웨어 융합 캡스톤 디자인/ApexDQFD/wandb/run-20210520_043103-17zm3d3e</code><br/><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h1>Run(17zm3d3e)</h1><iframe src=\"https://wandb.ai/neverparadise/apex_dqfd/runs/17zm3d3e\" style=\"border:none;width:100%;height:400px\"></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x7fecd78aa8d0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from subprocess import call\n",
    "call([\"wandb\", \"login\", \"e694c5143ff8b3ba1e2b275f0ddff63443464b98\"])\n",
    "wandb.init(group=\"pre-train\", project='apex_dqfd', entity='neverparadise')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#하이퍼 파라미터\n",
    "learning_rate = 0.0003\n",
    "gamma = 0.999\n",
    "buffer_limit = 50000\n",
    "L1 = 0.9\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def margin_loss(q_value, action, demo, weigths):\n",
    "    ae = F.one_hot(action, num_classes=19)\n",
    "    zero_indices = (ae == 0)\n",
    "    one_indices = (ae == 1)\n",
    "    ae[zero_indices] = 1\n",
    "    ae[one_indices] = 0\n",
    "    ae = ae.to(float)\n",
    "    max_value = torch.max(q_value + ae, axis=1)\n",
    "\n",
    "    ae = F.one_hot(action, num_classes=19)\n",
    "    ae = ae.to(float)\n",
    "\n",
    "    J_e = torch.abs(torch.sum(q_value * ae,axis=1) - max_value.values)\n",
    "    J_e = torch.mean(J_e * weigths * demo)\n",
    "    return J_e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_dqn(policy_net, target_net, demos, batch_size, demo_prob, optimizer):\n",
    "    demo_batch, idxs, is_weights = demos.sample.remote(batch_size)\n",
    "    # demo_batch = (batch_size, state, action, reward, next_state, done, n_rewards)\n",
    "    #print(len(demo_batch[0])) # 0번째 배치이므로 0이 나옴\n",
    "    state_list = []\n",
    "    action_list = []\n",
    "    reward_list =[]\n",
    "    next_state_list = []\n",
    "    done_mask_list = []\n",
    "    n_rewards_list = []\n",
    "\n",
    "    for transition in demo_batch:\n",
    "        s, a, r, s_prime, done_mask, n_rewards = transition\n",
    "        state_list.append(s)\n",
    "        action_list.append([a])\n",
    "        reward_list.append([r])\n",
    "        next_state_list.append(s_prime)\n",
    "        done_mask_list.append([done_mask])\n",
    "        n_rewards_list.append([n_rewards])\n",
    "\n",
    "    #a = state_list\n",
    "    #b = torch.tensor(action_list, dtype=torch.int64)\n",
    "    #c = torch.tensor(reward_list)\n",
    "    #d = next_state_list\n",
    "    #e = torch.tensor(done_mask_list)\n",
    "    #f = torch.tensor(n_rewards_list)\n",
    "\n",
    "    s = torch.stack(state_list).float().to(device)\n",
    "    a = torch.tensor(action_list, dtype=torch.int64).to(device)\n",
    "    r =  torch.tensor(reward_list).to(device)\n",
    "    s_prime = torch.stack(next_state_list).float().to(device)\n",
    "    done_mask = torch.tensor(done_mask_list).float().to(device)\n",
    "    nr =  torch.tensor(n_rewards_list).to(device)\n",
    "\n",
    "    q_vals = policy_net(s)\n",
    "    state_action_values = q_vals.gather(1, a)\n",
    "\n",
    "    # comparing the q values to the values expected using the next states and reward\n",
    "    next_state_values = target_net(s_prime).max(1)[0].unsqueeze(1)\n",
    "    target = r + (next_state_values * gamma)\n",
    "\n",
    "    # calculating the q loss, n-step return lossm supervised_loss\n",
    "    is_weights = torch.FloatTensor(is_weights).to(device)\n",
    "    q_loss = (is_weights * F.mse_loss(state_action_values, target)).mean()\n",
    "    n_step_loss = (state_action_values.max(1)[0] + nr).mean()\n",
    "    supervised_loss = margin_loss(q_vals, a, 1, 1)\n",
    "\n",
    "    loss = q_loss + supervised_loss + n_step_loss\n",
    "    wandb.log({\"Q-loss\" : q_loss.item()})\n",
    "    wandb.log({\"n-step loss\" : n_step_loss.item()})\n",
    "    wandb.log({\"super_vised loss\" : supervised_loss.item()})\n",
    "    wandb.log({\"total loss\" : loss.item()})\n",
    "    \n",
    "    errors = torch.abs(state_action_values - target).data.cpu()\n",
    "    errors = errors.numpy()\n",
    "    # update priority\n",
    "    for i in range(batch_size):\n",
    "        idx = idxs[i]\n",
    "        memory.update(idx, errors[i])\n",
    "\n",
    "    # optimization step and logging\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    torch.nn.utils.clip_grad_norm(policy_net.parameters(), 100)\n",
    "    optimizer.step()\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from st import SumTree\n",
    "@ray.remote\n",
    "class Memory:  # stored as ( s, a, r, s_, n_rewards ) in SumTree\n",
    "    e = 0.01\n",
    "    a = 0.6\n",
    "    beta = 0.4\n",
    "    beta_increment_per_sampling = 0.001\n",
    "\n",
    "    def __init__(self, capacity):\n",
    "        self.tree = SumTree(capacity)\n",
    "        self.capacity = capacity\n",
    "\n",
    "    def _get_priority(self, error):\n",
    "        return (np.abs(error) + self.e) ** self.a\n",
    "\n",
    "    def add(self, error, sample):\n",
    "        p = self._get_priority(error)\n",
    "        self.tree.add(p, sample)\n",
    "\n",
    "    def size(self):\n",
    "        return self.tree.n_entries\n",
    "\n",
    "    def sample(self, n):\n",
    "        batch = []\n",
    "        idxs = []\n",
    "        segment = self.tree.total() / n\n",
    "        priorities = []\n",
    "\n",
    "        self.beta = np.min([1., self.beta + self.beta_increment_per_sampling])\n",
    "\n",
    "        for i in range(n):\n",
    "            a = segment * i\n",
    "            b = segment * (i + 1)\n",
    "\n",
    "            s = random.uniform(a, b)\n",
    "            (idx, p, data) = self.tree.get(s)\n",
    "            priorities.append(p)\n",
    "            batch.append(data)\n",
    "            idxs.append(idx)\n",
    "\n",
    "        sampling_probabilities = priorities / self.tree.total()\n",
    "        is_weight = np.power(self.tree.n_entries * sampling_probabilities, -self.beta)\n",
    "        is_weight /= is_weight.max() + 1e-5\n",
    "\n",
    "        return batch, idxs, is_weight\n",
    "\n",
    "    def update(self, idx, error):\n",
    "        p = self._get_priority(error)\n",
    "        self.tree.update(idx, p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def append_sample(memory, model, target_model, state, action, reward, next_state, done, n_rewards):\n",
    "    # Caluclating Priority (TD Error)\n",
    "    target = model(state.float()).data\n",
    "    old_val = target[0][action].cpu()\n",
    "    target_val = target_model(next_state.float()).data.cpu()\n",
    "    if done:\n",
    "        target[0][action] = reward\n",
    "    else:\n",
    "        target[0][action] = reward + 0.99 * torch.max(target_val)\n",
    "\n",
    "    error = abs(old_val - target[0][action])\n",
    "    error = error.cpu() \n",
    "    memory.add.remote(error, [state, action, reward, next_state, done, n_rewards])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_train(env_name, rep_buffer, policy_net, target_net, optimizer,threshold=10, num_epochs=1, batch_size=16, seq_len=10, gamma=0.99):\n",
    "    data = minerl.data.make(env_name)\n",
    "    demo_num = 0\n",
    "    for s_batch, a_batch, r_batch, ns_batch, d_batch in data.batch_iter(num_epochs=num_epochs, batch_size=batch_size,\n",
    "                                                                        seq_len=seq_len):\n",
    "        demo_num += 1\n",
    "        print(demo_num)\n",
    "        if r_batch.sum() < threshold:\n",
    "            continue\n",
    "        \"\"\"\n",
    "        state_batch : (batch_size, seq_len, 64, 64, 3)\n",
    "        action_batch : (batch_size, seq_len, action['property'].shape) ex camera = 2 otherwise 1\n",
    "\n",
    "        reward_batch : (batch_size, seq_len)\n",
    "        next_state_batch : (batch_size, seq_len, 64, 64, 3)\n",
    "        done_batch : (batch_size, seq_len)\n",
    "\n",
    "    \n",
    "        reward, _ = stats.mode(r_batch, axis=1)\n",
    "        reward = np.squeeze(reward)\n",
    "        done, _ = stats.mode(d_batch, axis=1)\n",
    "        done = np.squeeze(done)\n",
    "        \"\"\"\n",
    "        parse_ts = 0\n",
    "\n",
    "        # 각 state에 대한 action discretize를 위해 반복문을 사용\n",
    "        batch_length = (s_batch['pov'].shape)[0]  # (batch, seq, 64, 64, 3)[0]\n",
    "        for i in range(0, batch_length):\n",
    "            episode_start_ts = 0\n",
    "\n",
    "            n_step = 10\n",
    "            n_step_state_buffer = deque(maxlen=n_step)\n",
    "            n_step_action_buffer = deque(maxlen=n_step)\n",
    "            n_step_reward_buffer = deque(maxlen=n_step)\n",
    "            n_step_n_rewards_buffer = deque(maxlen=n_step)\n",
    "            n_step_next_state_buffer = deque(maxlen=n_step)\n",
    "            n_step_done_buffer = deque(maxlen=n_step)\n",
    "            gamma_list = [gamma ** i for i in range(n_step)]\n",
    "\n",
    "            for j in range(0, seq_len):\n",
    "                av = a_batch['attack'][i][j]  # attack value\n",
    "                aj = a_batch['jump'][i][j]  # jump value\n",
    "                af = a_batch['forward'][i][j]  # forward value\n",
    "                ab = a_batch['back'][i][j]  # back value\n",
    "                al = a_batch['left'][i][j]  # left value\n",
    "                ar = a_batch['right'][i][j]  # right value\n",
    "                va = a_batch['camera'][i][j][0]  # vertical angle and\n",
    "                ha = a_batch['camera'][i][j][1]  # horizontal angle\n",
    "\n",
    "                camera_thresholds = (abs(va) + abs(ha)) / 2.0\n",
    "                # 카메라를 움직이는 경우\n",
    "                if (camera_thresholds > 2.5):\n",
    "                    # camera = [0, -5]\n",
    "                    if abs(va) < abs(ha) and ha < 0:\n",
    "                        if av == 0:\n",
    "                            action_index = 0\n",
    "                        else:\n",
    "                            action_index = 1\n",
    "                    # camera = [0, 5]\n",
    "                    elif abs(va) < abs(ha) and ha > 0:\n",
    "                        if av == 0:\n",
    "                            action_index = 2\n",
    "                        else:\n",
    "                            action_index = 3\n",
    "                    # camera = [-5, 0]\n",
    "                    elif abs(va) > abs(ha) and ha < 0:\n",
    "                        if av == 0:\n",
    "                            action_index = 4\n",
    "                        else:\n",
    "                            action_index = 5\n",
    "                    # camera = [5, 0]\n",
    "                    elif abs(va) > abs(ha) and ha > 0:\n",
    "                        if av == 0:\n",
    "                            action_index = 6\n",
    "                        else:\n",
    "                            action_index = 7\n",
    "\n",
    "                            # 카메라를 안움직이는 경우\n",
    "                # 점프하는 경우\n",
    "                elif (aj == 1):\n",
    "                    if (af == 0):\n",
    "                        action_index = 8\n",
    "                    else:\n",
    "                        action_index = 9\n",
    "\n",
    "                # 앞으로 가는 경우\n",
    "                elif (af == 1):\n",
    "                    if (av == 0):\n",
    "                        action_index = 10\n",
    "                    else:\n",
    "                        action_index = 11\n",
    "\n",
    "                # 뒤로 가는 경우\n",
    "                elif (ab == 1):\n",
    "                    if (av == 0):\n",
    "                        action_index = 12\n",
    "                    else:\n",
    "                        action_index = 13\n",
    "\n",
    "                # 왼쪽으로 가는 경우\n",
    "                elif (al == 1):\n",
    "                    if (av == 0):\n",
    "                        action_index = 14\n",
    "                    else:\n",
    "                        action_index = 15\n",
    "\n",
    "                # 오른쪽으로 가는 경우\n",
    "                elif (ar == 1):\n",
    "                    if (av == 0):\n",
    "                        action_index = 16\n",
    "                    else:\n",
    "                        action_index = 17\n",
    "\n",
    "                # 카메라, 움직임이 다 0이고 공격만 하는 것\n",
    "                else:\n",
    "                    if (av == 0):\n",
    "                        continue\n",
    "                    else:\n",
    "                        action_index = 18\n",
    "\n",
    "                a_index = torch.LongTensor([action_index]).cpu()\n",
    "                curr_obs = converter2(s_batch['pov'][i][j]).float().cpu()\n",
    "                _obs = converter2(ns_batch['pov'][i][j]).float().cpu()\n",
    "                _reward = torch.FloatTensor([r_batch[i][j]]).cpu()\n",
    "                _done = d_batch[i][j]  # .astype(int)\n",
    "\n",
    "                n_step_state_buffer.append(curr_obs)\n",
    "                n_step_action_buffer.append(a_index)\n",
    "                n_step_reward_buffer.append(_reward)\n",
    "                n_step_next_state_buffer.append(_obs)\n",
    "                n_step_done_buffer.append(_done)\n",
    "                n_rewards = sum([gamma * reward for gamma, reward in zip(gamma_list, n_step_reward_buffer)])\n",
    "                n_step_n_rewards_buffer.append(n_rewards)\n",
    "                \n",
    "\n",
    "                append_sample(rep_buffer, policy_net, target_net, n_step_state_buffer[j], \\\n",
    "                              n_step_action_buffer[j], n_step_reward_buffer[j], \\\n",
    "                              n_step_next_state_buffer[j], \\\n",
    "                              n_step_done_buffer[j], \\\n",
    "                              n_step_n_rewards_buffer[j])\n",
    "                episode_start_ts += 1\n",
    "                parse_ts += 1\n",
    "                # if episode done we reset\n",
    "                if _done:\n",
    "                    break\n",
    "\n",
    "        # replay is over emptying the deques\n",
    "        #if rep_buffer.size() > rep_buffer.buffer_limit:\n",
    "        #    rep_buffer.buffer.popleft()\n",
    "        print('Parse finished. {} expert samples added.'.format(parse_ts))\n",
    "        train_dqn(policy_net, target_net, rep_buffer, batch_size, 1, optimizer)\n",
    "        torch.save(policy_net.state_dict(), model_path + 'pre_trained.pth')        \n",
    "        if demo_num % 5 == 0 and demo_num != 0:\n",
    "        # 특정 반복 수가 되면 타겟 네트워크도 업데이트\n",
    "            print(\"target network updated\")\n",
    "            target_net.load_state_dict(policy_net.state_dict())\n",
    "        print(\"train {} step finished\".format(demo_num))\n",
    "    print('pre_train finished')\n",
    "    return rep_buffer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_episodes = 1000\n",
    "startEpsilon = 1.0\n",
    "endEpsilon = 0.05\n",
    "epsilon = startEpsilon\n",
    "\n",
    "root_path = os.curdir\n",
    "model_path = root_path + '/dqn_model/'\n",
    "\n",
    "stepDrop = (startEpsilon - endEpsilon) / total_episodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "@ray.remote\n",
    "class Actor:\n",
    "    def __init__(self, shared_network, shared_memory, actor_idx, epsilon):\n",
    "        # environment initialization\n",
    "        self.env = gym.make(\"MineRLTreechop-v0\")\n",
    "        self.port_number = int(\"12340\")+index\n",
    "        self.env.make_interactive(port=self.port_number, realtime=False)\n",
    "        \n",
    "        # network initalization\n",
    "        self.shared_network = shared_network\n",
    "        self.shared_memory = shared_memory\n",
    "        self.actor_network = DQN(19).cpu()\n",
    "        self.actor_target_network = DQN(19).cpu()\n",
    "        \n",
    "        self.actor_network.load_state_dict(self.shared_network.state_dict(), map_location='cpu')\n",
    "        self.actor_target_network.load_state_dict(self.actor_network.load_state_dict(), map_location='cuda:0')\n",
    "        \n",
    "        # exploring info\n",
    "        self.actor_idx = actor_idx\n",
    "        self.epsilon = epsilon\n",
    "        self.max_step = 10\n",
    "        self.local_buffer_size = 100\n",
    "        self.local_buffer = deque(maxlen=self.local_buffer_size)\n",
    "        \n",
    "    # 1. 네트워크 파라미터 복사\n",
    "    # 2. 환경 탐험 (초기화, 행동)\n",
    "    # 3. 로컬버퍼에 저장\n",
    "    # 4. priority 계산\n",
    "    # 5. 글로벌 버퍼에 저장\n",
    "    # 6. 주기적으로 네트워크 업데이트 \n",
    "    \n",
    "    # 각 환경 인스턴스에서 각 엡실론에 따라 탐험을 진행한다.\n",
    "    # 탐험 과정에서 local buffer에 transition들을 저장한다.\n",
    "    # local buffer의 개수가 특정 개수 이상이면 global buffer에 추가해준다. \n",
    "    \n",
    "    def explore(self):\n",
    "        for num_epi in range(self.max_step):\n",
    "            obs = self.env.reset()\n",
    "            state = converter(obs).cpu()\n",
    "            state = state.float()\n",
    "            done = False\n",
    "            total_reward = 0\n",
    "            steps = 0\n",
    "            total_steps = 0\n",
    "            self.epsilon = 0.5\n",
    "            if(self.epsilon > endEpsilon):\n",
    "                self.epsilon -= stepDrop / (self.actor_idx + 1)\n",
    "            \n",
    "            n_step = 10\n",
    "            n_step_state_buffer = deque(maxlen=n_step)\n",
    "            n_step_action_buffer = deque(maxlen=n_step)\n",
    "            n_step_reward_buffer = deque(maxlen=n_step)\n",
    "            n_step_n_rewards_buffer = deque(maxlen=n_step)\n",
    "            n_step_next_state_buffer = deque(maxlen=n_step)\n",
    "            n_step_done_buffer = deque(maxlen=n_step)\n",
    "            gamma_list = [gamma ** i for i in range(n_step)]\n",
    "            \n",
    "            while not done:\n",
    "                steps += 1\n",
    "                total_steps += 1\n",
    "                a_out = self.actor_network.sample_action(state,self.epsilon)\n",
    "                action_index = a_out\n",
    "                action = make_action(env, action_index)\n",
    "                obs_prime, reward, done, info = env.step(action)\n",
    "                total_reward += reward\n",
    "                state_prime = converter(obs_prime)\n",
    "                \n",
    "                # local buffer add\n",
    "                n_step_state_buffer.append(state)\n",
    "                n_step_action_buffer.append(action_index)\n",
    "                n_step_reward_buffer.append(reward)\n",
    "                n_step_next_state_buffer.append(state_prime)\n",
    "                n_step_done_buffer.append(done)\n",
    "                n_rewards = sum([gamma * reward for gamma, reward in zip(gamma_list, n_step_reward_buffer)])\n",
    "                n_step_n_rewards_buffer.append(n_rewards)\n",
    "                \n",
    "                \n",
    "                if (len(n_step_state_buffer) >= n_step):\n",
    "                    # LocalBuffer Get\n",
    "                    # Compute Priorities\n",
    "                    for i in range(n_step):\n",
    "                        append_sample(shared_memory, self.actor_network , self.actor_target_network, \\\n",
    "                              n_step_state_buffer[i], \\\n",
    "                              n_step_action_buffer[i], n_step_reward_buffer[i], \\\n",
    "                              n_step_next_state_buffer[i], \\\n",
    "                              n_step_done_buffer[i], \\\n",
    "                              n_step_n_rewards_buffer[i])\n",
    "                        if(n_step_done_buffer[i]):\n",
    "                            break\n",
    "                    \n",
    "                    \n",
    "                state = state_prime.float().cpu()\n",
    "                if done:\n",
    "                    break\n",
    "\n",
    "            if done:\n",
    "                print(\"%d episode is done\" % num_epi)\n",
    "                print(\"total rewards : %d \" % total_reward)\n",
    "                break\n",
    "            \n",
    "            if (num_epi % 5 == 0 and num_epi != 0):\n",
    "                self.update_params()\n",
    "\n",
    "        env.close()\n",
    "    \n",
    "    def update_params(self):\n",
    "        self.actor_network.load_state_dict(self.shared_network.state_dict(), map_location='cpu')\n",
    "\n",
    "\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Learner:\n",
    "    def __init__(self, network, batch_size):\n",
    "        self.learner_network = DQN(19).to(device)\n",
    "        self.learner_target_network = DQN(19).to(device)\n",
    "        \n",
    "        self.learner_network.load_state_dict(network.state_dict(), map_location='cuda:0')\n",
    "        self.learner_target_network.load_state_dict(network.state_dict(), map_location='cuda:0')\n",
    "        \n",
    "        self.batch_size = batch_size\n",
    "    # 1. sampling\n",
    "    # 2. calculate gradient\n",
    "    # 3. weight update\n",
    "    # 4. compute priorities\n",
    "    # 5. priorities of buffer update\n",
    "    # 6. remove old memory \n",
    "    def get_network(self):\n",
    "        return self.learner_network\n",
    "\n",
    "    def update_network(memory, demos, batch_size, demo_prob, optimizer):\n",
    "        agent_batch, agent_idxs, agent_weights = memory.sample.remote(batch_size)\n",
    "        demo_batch, demo_idxs, demo_weights = demos.sample.remote(batch_size)\n",
    "        \n",
    "        # demo_batch = (batch_size, state, action, reward, next_state, done, n_rewards)\n",
    "        #print(len(demo_batch[0])) # 0번째 배치이므로 0이 나옴\n",
    "        state_list = []\n",
    "        action_list = []\n",
    "        reward_list =[]\n",
    "        next_state_list = []\n",
    "        done_mask_list = []\n",
    "        n_rewards_list = []\n",
    "\n",
    "        for agent_transition, expert_transition in zip(agent_batch, demo_batch):\n",
    "            s, a, r, s_prime, done_mask, n_rewards = agent_transition\n",
    "            state_list.append(s)\n",
    "            action_list.append([a])\n",
    "            reward_list.append([r])\n",
    "            next_state_list.append(s_prime)\n",
    "            done_mask_list.append([done_mask])\n",
    "            n_rewards_list.append([n_rewards])\n",
    "            \n",
    "            s, a, r, s_prime, done_mask, n_rewards = expert_transition\n",
    "            state_list.append(s)\n",
    "            action_list.append([a])\n",
    "            reward_list.append([r])\n",
    "            next_state_list.append(s_prime)\n",
    "            done_mask_list.append([done_mask])\n",
    "            n_rewards_list.append([n_rewards])\n",
    "            \n",
    "\n",
    "        s = torch.stack(state_list).float().to(device)\n",
    "        a = torch.tensor(action_list, dtype=torch.int64).to(device)\n",
    "        r =  torch.tensor(reward_list).to(device)\n",
    "        s_prime = torch.stack(next_state_list).float().to(device)\n",
    "        done_mask = torch.tensor(done_mask_list).float().to(device)\n",
    "        nr =  torch.tensor(n_rewards_list).to(device)\n",
    "        \n",
    "        q_vals = policy_net(s)\n",
    "        state_action_values = q_vals.gather(1, a)\n",
    "\n",
    "        # comparing the q values to the values expected using the next states and reward\n",
    "        next_state_values = target_net(s_prime).max(1)[0].unsqueeze(1)\n",
    "        target = r + (next_state_values * gamma)\n",
    "\n",
    "        # calculating the q loss, n-step return lossm supervised_loss\n",
    "        is_weights = torch.FloatTensor(is_weights).to(device)\n",
    "        q_loss = (is_weights * F.mse_loss(state_action_values, target)).mean()\n",
    "        n_step_loss = (state_action_values.max(1)[0] + nr).mean()\n",
    "        supervised_loss = margin_loss(q_vals, a, 1, 1)\n",
    "\n",
    "        loss = q_loss + supervised_loss + n_step_loss\n",
    "        wandb.log({\"Q-loss\" : q_loss.item()})\n",
    "        wandb.log({\"n-step loss\" : n_step_loss.item()})\n",
    "        wandb.log({\"super_vised loss\" : supervised_loss.item()})\n",
    "        wandb.log({\"total loss\" : loss.item()})\n",
    "\n",
    "        errors = torch.abs(state_action_values - target).data.cpu()\n",
    "        errors = errors.numpy()\n",
    "        # update priority\n",
    "        for i in range(batch_size):\n",
    "            idx = idxs[i]\n",
    "            memory.remote.update(idx, errors[i])\n",
    "\n",
    "        # optimization step and logging\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm(policy_net.parameters(), 100)\n",
    "        optimizer.step()\n",
    "        return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-05-20 04:31:18,052\tINFO services.py:1269 -- View the Ray dashboard at \u001b[1m\u001b[32mhttp://127.0.0.1:8265\u001b[39m\u001b[22m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'node_ip_address': '192.168.0.22',\n",
       " 'raylet_ip_address': '192.168.0.22',\n",
       " 'redis_address': '192.168.0.22:45744',\n",
       " 'object_store_address': '/tmp/ray/session_2021-05-20_04-31-17_273839_75039/sockets/plasma_store',\n",
       " 'raylet_socket_name': '/tmp/ray/session_2021-05-20_04-31-17_273839_75039/sockets/raylet',\n",
       " 'webui_url': '127.0.0.1:8265',\n",
       " 'session_dir': '/tmp/ray/session_2021-05-20_04-31-17_273839_75039',\n",
       " 'metrics_export_port': 60439,\n",
       " 'node_id': '294b738b23e47bb7b3c16e5e7e92476904787d3fccf4d334557331dc'}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ray.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy_net = DQN(19).to(device=device)\n",
    "target_net = DQN(19).to(device=device)\n",
    "target_net.load_state_dict(policy_net.state_dict())\n",
    "memory = Memory.remote(50000)\n",
    "demos = Memory.remote(50000)\n",
    "optimizer = optim.Adam(policy_net.parameters(), lr=learning_rate, weight_decay=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pre_train start\n",
      "pre_train finished\n",
      "pre_train finished\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Actor methods cannot be called directly. Instead of running 'object.size()', try 'object.size.remote()'.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-ccd942baf2d0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mdemos\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpre_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"MineRLTreechop-v0\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdemos\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpolicy_net\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_net\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mthreshold\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1536\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m512\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseq_len\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.99\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"pre_train finished\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdemos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/minerl/lib/python3.7/site-packages/ray/actor.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 107\u001b[0;31m         raise TypeError(\"Actor methods cannot be called directly. Instead \"\n\u001b[0m\u001b[1;32m    108\u001b[0m                         \u001b[0;34mf\"of running 'object.{self._method_name}()', try \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m                         f\"'object.{self._method_name}.remote()'.\")\n",
      "\u001b[0;31mTypeError\u001b[0m: Actor methods cannot be called directly. Instead of running 'object.size()', try 'object.size.remote()'."
     ]
    }
   ],
   "source": [
    "print(\"pre_train start\")\n",
    "demos = pre_train(\"MineRLTreechop-v0\", demos, policy_net, target_net, optimizer, threshold=1536, num_epochs=10, batch_size=512, seq_len=20, gamma=0.99)\n",
    "print(\"pre_train finished\")\n",
    "print(demos.size.remote())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Copy network params from pretrained Agent\n",
    "model_path = './dqn_model/pre_trained.pth'\n",
    "policy_net.load_state_dict(torch.load(model_path, map_location='cuda:0'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=75711)\u001b[0m /home/kukjin/anaconda3/envs/minerl/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=75711)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n",
      "\u001b[2m\u001b[36m(pid=75711)\u001b[0m 2021-05-20 04:34:39,308\tERROR serialization.py:248 -- Attempting to deserialize object on a CUDA device but torch.cuda.is_available() is False. If you are running on a CPU-only machine, please use torch.load with map_location=torch.device('cpu') to map your storages to the CPU.\n",
      "\u001b[2m\u001b[36m(pid=75711)\u001b[0m Traceback (most recent call last):\n",
      "\u001b[2m\u001b[36m(pid=75711)\u001b[0m   File \"/home/kukjin/anaconda3/envs/minerl/lib/python3.7/site-packages/ray/serialization.py\", line 246, in deserialize_objects\n",
      "\u001b[2m\u001b[36m(pid=75711)\u001b[0m     obj = self._deserialize_object(data, metadata, object_ref)\n",
      "\u001b[2m\u001b[36m(pid=75711)\u001b[0m   File \"/home/kukjin/anaconda3/envs/minerl/lib/python3.7/site-packages/ray/serialization.py\", line 188, in _deserialize_object\n",
      "\u001b[2m\u001b[36m(pid=75711)\u001b[0m     return self._deserialize_msgpack_data(data, metadata_fields)\n",
      "\u001b[2m\u001b[36m(pid=75711)\u001b[0m   File \"/home/kukjin/anaconda3/envs/minerl/lib/python3.7/site-packages/ray/serialization.py\", line 166, in _deserialize_msgpack_data\n",
      "\u001b[2m\u001b[36m(pid=75711)\u001b[0m     python_objects = self._deserialize_pickle5_data(pickle5_data)\n",
      "\u001b[2m\u001b[36m(pid=75711)\u001b[0m   File \"/home/kukjin/anaconda3/envs/minerl/lib/python3.7/site-packages/ray/serialization.py\", line 156, in _deserialize_pickle5_data\n",
      "\u001b[2m\u001b[36m(pid=75711)\u001b[0m     obj = pickle.loads(in_band)\n",
      "\u001b[2m\u001b[36m(pid=75711)\u001b[0m   File \"/home/kukjin/anaconda3/envs/minerl/lib/python3.7/site-packages/torch/storage.py\", line 161, in _load_from_bytes\n",
      "\u001b[2m\u001b[36m(pid=75711)\u001b[0m     return torch.load(io.BytesIO(b))\n",
      "\u001b[2m\u001b[36m(pid=75711)\u001b[0m   File \"/home/kukjin/anaconda3/envs/minerl/lib/python3.7/site-packages/torch/serialization.py\", line 593, in load\n",
      "\u001b[2m\u001b[36m(pid=75711)\u001b[0m     return _legacy_load(opened_file, map_location, pickle_module, **pickle_load_args)\n",
      "\u001b[2m\u001b[36m(pid=75711)\u001b[0m   File \"/home/kukjin/anaconda3/envs/minerl/lib/python3.7/site-packages/torch/serialization.py\", line 772, in _legacy_load\n",
      "\u001b[2m\u001b[36m(pid=75711)\u001b[0m     result = unpickler.load()\n",
      "\u001b[2m\u001b[36m(pid=75711)\u001b[0m   File \"/home/kukjin/anaconda3/envs/minerl/lib/python3.7/site-packages/torch/serialization.py\", line 728, in persistent_load\n",
      "\u001b[2m\u001b[36m(pid=75711)\u001b[0m     deserialized_objects[root_key] = restore_location(obj, location)\n",
      "\u001b[2m\u001b[36m(pid=75711)\u001b[0m   File \"/home/kukjin/anaconda3/envs/minerl/lib/python3.7/site-packages/torch/serialization.py\", line 175, in default_restore_location\n",
      "\u001b[2m\u001b[36m(pid=75711)\u001b[0m     result = fn(storage, location)\n",
      "\u001b[2m\u001b[36m(pid=75711)\u001b[0m   File \"/home/kukjin/anaconda3/envs/minerl/lib/python3.7/site-packages/torch/serialization.py\", line 151, in _cuda_deserialize\n",
      "\u001b[2m\u001b[36m(pid=75711)\u001b[0m     device = validate_cuda_device(location)\n",
      "\u001b[2m\u001b[36m(pid=75711)\u001b[0m   File \"/home/kukjin/anaconda3/envs/minerl/lib/python3.7/site-packages/torch/serialization.py\", line 135, in validate_cuda_device\n",
      "\u001b[2m\u001b[36m(pid=75711)\u001b[0m     raise RuntimeError('Attempting to deserialize object on a CUDA '\n",
      "\u001b[2m\u001b[36m(pid=75711)\u001b[0m RuntimeError: Attempting to deserialize object on a CUDA device but torch.cuda.is_available() is False. If you are running on a CPU-only machine, please use torch.load with map_location=torch.device('cpu') to map your storages to the CPU.\n",
      "\u001b[2m\u001b[36m(pid=75711)\u001b[0m 2021-05-20 04:34:39,309\tERROR worker.py:382 -- Exception raised in creation task: The actor died because of an error raised in its creation task, \u001b[36mray::Actor.__init__()\u001b[39m (pid=75711, ip=192.168.0.22)\n",
      "\u001b[2m\u001b[36m(pid=75711)\u001b[0m   File \"python/ray/_raylet.pyx\", line 458, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(pid=75711)\u001b[0m   File \"python/ray/_raylet.pyx\", line 479, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(pid=75711)\u001b[0m   File \"python/ray/_raylet.pyx\", line 349, in ray._raylet.raise_if_dependency_failed\n",
      "\u001b[2m\u001b[36m(pid=75711)\u001b[0m ray.exceptions.RaySystemError: System error: Attempting to deserialize object on a CUDA device but torch.cuda.is_available() is False. If you are running on a CPU-only machine, please use torch.load with map_location=torch.device('cpu') to map your storages to the CPU.\n",
      "\u001b[2m\u001b[36m(pid=75711)\u001b[0m traceback: Traceback (most recent call last):\n",
      "\u001b[2m\u001b[36m(pid=75711)\u001b[0m   File \"/home/kukjin/anaconda3/envs/minerl/lib/python3.7/site-packages/ray/serialization.py\", line 246, in deserialize_objects\n",
      "\u001b[2m\u001b[36m(pid=75711)\u001b[0m     obj = self._deserialize_object(data, metadata, object_ref)\n",
      "\u001b[2m\u001b[36m(pid=75711)\u001b[0m   File \"/home/kukjin/anaconda3/envs/minerl/lib/python3.7/site-packages/ray/serialization.py\", line 188, in _deserialize_object\n",
      "\u001b[2m\u001b[36m(pid=75711)\u001b[0m     return self._deserialize_msgpack_data(data, metadata_fields)\n",
      "\u001b[2m\u001b[36m(pid=75711)\u001b[0m   File \"/home/kukjin/anaconda3/envs/minerl/lib/python3.7/site-packages/ray/serialization.py\", line 166, in _deserialize_msgpack_data\n",
      "\u001b[2m\u001b[36m(pid=75711)\u001b[0m     python_objects = self._deserialize_pickle5_data(pickle5_data)\n",
      "\u001b[2m\u001b[36m(pid=75711)\u001b[0m   File \"/home/kukjin/anaconda3/envs/minerl/lib/python3.7/site-packages/ray/serialization.py\", line 156, in _deserialize_pickle5_data\n",
      "\u001b[2m\u001b[36m(pid=75711)\u001b[0m     obj = pickle.loads(in_band)\n",
      "\u001b[2m\u001b[36m(pid=75711)\u001b[0m   File \"/home/kukjin/anaconda3/envs/minerl/lib/python3.7/site-packages/torch/storage.py\", line 161, in _load_from_bytes\n",
      "\u001b[2m\u001b[36m(pid=75711)\u001b[0m     return torch.load(io.BytesIO(b))\n",
      "\u001b[2m\u001b[36m(pid=75711)\u001b[0m   File \"/home/kukjin/anaconda3/envs/minerl/lib/python3.7/site-packages/torch/serialization.py\", line 593, in load\n",
      "\u001b[2m\u001b[36m(pid=75711)\u001b[0m     return _legacy_load(opened_file, map_location, pickle_module, **pickle_load_args)\n",
      "\u001b[2m\u001b[36m(pid=75711)\u001b[0m   File \"/home/kukjin/anaconda3/envs/minerl/lib/python3.7/site-packages/torch/serialization.py\", line 772, in _legacy_load\n",
      "\u001b[2m\u001b[36m(pid=75711)\u001b[0m     result = unpickler.load()\n",
      "\u001b[2m\u001b[36m(pid=75711)\u001b[0m   File \"/home/kukjin/anaconda3/envs/minerl/lib/python3.7/site-packages/torch/serialization.py\", line 728, in persistent_load\n",
      "\u001b[2m\u001b[36m(pid=75711)\u001b[0m     deserialized_objects[root_key] = restore_location(obj, location)\n",
      "\u001b[2m\u001b[36m(pid=75711)\u001b[0m   File \"/home/kukjin/anaconda3/envs/minerl/lib/python3.7/site-packages/torch/serialization.py\", line 175, in default_restore_location\n",
      "\u001b[2m\u001b[36m(pid=75711)\u001b[0m     result = fn(storage, location)\n",
      "\u001b[2m\u001b[36m(pid=75711)\u001b[0m   File \"/home/kukjin/anaconda3/envs/minerl/lib/python3.7/site-packages/torch/serialization.py\", line 151, in _cuda_deserialize\n",
      "\u001b[2m\u001b[36m(pid=75711)\u001b[0m     device = validate_cuda_device(location)\n",
      "\u001b[2m\u001b[36m(pid=75711)\u001b[0m   File \"/home/kukjin/anaconda3/envs/minerl/lib/python3.7/site-packages/torch/serialization.py\", line 135, in validate_cuda_device\n",
      "\u001b[2m\u001b[36m(pid=75711)\u001b[0m     raise RuntimeError('Attempting to deserialize object on a CUDA '\n",
      "\u001b[2m\u001b[36m(pid=75711)\u001b[0m RuntimeError: Attempting to deserialize object on a CUDA device but torch.cuda.is_available() is False. If you are running on a CPU-only machine, please use torch.load with map_location=torch.device('cpu') to map your storages to the CPU.\n",
      "\u001b[2m\u001b[36m(pid=75716)\u001b[0m /home/kukjin/anaconda3/envs/minerl/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=75716)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n",
      "\u001b[2m\u001b[36m(pid=75716)\u001b[0m 2021-05-20 04:34:39,341\tERROR serialization.py:248 -- Attempting to deserialize object on a CUDA device but torch.cuda.is_available() is False. If you are running on a CPU-only machine, please use torch.load with map_location=torch.device('cpu') to map your storages to the CPU.\n",
      "\u001b[2m\u001b[36m(pid=75716)\u001b[0m Traceback (most recent call last):\n",
      "\u001b[2m\u001b[36m(pid=75716)\u001b[0m   File \"/home/kukjin/anaconda3/envs/minerl/lib/python3.7/site-packages/ray/serialization.py\", line 246, in deserialize_objects\n",
      "\u001b[2m\u001b[36m(pid=75716)\u001b[0m     obj = self._deserialize_object(data, metadata, object_ref)\n",
      "\u001b[2m\u001b[36m(pid=75716)\u001b[0m   File \"/home/kukjin/anaconda3/envs/minerl/lib/python3.7/site-packages/ray/serialization.py\", line 188, in _deserialize_object\n",
      "\u001b[2m\u001b[36m(pid=75716)\u001b[0m     return self._deserialize_msgpack_data(data, metadata_fields)\n",
      "\u001b[2m\u001b[36m(pid=75716)\u001b[0m   File \"/home/kukjin/anaconda3/envs/minerl/lib/python3.7/site-packages/ray/serialization.py\", line 166, in _deserialize_msgpack_data\n",
      "\u001b[2m\u001b[36m(pid=75716)\u001b[0m     python_objects = self._deserialize_pickle5_data(pickle5_data)\n",
      "\u001b[2m\u001b[36m(pid=75716)\u001b[0m   File \"/home/kukjin/anaconda3/envs/minerl/lib/python3.7/site-packages/ray/serialization.py\", line 156, in _deserialize_pickle5_data\n",
      "\u001b[2m\u001b[36m(pid=75716)\u001b[0m     obj = pickle.loads(in_band)\n",
      "\u001b[2m\u001b[36m(pid=75716)\u001b[0m   File \"/home/kukjin/anaconda3/envs/minerl/lib/python3.7/site-packages/torch/storage.py\", line 161, in _load_from_bytes\n",
      "\u001b[2m\u001b[36m(pid=75716)\u001b[0m     return torch.load(io.BytesIO(b))\n",
      "\u001b[2m\u001b[36m(pid=75716)\u001b[0m   File \"/home/kukjin/anaconda3/envs/minerl/lib/python3.7/site-packages/torch/serialization.py\", line 593, in load\n",
      "\u001b[2m\u001b[36m(pid=75716)\u001b[0m     return _legacy_load(opened_file, map_location, pickle_module, **pickle_load_args)\n",
      "\u001b[2m\u001b[36m(pid=75716)\u001b[0m   File \"/home/kukjin/anaconda3/envs/minerl/lib/python3.7/site-packages/torch/serialization.py\", line 772, in _legacy_load\n",
      "\u001b[2m\u001b[36m(pid=75716)\u001b[0m     result = unpickler.load()\n",
      "\u001b[2m\u001b[36m(pid=75716)\u001b[0m   File \"/home/kukjin/anaconda3/envs/minerl/lib/python3.7/site-packages/torch/serialization.py\", line 728, in persistent_load\n",
      "\u001b[2m\u001b[36m(pid=75716)\u001b[0m     deserialized_objects[root_key] = restore_location(obj, location)\n",
      "\u001b[2m\u001b[36m(pid=75716)\u001b[0m   File \"/home/kukjin/anaconda3/envs/minerl/lib/python3.7/site-packages/torch/serialization.py\", line 175, in default_restore_location\n",
      "\u001b[2m\u001b[36m(pid=75716)\u001b[0m     result = fn(storage, location)\n",
      "\u001b[2m\u001b[36m(pid=75716)\u001b[0m   File \"/home/kukjin/anaconda3/envs/minerl/lib/python3.7/site-packages/torch/serialization.py\", line 151, in _cuda_deserialize\n",
      "\u001b[2m\u001b[36m(pid=75716)\u001b[0m     device = validate_cuda_device(location)\n",
      "\u001b[2m\u001b[36m(pid=75716)\u001b[0m   File \"/home/kukjin/anaconda3/envs/minerl/lib/python3.7/site-packages/torch/serialization.py\", line 135, in validate_cuda_device\n",
      "\u001b[2m\u001b[36m(pid=75716)\u001b[0m     raise RuntimeError('Attempting to deserialize object on a CUDA '\n",
      "\u001b[2m\u001b[36m(pid=75716)\u001b[0m RuntimeError: Attempting to deserialize object on a CUDA device but torch.cuda.is_available() is False. If you are running on a CPU-only machine, please use torch.load with map_location=torch.device('cpu') to map your storages to the CPU.\n",
      "\u001b[2m\u001b[36m(pid=75716)\u001b[0m 2021-05-20 04:34:39,341\tERROR worker.py:382 -- Exception raised in creation task: The actor died because of an error raised in its creation task, \u001b[36mray::Actor.__init__()\u001b[39m (pid=75716, ip=192.168.0.22)\n",
      "\u001b[2m\u001b[36m(pid=75716)\u001b[0m   File \"python/ray/_raylet.pyx\", line 458, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(pid=75716)\u001b[0m   File \"python/ray/_raylet.pyx\", line 479, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(pid=75716)\u001b[0m   File \"python/ray/_raylet.pyx\", line 349, in ray._raylet.raise_if_dependency_failed\n",
      "\u001b[2m\u001b[36m(pid=75716)\u001b[0m ray.exceptions.RaySystemError: System error: Attempting to deserialize object on a CUDA device but torch.cuda.is_available() is False. If you are running on a CPU-only machine, please use torch.load with map_location=torch.device('cpu') to map your storages to the CPU.\n",
      "\u001b[2m\u001b[36m(pid=75716)\u001b[0m traceback: Traceback (most recent call last):\n",
      "\u001b[2m\u001b[36m(pid=75716)\u001b[0m   File \"/home/kukjin/anaconda3/envs/minerl/lib/python3.7/site-packages/ray/serialization.py\", line 246, in deserialize_objects\n",
      "\u001b[2m\u001b[36m(pid=75716)\u001b[0m     obj = self._deserialize_object(data, metadata, object_ref)\n",
      "\u001b[2m\u001b[36m(pid=75716)\u001b[0m   File \"/home/kukjin/anaconda3/envs/minerl/lib/python3.7/site-packages/ray/serialization.py\", line 188, in _deserialize_object\n",
      "\u001b[2m\u001b[36m(pid=75716)\u001b[0m     return self._deserialize_msgpack_data(data, metadata_fields)\n",
      "\u001b[2m\u001b[36m(pid=75716)\u001b[0m   File \"/home/kukjin/anaconda3/envs/minerl/lib/python3.7/site-packages/ray/serialization.py\", line 166, in _deserialize_msgpack_data\n",
      "\u001b[2m\u001b[36m(pid=75716)\u001b[0m     python_objects = self._deserialize_pickle5_data(pickle5_data)\n",
      "\u001b[2m\u001b[36m(pid=75716)\u001b[0m   File \"/home/kukjin/anaconda3/envs/minerl/lib/python3.7/site-packages/ray/serialization.py\", line 156, in _deserialize_pickle5_data\n",
      "\u001b[2m\u001b[36m(pid=75716)\u001b[0m     obj = pickle.loads(in_band)\n",
      "\u001b[2m\u001b[36m(pid=75716)\u001b[0m   File \"/home/kukjin/anaconda3/envs/minerl/lib/python3.7/site-packages/torch/storage.py\", line 161, in _load_from_bytes\n",
      "\u001b[2m\u001b[36m(pid=75716)\u001b[0m     return torch.load(io.BytesIO(b))\n",
      "\u001b[2m\u001b[36m(pid=75716)\u001b[0m   File \"/home/kukjin/anaconda3/envs/minerl/lib/python3.7/site-packages/torch/serialization.py\", line 593, in load\n",
      "\u001b[2m\u001b[36m(pid=75716)\u001b[0m     return _legacy_load(opened_file, map_location, pickle_module, **pickle_load_args)\n",
      "\u001b[2m\u001b[36m(pid=75716)\u001b[0m   File \"/home/kukjin/anaconda3/envs/minerl/lib/python3.7/site-packages/torch/serialization.py\", line 772, in _legacy_load\n",
      "\u001b[2m\u001b[36m(pid=75716)\u001b[0m     result = unpickler.load()\n",
      "\u001b[2m\u001b[36m(pid=75716)\u001b[0m   File \"/home/kukjin/anaconda3/envs/minerl/lib/python3.7/site-packages/torch/serialization.py\", line 728, in persistent_load\n",
      "\u001b[2m\u001b[36m(pid=75716)\u001b[0m     deserialized_objects[root_key] = restore_location(obj, location)\n",
      "\u001b[2m\u001b[36m(pid=75716)\u001b[0m   File \"/home/kukjin/anaconda3/envs/minerl/lib/python3.7/site-packages/torch/serialization.py\", line 175, in default_restore_location\n",
      "\u001b[2m\u001b[36m(pid=75716)\u001b[0m     result = fn(storage, location)\n",
      "\u001b[2m\u001b[36m(pid=75716)\u001b[0m   File \"/home/kukjin/anaconda3/envs/minerl/lib/python3.7/site-packages/torch/serialization.py\", line 151, in _cuda_deserialize\n",
      "\u001b[2m\u001b[36m(pid=75716)\u001b[0m     device = validate_cuda_device(location)\n",
      "\u001b[2m\u001b[36m(pid=75716)\u001b[0m   File \"/home/kukjin/anaconda3/envs/minerl/lib/python3.7/site-packages/torch/serialization.py\", line 135, in validate_cuda_device\n",
      "\u001b[2m\u001b[36m(pid=75716)\u001b[0m     raise RuntimeError('Attempting to deserialize object on a CUDA '\n",
      "\u001b[2m\u001b[36m(pid=75716)\u001b[0m RuntimeError: Attempting to deserialize object on a CUDA device but torch.cuda.is_available() is False. If you are running on a CPU-only machine, please use torch.load with map_location=torch.device('cpu') to map your storages to the CPU.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-05-20 04:34:44,324\tERROR worker.py:1056 -- Possible unhandled error from worker: \u001b[36mray::Actor.__init__()\u001b[39m (pid=75711, ip=192.168.0.22)\n",
      "  File \"python/ray/_raylet.pyx\", line 458, in ray._raylet.execute_task\n",
      "  File \"python/ray/_raylet.pyx\", line 479, in ray._raylet.execute_task\n",
      "  File \"python/ray/_raylet.pyx\", line 349, in ray._raylet.raise_if_dependency_failed\n",
      "ray.exceptions.RaySystemError: System error: Attempting to deserialize object on a CUDA device but torch.cuda.is_available() is False. If you are running on a CPU-only machine, please use torch.load with map_location=torch.device('cpu') to map your storages to the CPU.\n",
      "traceback: Traceback (most recent call last):\n",
      "  File \"/home/kukjin/anaconda3/envs/minerl/lib/python3.7/site-packages/ray/serialization.py\", line 246, in deserialize_objects\n",
      "    obj = self._deserialize_object(data, metadata, object_ref)\n",
      "  File \"/home/kukjin/anaconda3/envs/minerl/lib/python3.7/site-packages/ray/serialization.py\", line 188, in _deserialize_object\n",
      "    return self._deserialize_msgpack_data(data, metadata_fields)\n",
      "  File \"/home/kukjin/anaconda3/envs/minerl/lib/python3.7/site-packages/ray/serialization.py\", line 166, in _deserialize_msgpack_data\n",
      "    python_objects = self._deserialize_pickle5_data(pickle5_data)\n",
      "  File \"/home/kukjin/anaconda3/envs/minerl/lib/python3.7/site-packages/ray/serialization.py\", line 156, in _deserialize_pickle5_data\n",
      "    obj = pickle.loads(in_band)\n",
      "  File \"/home/kukjin/anaconda3/envs/minerl/lib/python3.7/site-packages/torch/storage.py\", line 161, in _load_from_bytes\n",
      "    return torch.load(io.BytesIO(b))\n",
      "  File \"/home/kukjin/anaconda3/envs/minerl/lib/python3.7/site-packages/torch/serialization.py\", line 593, in load\n",
      "    return _legacy_load(opened_file, map_location, pickle_module, **pickle_load_args)\n",
      "  File \"/home/kukjin/anaconda3/envs/minerl/lib/python3.7/site-packages/torch/serialization.py\", line 772, in _legacy_load\n",
      "    result = unpickler.load()\n",
      "  File \"/home/kukjin/anaconda3/envs/minerl/lib/python3.7/site-packages/torch/serialization.py\", line 728, in persistent_load\n",
      "    deserialized_objects[root_key] = restore_location(obj, location)\n",
      "  File \"/home/kukjin/anaconda3/envs/minerl/lib/python3.7/site-packages/torch/serialization.py\", line 175, in default_restore_location\n",
      "    result = fn(storage, location)\n",
      "  File \"/home/kukjin/anaconda3/envs/minerl/lib/python3.7/site-packages/torch/serialization.py\", line 151, in _cuda_deserialize\n",
      "    device = validate_cuda_device(location)\n",
      "  File \"/home/kukjin/anaconda3/envs/minerl/lib/python3.7/site-packages/torch/serialization.py\", line 135, in validate_cuda_device\n",
      "    raise RuntimeError('Attempting to deserialize object on a CUDA '\n",
      "RuntimeError: Attempting to deserialize object on a CUDA device but torch.cuda.is_available() is False. If you are running on a CPU-only machine, please use torch.load with map_location=torch.device('cpu') to map your storages to the CPU.\n",
      "2021-05-20 04:34:44,363\tERROR worker.py:1056 -- Possible unhandled error from worker: \u001b[36mray::Actor.__init__()\u001b[39m (pid=75716, ip=192.168.0.22)\n",
      "  File \"python/ray/_raylet.pyx\", line 458, in ray._raylet.execute_task\n",
      "  File \"python/ray/_raylet.pyx\", line 479, in ray._raylet.execute_task\n",
      "  File \"python/ray/_raylet.pyx\", line 349, in ray._raylet.raise_if_dependency_failed\n",
      "ray.exceptions.RaySystemError: System error: Attempting to deserialize object on a CUDA device but torch.cuda.is_available() is False. If you are running on a CPU-only machine, please use torch.load with map_location=torch.device('cpu') to map your storages to the CPU.\n",
      "traceback: Traceback (most recent call last):\n",
      "  File \"/home/kukjin/anaconda3/envs/minerl/lib/python3.7/site-packages/ray/serialization.py\", line 246, in deserialize_objects\n",
      "    obj = self._deserialize_object(data, metadata, object_ref)\n",
      "  File \"/home/kukjin/anaconda3/envs/minerl/lib/python3.7/site-packages/ray/serialization.py\", line 188, in _deserialize_object\n",
      "    return self._deserialize_msgpack_data(data, metadata_fields)\n",
      "  File \"/home/kukjin/anaconda3/envs/minerl/lib/python3.7/site-packages/ray/serialization.py\", line 166, in _deserialize_msgpack_data\n",
      "    python_objects = self._deserialize_pickle5_data(pickle5_data)\n",
      "  File \"/home/kukjin/anaconda3/envs/minerl/lib/python3.7/site-packages/ray/serialization.py\", line 156, in _deserialize_pickle5_data\n",
      "    obj = pickle.loads(in_band)\n",
      "  File \"/home/kukjin/anaconda3/envs/minerl/lib/python3.7/site-packages/torch/storage.py\", line 161, in _load_from_bytes\n",
      "    return torch.load(io.BytesIO(b))\n",
      "  File \"/home/kukjin/anaconda3/envs/minerl/lib/python3.7/site-packages/torch/serialization.py\", line 593, in load\n",
      "    return _legacy_load(opened_file, map_location, pickle_module, **pickle_load_args)\n",
      "  File \"/home/kukjin/anaconda3/envs/minerl/lib/python3.7/site-packages/torch/serialization.py\", line 772, in _legacy_load\n",
      "    result = unpickler.load()\n",
      "  File \"/home/kukjin/anaconda3/envs/minerl/lib/python3.7/site-packages/torch/serialization.py\", line 728, in persistent_load\n",
      "    deserialized_objects[root_key] = restore_location(obj, location)\n",
      "  File \"/home/kukjin/anaconda3/envs/minerl/lib/python3.7/site-packages/torch/serialization.py\", line 175, in default_restore_location\n",
      "    result = fn(storage, location)\n",
      "  File \"/home/kukjin/anaconda3/envs/minerl/lib/python3.7/site-packages/torch/serialization.py\", line 151, in _cuda_deserialize\n",
      "    device = validate_cuda_device(location)\n",
      "  File \"/home/kukjin/anaconda3/envs/minerl/lib/python3.7/site-packages/torch/serialization.py\", line 135, in validate_cuda_device\n",
      "    raise RuntimeError('Attempting to deserialize object on a CUDA '\n",
      "RuntimeError: Attempting to deserialize object on a CUDA device but torch.cuda.is_available() is False. If you are running on a CPU-only machine, please use torch.load with map_location=torch.device('cpu') to map your storages to the CPU.\n"
     ]
    }
   ],
   "source": [
    "# Generating each own instances\n",
    "# main()\n",
    "num_actors = 2\n",
    "epsilon = 0.5\n",
    "\n",
    "\n",
    "\n",
    "# learner network initialzation\n",
    "learner = Learner(policy_net, 256)\n",
    "\n",
    "ray.put(memory)\n",
    "ray.put(demos)\n",
    "ray.put(learner)\n",
    "\n",
    "# actor network, environments initialization\n",
    "actor_list = [Actor.remote(memory, learner.get_network(), i, 0.5) for i in range(num_actors)]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explore = [actor.explore.remote() for actor in actor_list]\n",
    "ray.get(explore)\n",
    "for i in range(1000):\n",
    "    if(memory.size() > 1000):\n",
    "        leaner.update_network(memory, demos, batch_size, demo_prob, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env0 = gym.make(\"MineRLTreechop-v0\")\n",
    "env1 = gym.make(\"MineRLTreechop-v0\")\n",
    "env2 = gym.make(\"MineRLTreechop-v0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unittest.mock import Mock, MagicMock, call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import ray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ray.shutdown()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ray.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@ray.remote\n",
    "def make_env(index):\n",
    "    env = Mock()\n",
    "    env.return_value = \"Environment %d is created\" %index\n",
    "    port_number = int(\"12340\")+index\n",
    "    return env()\n",
    "num_envs = 4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_list = [make_env.remote(i) for i in range(num_envs)]\n",
    "ray.get(env_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import unittest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    class EnvTests(unittest.TestCase):\n",
    "        def test(self):\n",
    "            make_env()\n",
    "\n",
    "if __name__=='__main__':\n",
    "    unittest.main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    class EnvTests2(unittest.TestCase):\n",
    "        def test(self):\n",
    "            num_envs = 4\n",
    "            env_list = [make_env.remote(i) for i in range(num_envs)]\n",
    "            print(ray.get(env_list))\n",
    "\n",
    "if __name__=='__main__':\n",
    "    unittest.main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
