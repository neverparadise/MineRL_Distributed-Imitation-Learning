{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "annual-printing",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kukjin/anaconda3/envs/minerl/lib/python3.7/site-packages/ray/autoscaler/_private/cli_logger.py:61: FutureWarning: Not all Ray CLI dependencies were found. In Ray 1.4+, the Ray CLI, autoscaler, and dashboard will only be usable via `pip install 'ray[default]'`. Please update your install command.\n",
      "  \"update your install command.\", FutureWarning)\n",
      "/home/kukjin/anaconda3/envs/minerl/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    }
   ],
   "source": [
    "from model import DQN\n",
    "import os\n",
    "import minerl\n",
    "import gym\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import ray\n",
    "import pandas as pd\n",
    "import gc\n",
    "import asyncio\n",
    "from _collections import deque\n",
    "from utils import *\n",
    "import random\n",
    "import copy\n",
    "\n",
    "def learner_append_sample(memory, model, target_model, state, action, reward, next_state, done):\n",
    "    # Caluclating Priority (TD Error)\n",
    "    target = model(state.float()).data.cpu()\n",
    "    old_val = target[0][action].cpu()\n",
    "    target_val = target_model(next_state.float()).data.cpu()\n",
    "    if done:\n",
    "        target[0][action] = reward\n",
    "    else:\n",
    "        target[0][action] = reward + 0.99 * torch.max(target_val)\n",
    "\n",
    "    error = abs(old_val - target[0][action])\n",
    "    error = error.cpu()\n",
    "    memory.add.remote(error, [state, action, reward, next_state, done])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "express-reservation",
   "metadata": {},
   "outputs": [],
   "source": [
    "@ray.remote\n",
    "class Actor:\n",
    "    def __init__(self, learner, actor_idx, startEpsilon, endEpsilon, paramServer, action_size):\n",
    "        # environment initialization\n",
    "        self.actor_idx = actor_idx\n",
    "        self.env = gym.make(\"MineRLTreechop-v0\")\n",
    "        self.port_number = int(\"12340\") + actor_idx\n",
    "        print(\"actor environment %d initialize successfully\" % self.actor_idx)\n",
    "        self.env.make_interactive(port=self.port_number, realtime=False)\n",
    "        self.shared_network_cpu = ray.get(learner.get_network.remote())\n",
    "        # self.shared_memory = ray.get(shared_memory_id)\n",
    "        # print(\"shared memory assign successfully\")\n",
    "        \n",
    "        # network initalization\n",
    "        self.actor_network = DQN(action_size).cpu()\n",
    "        self.actor_target_network = DQN(action_size).cpu()\n",
    "        self.actor_network.load_state_dict(self.shared_network_cpu.state_dict())\n",
    "        self.actor_target_network.load_state_dict(self.actor_network.state_dict())\n",
    "        print(\"actor network %d initialize successfully\" % self.actor_idx)\n",
    "\n",
    "        self.initialized = False\n",
    "        self.epi_counter = 0\n",
    "        # exploring info\n",
    "        self.startEpsilon = startEpsilon\n",
    "        self.endEpsilon = endEpsilon\n",
    "        self.max_episodes = 100\n",
    "\n",
    "        self.paramServer = paramServer\n",
    "        \n",
    "    \n",
    "    # 1. 네트워크 파라미터 복사\n",
    "    # 2. 환경 탐험 (초기화, 행동)\n",
    "    # 3. 로컬버퍼에 저장\n",
    "    # 4. priority 계산\n",
    "    # 5. 글로벌 버퍼에 저장\n",
    "    # 6. 주기적으로 네트워크 업데이트\n",
    "\n",
    "    def get_initialized(self):\n",
    "        return self.initialized\n",
    "\n",
    "    def get_counter(self):\n",
    "        return self.epi_counter\n",
    "\n",
    "    # 각 환경 인스턴스에서 각 엡실론에 따라 탐험을 진행한다.\n",
    "    # 탐험 과정에서 local buffer에 transition들을 저장한다.\n",
    "    # local buffer의 개수가 특정 개수 이상이면 global buffer에 추가해준다.\n",
    "\n",
    "    def explore(self, learner, shared_memory):\n",
    "        \n",
    "        self.initialized = True\n",
    "        stepDrop = (self.startEpsilon - self.endEpsilon) / self.max_episodes\n",
    "        epsilon = self.startEpsilon\n",
    "        total_steps = 0\n",
    "        \n",
    "        episodes = [x for x in range(self.max_episodes)]\n",
    "        train_stats = pd.DataFrame(index=episodes, columns=['rewards'])\n",
    "        \n",
    "        for num_epi in range(self.max_episodes):\n",
    "            obs = self.env.reset()\n",
    "            state = converter(obs).cpu().float()\n",
    "            done = False\n",
    "            total_reward = 0\n",
    "            steps = 0\n",
    "            if (epsilon > self.endEpsilon):\n",
    "                epsilon -= stepDrop\n",
    "                \n",
    "            while not done:\n",
    "                steps += 1\n",
    "                total_steps += 1\n",
    "                a_out = self.actor_network.sample_action(state, epsilon)\n",
    "                action_index = a_out\n",
    "                action = make_action2(self.env, action_index)\n",
    "                obs_prime, reward, done, info = self.env.step(action)\n",
    "                total_reward += reward\n",
    "                state_prime = converter(obs_prime)\n",
    "\n",
    "                self.actor_append_sample(shared_memory, self.actor_network, self.actor_target_network, \\\n",
    "                                       state, action_index, reward, state_prime, done)\n",
    "\n",
    "                state = state_prime.float().cpu()\n",
    "                if done:\n",
    "                    break\n",
    "                    \n",
    "            # pandas로 리워드 기록하기\n",
    "            print(\"%d episode is done\" % num_epi)\n",
    "            print(\"total rewards : %d \" % total_reward)\n",
    "            train_stats.loc[num_epi]['rewards'] = total_reward\n",
    "            train_stats.to_csv('train_stat_minerl_agent {}.csv'.format(str(self.actor_idx)))\n",
    "            \n",
    "  \n",
    "            self.pull_parameters(learner) \n",
    "            print(\"actor network is updated \")\n",
    "            print(\"actor target_network is updated\")\n",
    "    \n",
    "    def pull_parameters(self, learner):\n",
    "        ray.get(self.paramServer.pull_parameters.remote(learner)) \n",
    "        policy_params, target_params = ray.get(self.paramServer.return_parameters.remote())\n",
    "        self.actor_network.load_state_dict(policy_params)\n",
    "        self.actor_target_network.load_state_dict(target_params)\n",
    "        \n",
    "    def env_close(self):\n",
    "        self.env.close()        \n",
    "\n",
    "    def actor_append_sample(self, memory, model, target_model, state, action, reward, next_state, done):\n",
    "        # Caluclating Priority (TD Error)\n",
    "        target = model(state.float()).data.cpu()\n",
    "        old_val = target[0][action].cpu()\n",
    "        target_val = target_model(next_state.float()).data.cpu()\n",
    "        if done:\n",
    "            target[0][action] = reward\n",
    "        else:\n",
    "            target[0][action] = reward + 0.99 * torch.max(target_val)\n",
    "\n",
    "        error = abs(old_val - target[0][action])\n",
    "        error = error.cpu()\n",
    "        memory.add.remote(error, [state, action, reward, next_state, done])\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "clean-university",
   "metadata": {},
   "outputs": [],
   "source": [
    "@ray.remote\n",
    "class ParameterServer:\n",
    "    def __init__(self, action_size):\n",
    "        self.policy_params = DQN(action_size).state_dict()\n",
    "        self.target_params = DQN(action_size).state_dict()\n",
    "    \n",
    "    def pull_parameters(self, learner):\n",
    "        learner.push_parameters.remote(self.policy_params, self.target_params)\n",
    "        return 1\n",
    "    \n",
    "    def return_parameters(self):\n",
    "        return self.policy_params, self.target_params\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "correct-pride",
   "metadata": {},
   "outputs": [],
   "source": [
    "@ray.remote(num_gpus=1)\n",
    "class Learner:\n",
    "    def __init__(self, network, batch_size, paramServer, action_size):\n",
    "        self.learner_network = DQN(action_size).cuda().float()\n",
    "        self.learner_target_network = DQN(action_size).cuda().float()\n",
    "        self.learner_network.load_state_dict(network.state_dict())\n",
    "        self.learner_target_network.load_state_dict(network.state_dict())\n",
    "        self.shared_network = DQN(action_size).cpu()\n",
    "        self.shared_target_network = DQN(action_size).cpu()\n",
    "        \n",
    "        self.paramServer = paramServer\n",
    "        \n",
    "        self.count = 0\n",
    "        self.batch_size = batch_size\n",
    "        self.max_counts= 1000000\n",
    "\n",
    "    # 1. sampling\n",
    "    # 2. calculate gradient\n",
    "    # 3. weight update\n",
    "    # 4. compute priorities\n",
    "    # 5. priorities of buffer update\n",
    "    # 6. remove old memory\n",
    "    \n",
    "    def push_parameters(self, server_policy_params, server_target_params):\n",
    "        self.shared_network.load_state_dict(self.learner_network.state_dict())\n",
    "        self.shared_target_network.load_state_dict(self.learner_target_network.state_dict())\n",
    "        policy_net_params = self.shared_network.state_dict()\n",
    "        target_net_params = self.shared_target_network.state_dict()\n",
    "        server_policy_params = policy_net_params\n",
    "        server_target_params = target_net_params\n",
    "        \n",
    "    def count(self):\n",
    "        return self.count\n",
    "    \n",
    "    def get_network(self):\n",
    "        self.shared_network.load_state_dict(self.learner_network.state_dict())\n",
    "        print(\"return learner network\")\n",
    "        return self.shared_network\n",
    "    \n",
    "    def get_target_network(self):\n",
    "        self.shared_target_network.load_state_dict(self.learner_target_network.state_dict())\n",
    "        return self.shared_target_network\n",
    "\n",
    "    def update_network(self, memory, batch_size, optimizer):\n",
    "        print(\"started\")\n",
    "\n",
    "        \n",
    "        counts = [x for x in range(self.max_counts)]\n",
    "        train_stats = pd.DataFrame(index=counts, columns=['loss'])\n",
    "        while(self.count < 10000000):\n",
    "            agent_batch, agent_idxs, agent_weights = ray.get(memory.sample.remote(batch_size))\n",
    "            state_list = []\n",
    "            action_list = []\n",
    "            reward_list = []\n",
    "            next_state_list = []\n",
    "            done_mask_list = []\n",
    "\n",
    "            #print(\"agent batch len : {} \".format(str(len(agent_batch))))\n",
    "            for agent_transition in agent_batch:\n",
    "                s, a, r, s_prime, done_mask = agent_transition\n",
    "                state_list.append(s)\n",
    "                action_list.append([a])\n",
    "                reward_list.append([r])\n",
    "                next_state_list.append(s_prime)\n",
    "                done_mask_list.append([done_mask])\n",
    "\n",
    "            s = torch.stack(state_list).float().cuda()\n",
    "            a = torch.tensor(action_list, dtype=torch.int64).cuda()\n",
    "            r = torch.tensor(reward_list).cuda()\n",
    "            s_prime = torch.stack(next_state_list).float().cuda()\n",
    "            done_mask = torch.tensor(done_mask_list).float().cuda()\n",
    "\n",
    "            q_vals = self.learner_network(s)\n",
    "            state_action_values = q_vals.gather(1, a)\n",
    "\n",
    "            # comparing the q values to the values expected using the next states and reward\n",
    "            next_state_values = self.learner_target_network(s_prime).max(1)[0].unsqueeze(1)\n",
    "            target = r + (next_state_values * gamma * done_mask)\n",
    "\n",
    "            # calculating the q loss, n-step return lossm supervised_loss\n",
    "            is_weights = torch.FloatTensor(agent_weights).to(device)\n",
    "            q_loss = (is_weights * F.mse_loss(state_action_values, target)).mean()\n",
    "            #supervised_loss = margin_loss(q_vals, a, 1, 1)\n",
    "\n",
    "            loss = q_loss #+ supervised_loss\n",
    "            errors = torch.abs(state_action_values - target).data.cpu().detach()\n",
    "            errors = errors.numpy()\n",
    "            # update priority\n",
    "            for i in range(batch_size):\n",
    "                idx = agent_idxs[i]\n",
    "                memory.update.remote(idx, errors[i])\n",
    "\n",
    "            train_stats.loc[self.count ]['loss'] = float(loss.item())\n",
    "            train_stats.to_csv('train_stat_minerl_learner.csv')\n",
    "\n",
    "            # optimization step and logging\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            self.count +=1\n",
    "            if(self.count % 50 == 0 and self.count != 0):\n",
    "                self.learner_target_network.load_state_dict(self.learner_network.state_dict())\n",
    "                print(\"Count : {} leaner_target_network updated\".format(self.count))\n",
    "                \n",
    "            if(self.count % 10 == 0 and self.count!= 0):\n",
    "                print(\"Count : {} leaner_network updated\".format(self.count))\n",
    "                torch.save(self.learner_network.state_dict(),\"apex_dqfd_learner2.pth\")\n",
    "                print(\"learner model saved\")\n",
    "                \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dirty-summary",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-06-09 20:17:49,788\tINFO services.py:1269 -- View the Ray dashboard at \u001b[1m\u001b[32mhttp://127.0.0.1:8265\u001b[39m\u001b[22m\n"
     ]
    }
   ],
   "source": [
    "ray.init()\n",
    "\n",
    "#하이퍼 파라미터\n",
    "learning_rate = 0.0001\n",
    "gamma = 0.99\n",
    "buffer_limit = 50000\n",
    "L1 = 0.9\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "root_path = os.curdir\n",
    "model_path = root_path + '/dqn_model/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "certain-savage",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=89337)\u001b[0m Memory is initialized\n"
     ]
    }
   ],
   "source": [
    "policy_net = DQN(19).cuda()\n",
    "target_net = DQN(19).cuda()\n",
    "target_net.load_state_dict(policy_net.state_dict())\n",
    "memory = Memory.remote(40000)\n",
    "optimizer = optim.Adam(policy_net.parameters(), lr=learning_rate, weight_decay=1e-5)\n",
    "\n",
    "# Copy network params from pretrained Agent\n",
    "model_path = './dqn_model/apex_dqfd_learner.pth'\n",
    "policy_net.load_state_dict(torch.load(model_path, map_location='cuda:0'))\n",
    "target_net.load_state_dict(policy_net.state_dict())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "organizational-relative",
   "metadata": {},
   "outputs": [],
   "source": [
    "params_server = ParameterServer.remote()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "macro-criticism",
   "metadata": {},
   "outputs": [],
   "source": [
    "# learner network initialzation\n",
    "batch_size = 512\n",
    "learner = Learner.remote(policy_net, batch_size, params_server)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fundamental-heart",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=89343)\u001b[0m /home/kukjin/anaconda3/envs/minerl/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=89343)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n",
      "\u001b[2m\u001b[36m(pid=89347)\u001b[0m /home/kukjin/anaconda3/envs/minerl/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=89347)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n",
      "\u001b[2m\u001b[36m(pid=89340)\u001b[0m /home/kukjin/anaconda3/envs/minerl/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=89340)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=89340)\u001b[0m actor environment 2 initialize successfully\n",
      "\u001b[2m\u001b[36m(pid=89340)\u001b[0m actor network 2 initialize successfully\n",
      "\u001b[2m\u001b[36m(pid=89346)\u001b[0m return learner network\n",
      "\u001b[2m\u001b[36m(pid=89346)\u001b[0m return learner network\n",
      "\u001b[2m\u001b[36m(pid=89347)\u001b[0m actor environment 0 initialize successfully\n",
      "\u001b[2m\u001b[36m(pid=89347)\u001b[0m actor network 0 initialize successfully\n",
      "\u001b[2m\u001b[36m(pid=89346)\u001b[0m return learner network\n",
      "\u001b[2m\u001b[36m(pid=89343)\u001b[0m actor environment 1 initialize successfully\n",
      "\u001b[2m\u001b[36m(pid=89343)\u001b[0m actor network 1 initialize successfully\n"
     ]
    }
   ],
   "source": [
    "# actor network, environments initialization\n",
    "# Generating each own instances\n",
    "\n",
    "actor1 = Actor.remote(learner, 0, 0.95, 0.01, params_server)\n",
    "actor2 = Actor.remote(learner, 1, 0.5, 0.01, params_server)\n",
    "actor3 = Actor.remote(learner, 2, 0.1, 0.01, params_server)\n",
    "#actor_list = [actor1, actor2]\n",
    "actor_list = [actor1, actor2, actor3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "executed-convergence",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=89347)\u001b[0m MineRL agent is public, connect on port 12340 with Minecraft 1.11\n",
      "\u001b[2m\u001b[36m(pid=89343)\u001b[0m MineRL agent is public, connect on port 12341 with Minecraft 1.11\n",
      "\u001b[2m\u001b[36m(pid=89340)\u001b[0m MineRL agent is public, connect on port 12342 with Minecraft 1.11\n"
     ]
    }
   ],
   "source": [
    "explore = [actor.explore.remote(learner, memory) for actor in actor_list]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "prescription-matter",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=89346)\u001b[0m started\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=89346)\u001b[0m /home/kukjin/anaconda3/envs/minerl/lib/python3.7/site-packages/ray/workers/default_worker.py:81: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /pytorch/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=89346)\u001b[0m   action=\"store_true\",\n"
     ]
    }
   ],
   "source": [
    "update = learner.update_network.remote(memory,batch_size, optimizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "induced-transfer",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7180"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=89346)\u001b[0m Count : 10 leaner_network updated\n",
      "\u001b[2m\u001b[36m(pid=89346)\u001b[0m learner model saved\n",
      "\u001b[2m\u001b[36m(pid=89346)\u001b[0m Count : 20 leaner_network updated\n",
      "\u001b[2m\u001b[36m(pid=89346)\u001b[0m learner model saved\n",
      "\u001b[2m\u001b[36m(pid=89346)\u001b[0m Count : 30 leaner_network updated\n",
      "\u001b[2m\u001b[36m(pid=89346)\u001b[0m learner model saved\n",
      "\u001b[2m\u001b[36m(pid=89347)\u001b[0m 0 episode is done\n",
      "\u001b[2m\u001b[36m(pid=89347)\u001b[0m total rewards : 0 \n",
      "\u001b[2m\u001b[36m(pid=89347)\u001b[0m actor network is updated \n",
      "\u001b[2m\u001b[36m(pid=89347)\u001b[0m actor target_network is updated\n",
      "\u001b[2m\u001b[36m(pid=89346)\u001b[0m Count : 40 leaner_network updated\n",
      "\u001b[2m\u001b[36m(pid=89346)\u001b[0m learner model saved\n",
      "\u001b[2m\u001b[36m(pid=89343)\u001b[0m 0 episode is done\n",
      "\u001b[2m\u001b[36m(pid=89343)\u001b[0m total rewards : 1 \n",
      "\u001b[2m\u001b[36m(pid=89343)\u001b[0m actor network is updated \n",
      "\u001b[2m\u001b[36m(pid=89343)\u001b[0m actor target_network is updated\n",
      "\u001b[2m\u001b[36m(pid=89340)\u001b[0m 0 episode is done\n",
      "\u001b[2m\u001b[36m(pid=89340)\u001b[0m total rewards : 0 \n",
      "\u001b[2m\u001b[36m(pid=89340)\u001b[0m actor network is updated \n",
      "\u001b[2m\u001b[36m(pid=89340)\u001b[0m actor target_network is updated\n",
      "\u001b[2m\u001b[36m(pid=89346)\u001b[0m Count : 50 leaner_target_network updated\n",
      "\u001b[2m\u001b[36m(pid=89346)\u001b[0m Count : 50 leaner_network updated\n",
      "\u001b[2m\u001b[36m(pid=89346)\u001b[0m learner model saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=89347)\u001b[0m MineRL agent is public, connect on port 12340 with Minecraft 1.11\n",
      "\u001b[2m\u001b[36m(pid=89343)\u001b[0m MineRL agent is public, connect on port 12341 with Minecraft 1.11\n",
      "\u001b[2m\u001b[36m(pid=89340)\u001b[0m MineRL agent is public, connect on port 12342 with Minecraft 1.11\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=89346)\u001b[0m Count : 60 leaner_network updated\n",
      "\u001b[2m\u001b[36m(pid=89346)\u001b[0m learner model saved\n",
      "\u001b[2m\u001b[36m(pid=89346)\u001b[0m Count : 70 leaner_network updated\n",
      "\u001b[2m\u001b[36m(pid=89346)\u001b[0m learner model saved\n",
      "\u001b[2m\u001b[36m(pid=89346)\u001b[0m Count : 80 leaner_network updated\n",
      "\u001b[2m\u001b[36m(pid=89346)\u001b[0m learner model saved\n",
      "\u001b[2m\u001b[36m(pid=89346)\u001b[0m Count : 90 leaner_network updated\n",
      "\u001b[2m\u001b[36m(pid=89346)\u001b[0m learner model saved\n",
      "\u001b[2m\u001b[36m(pid=89346)\u001b[0m Count : 100 leaner_target_network updated\n",
      "\u001b[2m\u001b[36m(pid=89346)\u001b[0m Count : 100 leaner_network updated\n",
      "\u001b[2m\u001b[36m(pid=89346)\u001b[0m learner model saved\n",
      "\u001b[2m\u001b[36m(pid=89343)\u001b[0m 1 episode is done\n",
      "\u001b[2m\u001b[36m(pid=89343)\u001b[0m total rewards : 0 \n",
      "\u001b[2m\u001b[36m(pid=89343)\u001b[0m actor network is updated \n",
      "\u001b[2m\u001b[36m(pid=89343)\u001b[0m actor target_network is updated\n",
      "\u001b[2m\u001b[36m(pid=89347)\u001b[0m 1 episode is done\n",
      "\u001b[2m\u001b[36m(pid=89347)\u001b[0m total rewards : 0 \n",
      "\u001b[2m\u001b[36m(pid=89347)\u001b[0m actor network is updated \n",
      "\u001b[2m\u001b[36m(pid=89347)\u001b[0m actor target_network is updated\n",
      "\u001b[2m\u001b[36m(pid=89346)\u001b[0m Count : 110 leaner_network updated\n",
      "\u001b[2m\u001b[36m(pid=89346)\u001b[0m learner model saved\n",
      "\u001b[2m\u001b[36m(pid=89340)\u001b[0m 1 episode is done\n",
      "\u001b[2m\u001b[36m(pid=89340)\u001b[0m total rewards : 0 \n",
      "\u001b[2m\u001b[36m(pid=89340)\u001b[0m actor network is updated \n",
      "\u001b[2m\u001b[36m(pid=89340)\u001b[0m actor target_network is updated\n",
      "\u001b[2m\u001b[36m(pid=89346)\u001b[0m Count : 120 leaner_network updated\n",
      "\u001b[2m\u001b[36m(pid=89346)\u001b[0m learner model saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=89343)\u001b[0m MineRL agent is public, connect on port 12341 with Minecraft 1.11\n",
      "\u001b[2m\u001b[36m(pid=89347)\u001b[0m MineRL agent is public, connect on port 12340 with Minecraft 1.11\n",
      "\u001b[2m\u001b[36m(pid=89340)\u001b[0m MineRL agent is public, connect on port 12342 with Minecraft 1.11\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=89346)\u001b[0m Count : 130 leaner_network updated\n",
      "\u001b[2m\u001b[36m(pid=89346)\u001b[0m learner model saved\n",
      "\u001b[2m\u001b[36m(pid=89346)\u001b[0m Count : 140 leaner_network updated\n",
      "\u001b[2m\u001b[36m(pid=89346)\u001b[0m learner model saved\n",
      "\u001b[2m\u001b[36m(pid=89346)\u001b[0m Count : 150 leaner_target_network updated\n",
      "\u001b[2m\u001b[36m(pid=89346)\u001b[0m Count : 150 leaner_network updated\n",
      "\u001b[2m\u001b[36m(pid=89346)\u001b[0m learner model saved\n",
      "\u001b[2m\u001b[36m(pid=89346)\u001b[0m Count : 160 leaner_network updated\n",
      "\u001b[2m\u001b[36m(pid=89346)\u001b[0m learner model saved\n",
      "\u001b[2m\u001b[36m(pid=89346)\u001b[0m Count : 170 leaner_network updated\n",
      "\u001b[2m\u001b[36m(pid=89346)\u001b[0m learner model saved\n",
      "\u001b[2m\u001b[36m(pid=89343)\u001b[0m 2 episode is done\n",
      "\u001b[2m\u001b[36m(pid=89343)\u001b[0m total rewards : 0 \n",
      "\u001b[2m\u001b[36m(pid=89343)\u001b[0m actor network is updated \n",
      "\u001b[2m\u001b[36m(pid=89343)\u001b[0m actor target_network is updated\n",
      "\u001b[2m\u001b[36m(pid=89347)\u001b[0m 2 episode is done\n",
      "\u001b[2m\u001b[36m(pid=89347)\u001b[0m total rewards : 0 \n",
      "\u001b[2m\u001b[36m(pid=89347)\u001b[0m actor network is updated \n",
      "\u001b[2m\u001b[36m(pid=89347)\u001b[0m actor target_network is updated\n",
      "\u001b[2m\u001b[36m(pid=89346)\u001b[0m Count : 180 leaner_network updated\n",
      "\u001b[2m\u001b[36m(pid=89346)\u001b[0m learner model saved\n",
      "\u001b[2m\u001b[36m(pid=89340)\u001b[0m 2 episode is done\n",
      "\u001b[2m\u001b[36m(pid=89340)\u001b[0m total rewards : 0 \n",
      "\u001b[2m\u001b[36m(pid=89340)\u001b[0m actor network is updated \n",
      "\u001b[2m\u001b[36m(pid=89340)\u001b[0m actor target_network is updated\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=89343)\u001b[0m MineRL agent is public, connect on port 12341 with Minecraft 1.11\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=89346)\u001b[0m Count : 190 leaner_network updated\n",
      "\u001b[2m\u001b[36m(pid=89346)\u001b[0m learner model saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=89347)\u001b[0m MineRL agent is public, connect on port 12340 with Minecraft 1.11\n",
      "\u001b[2m\u001b[36m(pid=89340)\u001b[0m MineRL agent is public, connect on port 12342 with Minecraft 1.11\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=89346)\u001b[0m Count : 200 leaner_target_network updated\n",
      "\u001b[2m\u001b[36m(pid=89346)\u001b[0m Count : 200 leaner_network updated\n",
      "\u001b[2m\u001b[36m(pid=89346)\u001b[0m learner model saved\n",
      "\u001b[2m\u001b[36m(pid=89346)\u001b[0m Count : 210 leaner_network updated\n",
      "\u001b[2m\u001b[36m(pid=89346)\u001b[0m learner model saved\n",
      "\u001b[2m\u001b[36m(pid=89346)\u001b[0m Count : 220 leaner_network updated\n",
      "\u001b[2m\u001b[36m(pid=89346)\u001b[0m learner model saved\n",
      "\u001b[2m\u001b[36m(pid=89346)\u001b[0m Count : 230 leaner_network updated\n",
      "\u001b[2m\u001b[36m(pid=89346)\u001b[0m learner model saved\n",
      "\u001b[2m\u001b[36m(pid=89343)\u001b[0m 3 episode is done\n",
      "\u001b[2m\u001b[36m(pid=89343)\u001b[0m total rewards : 0 \n",
      "\u001b[2m\u001b[36m(pid=89346)\u001b[0m Count : 240 leaner_network updated\n",
      "\u001b[2m\u001b[36m(pid=89346)\u001b[0m learner model saved\n",
      "\u001b[2m\u001b[36m(pid=89343)\u001b[0m actor network is updated \n",
      "\u001b[2m\u001b[36m(pid=89343)\u001b[0m actor target_network is updated\n",
      "\u001b[2m\u001b[36m(pid=89347)\u001b[0m 3 episode is done\n",
      "\u001b[2m\u001b[36m(pid=89347)\u001b[0m total rewards : 0 \n",
      "\u001b[2m\u001b[36m(pid=89347)\u001b[0m actor network is updated \n",
      "\u001b[2m\u001b[36m(pid=89347)\u001b[0m actor target_network is updated\n",
      "\u001b[2m\u001b[36m(pid=89346)\u001b[0m Count : 250 leaner_target_network updated\n",
      "\u001b[2m\u001b[36m(pid=89346)\u001b[0m Count : 250 leaner_network updated\n",
      "\u001b[2m\u001b[36m(pid=89346)\u001b[0m learner model saved\n",
      "\u001b[2m\u001b[36m(pid=89340)\u001b[0m 3 episode is done\n",
      "\u001b[2m\u001b[36m(pid=89340)\u001b[0m total rewards : 0 \n",
      "\u001b[2m\u001b[36m(pid=89340)\u001b[0m actor network is updated \n",
      "\u001b[2m\u001b[36m(pid=89340)\u001b[0m actor target_network is updated\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=89343)\u001b[0m MineRL agent is public, connect on port 12341 with Minecraft 1.11\n",
      "\u001b[2m\u001b[36m(pid=89347)\u001b[0m MineRL agent is public, connect on port 12340 with Minecraft 1.11\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=89346)\u001b[0m Count : 260 leaner_network updated\n",
      "\u001b[2m\u001b[36m(pid=89346)\u001b[0m learner model saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=89340)\u001b[0m MineRL agent is public, connect on port 12342 with Minecraft 1.11\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=89346)\u001b[0m Count : 270 leaner_network updated\n",
      "\u001b[2m\u001b[36m(pid=89346)\u001b[0m learner model saved\n",
      "\u001b[2m\u001b[36m(pid=89346)\u001b[0m Count : 280 leaner_network updated\n",
      "\u001b[2m\u001b[36m(pid=89346)\u001b[0m learner model saved\n",
      "\u001b[2m\u001b[36m(pid=89346)\u001b[0m Count : 290 leaner_network updated\n",
      "\u001b[2m\u001b[36m(pid=89346)\u001b[0m learner model saved\n",
      "\u001b[2m\u001b[36m(pid=89346)\u001b[0m Count : 300 leaner_target_network updated\n",
      "\u001b[2m\u001b[36m(pid=89346)\u001b[0m Count : 300 leaner_network updated\n",
      "\u001b[2m\u001b[36m(pid=89346)\u001b[0m learner model saved\n",
      "\u001b[2m\u001b[36m(pid=89343)\u001b[0m 4 episode is done\n",
      "\u001b[2m\u001b[36m(pid=89343)\u001b[0m total rewards : 0 \n",
      "\u001b[2m\u001b[36m(pid=89343)\u001b[0m actor network is updated \n",
      "\u001b[2m\u001b[36m(pid=89343)\u001b[0m actor target_network is updated\n",
      "\u001b[2m\u001b[36m(pid=89346)\u001b[0m Count : 310 leaner_network updated\n",
      "\u001b[2m\u001b[36m(pid=89346)\u001b[0m learner model saved\n",
      "\u001b[2m\u001b[36m(pid=89347)\u001b[0m 4 episode is done\n",
      "\u001b[2m\u001b[36m(pid=89347)\u001b[0m total rewards : 0 \n",
      "\u001b[2m\u001b[36m(pid=89347)\u001b[0m actor network is updated \n",
      "\u001b[2m\u001b[36m(pid=89347)\u001b[0m actor target_network is updated\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=89343)\u001b[0m MineRL agent is public, connect on port 12341 with Minecraft 1.11\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=89340)\u001b[0m 4 episode is done\n",
      "\u001b[2m\u001b[36m(pid=89340)\u001b[0m total rewards : 0 \n",
      "\u001b[2m\u001b[36m(pid=89340)\u001b[0m actor network is updated \n",
      "\u001b[2m\u001b[36m(pid=89340)\u001b[0m actor target_network is updated\n",
      "\u001b[2m\u001b[36m(pid=89346)\u001b[0m Count : 320 leaner_network updated\n",
      "\u001b[2m\u001b[36m(pid=89346)\u001b[0m learner model saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=89347)\u001b[0m MineRL agent is public, connect on port 12340 with Minecraft 1.11\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=89346)\u001b[0m Count : 330 leaner_network updated\n",
      "\u001b[2m\u001b[36m(pid=89346)\u001b[0m learner model saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=89340)\u001b[0m MineRL agent is public, connect on port 12342 with Minecraft 1.11\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=89346)\u001b[0m Count : 340 leaner_network updated\n",
      "\u001b[2m\u001b[36m(pid=89346)\u001b[0m learner model saved\n",
      "\u001b[2m\u001b[36m(pid=89346)\u001b[0m Count : 350 leaner_target_network updated\n",
      "\u001b[2m\u001b[36m(pid=89346)\u001b[0m Count : 350 leaner_network updated\n",
      "\u001b[2m\u001b[36m(pid=89346)\u001b[0m learner model saved\n",
      "\u001b[2m\u001b[36m(pid=89346)\u001b[0m Count : 360 leaner_network updated\n",
      "\u001b[2m\u001b[36m(pid=89346)\u001b[0m learner model saved\n",
      "\u001b[2m\u001b[36m(pid=89346)\u001b[0m Count : 370 leaner_network updated\n",
      "\u001b[2m\u001b[36m(pid=89346)\u001b[0m learner model saved\n",
      "\u001b[2m\u001b[36m(pid=89343)\u001b[0m 5 episode is done\n",
      "\u001b[2m\u001b[36m(pid=89343)\u001b[0m total rewards : 0 \n",
      "\u001b[2m\u001b[36m(pid=89343)\u001b[0m actor network is updated \n",
      "\u001b[2m\u001b[36m(pid=89343)\u001b[0m actor target_network is updated\n",
      "\u001b[2m\u001b[36m(pid=89347)\u001b[0m 5 episode is done\n",
      "\u001b[2m\u001b[36m(pid=89347)\u001b[0m total rewards : 0 \n",
      "\u001b[2m\u001b[36m(pid=89347)\u001b[0m actor network is updated \n",
      "\u001b[2m\u001b[36m(pid=89347)\u001b[0m actor target_network is updated\n",
      "\u001b[2m\u001b[36m(pid=89346)\u001b[0m Count : 380 leaner_network updated\n",
      "\u001b[2m\u001b[36m(pid=89346)\u001b[0m learner model saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=89343)\u001b[0m MineRL agent is public, connect on port 12341 with Minecraft 1.11\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=89340)\u001b[0m 5 episode is done\n",
      "\u001b[2m\u001b[36m(pid=89340)\u001b[0m total rewards : 0 \n",
      "\u001b[2m\u001b[36m(pid=89340)\u001b[0m actor network is updated \n",
      "\u001b[2m\u001b[36m(pid=89340)\u001b[0m actor target_network is updated\n",
      "\u001b[2m\u001b[36m(pid=89346)\u001b[0m Count : 390 leaner_network updated\n",
      "\u001b[2m\u001b[36m(pid=89346)\u001b[0m learner model saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=89347)\u001b[0m MineRL agent is public, connect on port 12340 with Minecraft 1.11\n",
      "\u001b[2m\u001b[36m(pid=89340)\u001b[0m MineRL agent is public, connect on port 12342 with Minecraft 1.11\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=89346)\u001b[0m Count : 400 leaner_target_network updated\n",
      "\u001b[2m\u001b[36m(pid=89346)\u001b[0m Count : 400 leaner_network updated\n",
      "\u001b[2m\u001b[36m(pid=89346)\u001b[0m learner model saved\n",
      "\u001b[2m\u001b[36m(pid=89346)\u001b[0m Count : 410 leaner_network updated\n",
      "\u001b[2m\u001b[36m(pid=89346)\u001b[0m learner model saved\n",
      "\u001b[2m\u001b[36m(pid=89346)\u001b[0m Count : 420 leaner_network updated\n",
      "\u001b[2m\u001b[36m(pid=89346)\u001b[0m learner model saved\n",
      "\u001b[2m\u001b[36m(pid=89346)\u001b[0m Count : 430 leaner_network updated\n",
      "\u001b[2m\u001b[36m(pid=89346)\u001b[0m learner model saved\n",
      "\u001b[2m\u001b[36m(pid=89343)\u001b[0m 6 episode is done\n",
      "\u001b[2m\u001b[36m(pid=89343)\u001b[0m total rewards : 0 \n",
      "\u001b[2m\u001b[36m(pid=89343)\u001b[0m actor network is updated \n",
      "\u001b[2m\u001b[36m(pid=89343)\u001b[0m actor target_network is updated\n",
      "\u001b[2m\u001b[36m(pid=89346)\u001b[0m Count : 440 leaner_network updated\n",
      "\u001b[2m\u001b[36m(pid=89346)\u001b[0m learner model saved\n",
      "\u001b[2m\u001b[36m(pid=89347)\u001b[0m 6 episode is done\n",
      "\u001b[2m\u001b[36m(pid=89347)\u001b[0m total rewards : 0 \n",
      "\u001b[2m\u001b[36m(pid=89347)\u001b[0m actor network is updated \n",
      "\u001b[2m\u001b[36m(pid=89347)\u001b[0m actor target_network is updated\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=89343)\u001b[0m MineRL agent is public, connect on port 12341 with Minecraft 1.11\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=89346)\u001b[0m Count : 450 leaner_target_network updated\n",
      "\u001b[2m\u001b[36m(pid=89346)\u001b[0m Count : 450 leaner_network updated\n",
      "\u001b[2m\u001b[36m(pid=89346)\u001b[0m learner model saved\n",
      "\u001b[2m\u001b[36m(pid=89340)\u001b[0m 6 episode is done\n",
      "\u001b[2m\u001b[36m(pid=89340)\u001b[0m total rewards : 0 \n",
      "\u001b[2m\u001b[36m(pid=89340)\u001b[0m actor network is updated \n",
      "\u001b[2m\u001b[36m(pid=89340)\u001b[0m actor target_network is updated\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=89347)\u001b[0m MineRL agent is public, connect on port 12340 with Minecraft 1.11\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=89346)\u001b[0m Count : 460 leaner_network updated\n",
      "\u001b[2m\u001b[36m(pid=89346)\u001b[0m learner model saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=89340)\u001b[0m MineRL agent is public, connect on port 12342 with Minecraft 1.11\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=89346)\u001b[0m Count : 470 leaner_network updated\n",
      "\u001b[2m\u001b[36m(pid=89346)\u001b[0m learner model saved\n",
      "\u001b[2m\u001b[36m(pid=89346)\u001b[0m Count : 480 leaner_network updated\n",
      "\u001b[2m\u001b[36m(pid=89346)\u001b[0m learner model saved\n",
      "\u001b[2m\u001b[36m(pid=89346)\u001b[0m Count : 490 leaner_network updated\n",
      "\u001b[2m\u001b[36m(pid=89346)\u001b[0m learner model saved\n",
      "\u001b[2m\u001b[36m(pid=89346)\u001b[0m Count : 500 leaner_target_network updated\n",
      "\u001b[2m\u001b[36m(pid=89346)\u001b[0m Count : 500 leaner_network updated\n",
      "\u001b[2m\u001b[36m(pid=89346)\u001b[0m learner model saved\n",
      "\u001b[2m\u001b[36m(pid=89343)\u001b[0m 7 episode is done\n",
      "\u001b[2m\u001b[36m(pid=89343)\u001b[0m total rewards : 0 \n",
      "\u001b[2m\u001b[36m(pid=89343)\u001b[0m actor network is updated \n",
      "\u001b[2m\u001b[36m(pid=89343)\u001b[0m actor target_network is updated\n",
      "\u001b[2m\u001b[36m(pid=89346)\u001b[0m Count : 510 leaner_network updated\n",
      "\u001b[2m\u001b[36m(pid=89346)\u001b[0m learner model saved\n",
      "\u001b[2m\u001b[36m(pid=89347)\u001b[0m 7 episode is done\n",
      "\u001b[2m\u001b[36m(pid=89347)\u001b[0m total rewards : 0 \n",
      "\u001b[2m\u001b[36m(pid=89347)\u001b[0m actor network is updated \n",
      "\u001b[2m\u001b[36m(pid=89347)\u001b[0m actor target_network is updated\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=89343)\u001b[0m MineRL agent is public, connect on port 12341 with Minecraft 1.11\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=89340)\u001b[0m 7 episode is done\n",
      "\u001b[2m\u001b[36m(pid=89340)\u001b[0m total rewards : 0 \n",
      "\u001b[2m\u001b[36m(pid=89340)\u001b[0m actor network is updated \n",
      "\u001b[2m\u001b[36m(pid=89340)\u001b[0m actor target_network is updated\n",
      "\u001b[2m\u001b[36m(pid=89346)\u001b[0m Count : 520 leaner_network updated\n",
      "\u001b[2m\u001b[36m(pid=89346)\u001b[0m learner model saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=89347)\u001b[0m MineRL agent is public, connect on port 12340 with Minecraft 1.11\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=89346)\u001b[0m Count : 530 leaner_network updated\n",
      "\u001b[2m\u001b[36m(pid=89346)\u001b[0m learner model saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=89340)\u001b[0m MineRL agent is public, connect on port 12342 with Minecraft 1.11\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=89346)\u001b[0m Count : 540 leaner_network updated\n",
      "\u001b[2m\u001b[36m(pid=89346)\u001b[0m learner model saved\n",
      "\u001b[2m\u001b[36m(pid=89346)\u001b[0m Count : 550 leaner_target_network updated\n",
      "\u001b[2m\u001b[36m(pid=89346)\u001b[0m Count : 550 leaner_network updated\n",
      "\u001b[2m\u001b[36m(pid=89346)\u001b[0m learner model saved\n",
      "\u001b[2m\u001b[36m(pid=89346)\u001b[0m Count : 560 leaner_network updated\n",
      "\u001b[2m\u001b[36m(pid=89346)\u001b[0m learner model saved\n",
      "\u001b[2m\u001b[36m(pid=89343)\u001b[0m 8 episode is done\n",
      "\u001b[2m\u001b[36m(pid=89343)\u001b[0m total rewards : 0 \n",
      "\u001b[2m\u001b[36m(pid=89343)\u001b[0m actor network is updated \n",
      "\u001b[2m\u001b[36m(pid=89343)\u001b[0m actor target_network is updated\n",
      "\u001b[2m\u001b[36m(pid=89346)\u001b[0m Count : 570 leaner_network updated\n",
      "\u001b[2m\u001b[36m(pid=89346)\u001b[0m learner model saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=89343)\u001b[0m MineRL agent is public, connect on port 12341 with Minecraft 1.11\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=89347)\u001b[0m 8 episode is done\n",
      "\u001b[2m\u001b[36m(pid=89347)\u001b[0m total rewards : 0 \n",
      "\u001b[2m\u001b[36m(pid=89347)\u001b[0m actor network is updated \n",
      "\u001b[2m\u001b[36m(pid=89347)\u001b[0m actor target_network is updated\n",
      "\u001b[2m\u001b[36m(pid=89346)\u001b[0m Count : 580 leaner_network updated\n",
      "\u001b[2m\u001b[36m(pid=89346)\u001b[0m learner model saved\n",
      "\u001b[2m\u001b[36m(pid=89340)\u001b[0m 8 episode is done\n",
      "\u001b[2m\u001b[36m(pid=89340)\u001b[0m total rewards : 0 \n",
      "\u001b[2m\u001b[36m(pid=89340)\u001b[0m actor network is updated \n",
      "\u001b[2m\u001b[36m(pid=89340)\u001b[0m actor target_network is updated\n",
      "\u001b[2m\u001b[36m(pid=89346)\u001b[0m Count : 590 leaner_network updated\n",
      "\u001b[2m\u001b[36m(pid=89346)\u001b[0m learner model saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=89347)\u001b[0m MineRL agent is public, connect on port 12340 with Minecraft 1.11\n",
      "\u001b[2m\u001b[36m(pid=89340)\u001b[0m MineRL agent is public, connect on port 12342 with Minecraft 1.11\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=89346)\u001b[0m Count : 600 leaner_target_network updated\n",
      "\u001b[2m\u001b[36m(pid=89346)\u001b[0m Count : 600 leaner_network updated\n",
      "\u001b[2m\u001b[36m(pid=89346)\u001b[0m learner model saved\n",
      "\u001b[2m\u001b[36m(pid=89346)\u001b[0m Count : 610 leaner_network updated\n",
      "\u001b[2m\u001b[36m(pid=89346)\u001b[0m learner model saved\n",
      "\u001b[2m\u001b[36m(pid=89346)\u001b[0m Count : 620 leaner_network updated\n",
      "\u001b[2m\u001b[36m(pid=89346)\u001b[0m learner model saved\n"
     ]
    }
   ],
   "source": [
    "ray.get(memory.size.remote())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "educated-impossible",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
